\chapter{Translation to \Erlang}
\index{functional language!Erlang@\Erlang|(}

Translating our toy functional language to \Erlang happens to be very
easy a task. Section~\ref{sec:implementation} in the introduction
already provided an example. Besides the need for module headers, some
lexical conventions are needed, as well as a knowledge about how the
compiler handles data sharing. Furthermore, we explain in terms of the
memory model in section~\ref{sec:memory} how the concepts of
\emph{control stack} and \emph{heap} emerge, as well as an important
optimisation technique implemented by most compilers of functional
languages: \emph{tail call optimisation}\index{memory!tail call
  optimisation} (also known as \emph{last call
  optimisation}\index{memory!last call optimisation|see{tail call
    optimisation}}).

\paragraph{Lexis and syntax}

In \Erlang, stacks are called \emph{lists}. Nevertheless, we shall
keep using `stack' to retain a uniform reading throughout this book.

The first letter of variables is set in uppercase, for instance,
\emph{data}~is translated as~\erlcode{Data}, and
\(x\)~becomes~\erlcode{X}.

Constant data constructors\index{functional language!data constructor}
are set without their pair of parentheses, for example,
\(\fun{absent}()\)~is translated as \erlcode{absent}. If arguments are
present, a tuple must be used. Tuples in \Erlang are written with
curly brackets, to distinguish them from the parentheses of function
calls, so \(\pair{x}{y}\)~is translated as \erlcode{\{X,Y\}}. Then,
\(\fun{one}(s)\) becomes \erlcode{\{one,S\}} in \Erlang if \fun{one/1}
is a constructor, otherwise \erlcode{one(S)}, if \erlcode{one/1} is a
function. In \Erlang, a constant constructor is called an
\emph{atom}\index{functional language!Erlang@\Erlang!atom}.

When a variable in a pattern is unused in the right\hyp{}hand side, it
may be replaced by an underscore, so
\begin{equation*}
\fun{len}(\el)         \rightarrow 0;\qquad
\fun{len}(\cons{x}{s}) \rightarrow 1 + \fun{len}(s).
\end{equation*}
may be translated in \Erlang as
\begin{verbatim}
len(   []) -> 0;
len([_|S]) -> 1 + len(S).
\end{verbatim}
Insofar syntax is concerned, we must translate the conditional rewrite
rules using the keyword \texttt{\textbf{when}} and lay the condition
on the left\hyp{}hand side. For instance, consider again straight
insertion in section~\ref{sec:straight_ins}:
\begin{equation*}
\fun{ins}(\cons{y}{s},x) \rightarrow
\cons{y}{\fun{ins}(s,x)}, \,\text{if \(x \succ y\)};\qquad
\fun{ins}(s,x) \rightarrow \cons{x}{s}.
\end{equation*}
This definition is translated in \Erlang as
\begin{alltt}
ins([Y|S],X) \textbf{when} X > Y -> [Y|ins(S,X)];
ins(    S,X)            -> [X|S].
\end{alltt}
Note that, in \Erlang, \erlcode{X > Y} implies that
\erlcode{X}~and~\erlcode{Y} are integers or atoms (which are ordered
alphabetically). In \Erlang, a rewrite rule is called
a~\emph{clause}. Its left\hyp{}hand side is called the~\emph{head} and
the right\hyp{}hand side the~\emph{body}. A condition on a clause is
called a~\emph{guard}. As a side note, the lexical conventions, syntax
and vocabulary of \Erlang have been drawn indirectly from the \Prolog
programming language \citep{SterlingShapiro_1994,Bratko_2000}. The
structure of a clause in \Erlang is summed up in
\fig~\vref{fig:clause}.
\begin{figure}[t]
\centering
\includegraphics[bb=214 602 397 646]{clause}
\caption{Structure of a clause in \Erlang\label{fig:clause}}
\end{figure}

A comment is introduced by \erlcode{\%} and extends till the end of
the line.

\paragraph{Inference systems}

The translation of programs defined by means of an inference system
consists either in refining it so the new version does not contain any
inference rule (see for example \fig~\vref{fig:per} and
\fig~\ref{fig:per0}), and then translate into \Erlang, or else in
using directly an \Erlang construct called \texttt{\textbf{case}},
which is a general conditional expression. Consider again
\fig~\vref{fig:per}. A direct translation to \Erlang yields
\begin{alltt}
per(ext)           -> 0;
per(\{int,\_,T1,T2\}) -> \textbf{case} per(T1) \textbf{of}
                        false -> false;
                            H -> \textbf{case} per(T2) \textbf{of}
                                   H -> H + 1;
                                   \_ -> false
                                 \textbf{end}
                      \textbf{end}.
\end{alltt}
Notice that we translated the inference rule in two cases, not one,
because it would be inefficient to compute both \erlcode{per(T1)} and
\erlcode{per(T2)} if \erlcode{per(T1)} evaluates in
\erlcode{false}. Moreover, a variable in a pattern in a case can be
bound to a value defined before (in \OCaml, for example, it cannot),
so \erlcode{\textbf{case} per(T2) \textbf{of} H -> ...} implicitly
implies that~\erlcode{H} has the same value as the~\erlcode{H} of
\erlcode{\textbf{case} per(T1) \textbf{of} ...; H -> ...}

Consider another example in \fig~\vref{fig:comp}. It is translated as follows:
\begin{alltt}
comp(\{int,\_,ext,ext\}) -> true;
comp(\{int,\_,T1,T2\})   -> \textbf{case} comp(T1) \textbf{of}
                           false -> false;
                               \_ -> comp(T2)
                         \textbf{end};
comp(ext)             -> false.
\end{alltt}
The call \(\fun{comp}(t_2)\) could evaluate in \(\fun{false}()\), leading to select the rule \(\fun{comp}(t) \rightarrow \fun{false}()\). This backtracking is not possible in \Erlang: once the head of a clause has been successfully matched, the remaining heads will not be considered. By factoring the call \(\fun{comp}(t_1)\), we can solve this problem and the inference rule becomes just one case.

The function in \fig~\vref{fig:pre2b0} becomes
\begin{alltt}
pre2b0(S) -> \textbf{case} pre2b1(S) \textbf{of}
               \{T,[]\} -> T
             \textbf{end}.

pre2b1([ext|S]) -> \{ext,S\};
pre2b1([X|S])   ->
  \textbf{case} pre2b1(S) \textbf{of}
    \{T1,S1\} -> \textbf{case} pre2b1(S1) \textbf{of}
                 \{T2,S2\} -> \{\{int,X,T1,T2\},S2\}
               \textbf{end}
  \textbf{end}.
\end{alltt}
Here, we have a situation with case constructs which cannot fail on
the one case they check. (It is said \emph{irrefutable}.) \Erlang
provides a shorter syntax for this usage as follows:
\begin{verbatim}
pre2b0(S) -> {T,[]}=pre2b1(S), T.

pre2b1([ext|S]) -> {ext,S};
pre2b1([X|S])   -> {T1,S1}=pre2b1(S),
                   {T2,S2}=pre2b1(S1),
                   {{int,X,T1,T2},S2}.
\end{verbatim}
Let us consider an example where inference rules are not used, but we
realise that some functions act simply as a conditional expressions. We
can see this in \fig~\vref{fig:bst}, with functions \fun{norm/1} and
\fun{cmp/3}:
\begin{alltt}
bst(T) -> \textbf{case} bst(T,infty) \textbf{of} false -> false;
                                   \_ -> true
          \textbf{end}.

bst(ext,M)           -> M;
bst(\{bst,X,T1,T2\},M) -> \textbf{case} bst(T2,M) \textbf{of}
                                 infty -> bst(T1,X);
                          N \textbf{when} N > X -> bst(T1,X);
                                     \_ -> false
                        \textbf{end}.
\end{alltt}
Note how we had to rename one variable~\(m\) into~\erlcode{N}, to
avoid the unwanted binding with~\erlcode{M} before. Would the
definition of \fun{bst/1} in \fig~\ref{fig:bst} be shorter had we used
inference rules?

Factoring a word in a text using the Morris\hyp{}Pratt algorithm
\index{word factoring!Morris-Pratt} yielded a definition with some
inference rules in \figs~\vrefrange{fig:fail}{fig:mp_def}. Including
the solution to Exercise~\vref{factoring:trick}, we translate in
\Erlang as follows:
\begin{alltt}
fail(        _,0) -> -1;
fail([\{A,K\}|P],I) -> fp(P,A,K,I-1).

fp(\_,\_,-1,\_) -> 0;
fp(P,A, K,I) -> \textbf{case} suf(P,I-K-1) \textbf{of}
                  [\{A,\_\}|\_] -> K + 1;
                  [\{\_,J\}|Q] -> fp(Q,A,J,K)
                \textbf{end}.

suf(    P,0) -> P;
suf([\_|P],I) -> suf(P,I-1).

mp(P,T) -> PP=pp(P), mp(PP,T,PP,0,0).

mp(        [],    \_, \_,I,J) -> \{factor,J-I\};
mp(         \_,   [], \_,\_,\_) -> absent;
mp( [\{A,\_\}|P],[A|T],PP,I,J) -> mp(P,T,PP,I+1,J+1);
mp([\{\_,-1\}|\_],[\_|T],PP,0,J) -> mp(PP,T,PP,0,J+1);
mp( [\{\_,K\}|\_],    T,PP,\_,J) -> mp(suf(PP,K),T,PP,K,J).

pp(X) -> pp(X,[],0).

pp(   [],\_,\_) -> [];
pp([A|X],P,I) -> U=\{A,fail(P,I)\}, [U|pp(X,[U|P],I+1)].
\end{alltt}

\paragraph{The \Erlang shell}

The pieces of source code up to now are not complete \Erlang programs
for an \Erlang program to be self\hyp{}contained needs to be a
\emph{module}. A module is a unit of compilation containing a
collection of function definitions. The module name must be the
basename of the file containing the module. For instance, the
following module named \erlcode{math1},
\begin{alltt}
\textbf{-module(math1).}\hfill% \emph{Drop the file extension} .erl
\textbf{-export([fact/1]).}
fact(1)            -> 1;
fact(N) when N > 1 -> N * fact(N-1).
\end{alltt}
must be written in a file named \erlcode{math1.erl}. The
\erlcode{-export} line lists the names of the functions which can be
called from outside the module, that is, either from another module or
from the \Erlang \emph{shell}. A shell is an application which reads
commands entered by some user, interprets them, prints a result or an
error message and waits for further commands.

In order to test some examples with \erlcode{fact/1}, we first have to
launch the \Erlang shell. Depending on the operating system, the
programming environment may vary greatly. Here, we shall assume a
command\hyp{}line interface, like the ones available in a terminal for
the \Unix operating systems and its derivatives. The \Erlang shell is
an application which allows us to interactively compile modules and
call functions from them. Its name is likely to be~\erlcode{erl}. Here
is the start of a session with the shell:
\begin{alltt}
\$ erl
Erlang R14B04 (erts-5.8.5) [source] [smp:4:4] [rq:4]
[async-threads:0] [hipe] [kernel-poll:false]

Eshell V5.8.5  (abort with ^G)
1> \(\talloblong\)
\end{alltt}
The first line is the command to run the shell. The last line is the
prompt of the \Erlang shell, the number~\texttt{1} meaning that the
shell is waiting for the first command. Note that the terminal prompt
is denoted by a dollar sign (\erlcode{\$}). The character
\(\talloblong\) denotes the blinking prompt of the \Erlang shell where
typing will occur. If we want to close the shell and return to the
operating system shell, just type~`\erlcode{q().}' (standing
for~`quit'). Each command must be terminated by a period~(\erlcode{.})
and followed by a pressure on the return key.
\begin{alltt}
1> q().
ok
2> \$ \textvisiblespace
\end{alltt}
The character \erlcode{\textvisiblespace} represents the place where
text is to be typed in the operating system shell. But before quitting
the \Erlang shell, the first action usually consists in calling the
\Erlang compiler to process some module we want to use. This is done
by the command~`\erlcode{c}', whose argument is the module name. In
our example, the filename is \texttt{math1.erl}:
\begin{alltt}
1> c(math1).
\{ok,math1\}
2> \(\talloblong\)
\end{alltt}
The compilation was successful, as the atom \erlcode{ok} says. Let us
compute some factorials now:
\begin{alltt}
2> math1:fact(4).
24
3> math1:fact(-3).
** exception error: no function clause matching
math1:fact(-3)
4> \(\talloblong\)
\end{alltt}
The error message is very legible. In this book, we will rarely copy
and paste the input to and the output from the \Erlang shell. We will
not write complete modules as well because we want to focus on the
programming itself and delegate the practical aspects to a user manual
or a textbook oriented towards practice.


\section{Memory}
\label{sec:memory}
\index{memory|(}

Let us review some programs under the angle of memory usage instead of cost. In the introduction, we stated that the essence of an expression is best captured by a bidimensional representation, namely a tree, as opposed to a line of punctuated text. In section~\vref{sec:skipping}, we introduced the syntactic notions of context of a call and tail form of a definition. We also assumed that identical data structures occurring in the left\hyp{}hand and right\hyp{}hand sides of the same rule are actually shared.

In the present section, we elaborate on these concepts and
representations, and show how they enable a better understanding of
memory management by the run\hyp{}time environment of a functional
language, generated by a compiler. Nevertheless, these matters depend
strongly on the compiler and the hardware architecture at hand, so it
would be imprudent to pursue a description too detailed. Therefore, it
is sufficient and appropriate here to provide a refined model based on
the directed acyclic graphs only, to wit, abstract syntax trees with
explicit sharing\index{tree!abstract syntax $\sim$}\index{directed
  acyclic graph}. Typically, we consider the number of nodes of these
trees, or a particular kind, like cons\hyp{}nodes\index{cons-node}, as
a measure of how much memory needs to be allocated in total.

\paragraph{Summing integers}

Here is the definition of \erlcode{sum/1}, which sums the integers in
a given stack:
\begin{verbatim}
sum([N])   -> N;
sum([N|S]) -> N + sum(S).
\end{verbatim}
For the sake of legibility, let us label the arrows:
\begin{alltt}
sum([N])   \(\xrightarrow{\smash[t]{\alpha}}\) N;
sum([N|S]) \(\xrightarrow{\smash[t]{\beta}}\) N + sum(S).
\end{alltt}
We have, for instance,
\begin{alltt}
sum([1|[2|[3|[]]]]) \(\xrightarrow{\smash[t]{\beta}}\) 1 + sum([2|[3|[[]]]])
                    \(\xrightarrow{\smash[t]{\beta}}\) 1 + (2 + sum([3|[]]))
                    \(\xrightarrow{\smash[t]{\alpha}}\) 1 + (2 + (3))
                    \(=\) 6.
\end{alltt}

What can be said about the speed and the memory usage of the function
\erlcode{sum/1}? The number of rewrites clearly equals the number of
integers in the stack because every integer is matched. Hence, if the
initial function is called on a stack of \(n\)~integers, the number of
steps to reach the result is~\(n\): \(n-1\) times using
clause~\clause{\beta}, and once using clause~\clause{\alpha}. Taking a
slightly longer stack can provide a hint about memory usage:
\begin{alltt}
sum([1|[2|[3|[4|[]]]]]) \(\xrightarrow{\smash[t]{\beta}}\) 1 + sum([2|[3|[4|[]]]])
                        \(\xrightarrow{\smash[t]{\beta}}\) 1 + (2 + sum([3|[4|[]]]))
                        \(\xrightarrow{\smash[t]{\beta}}\) 1 + (2 + (3 + sum([4|[]])))
                        \(\xrightarrow{\smash[t]{\alpha}}\) 1 + (2 + (3 + (4)))
                        \(=\) 10\textrm{.}
\end{alltt}
This prompts us to consider only the sizes of the right\hyp{}hand
sides:
\begin{alltt}
sum([1|[2|[3|[4|[]]]]]) \(\xrightarrow{\smash[t]{\beta}}\) \fbcode{1 + sum([2|[3|[4|[]]]])}
                        \(\xrightarrow{\smash[t]{\beta}}\) \fbcode{1 + (2 + sum([3|[4|[]]]))}
                        \(\xrightarrow{\smash[t]{\beta}}\) \fbcode{1 + (2 + (3 + sum([4|[]])))}
                        \(\xrightarrow{\smash[t]{\alpha}}\) \fbcode{1 + (2 + (3 + (4)))}\,\textrm{.}
\end{alltt}
It seems that the total memory usage increases slowly and then reduces
sharply after the last rewrite step. But omitting blanks yields
\begin{verbatim}
sum([1|[2|[3|[4|[]]]]]) -> 1+sum([2|[3|[4|[]]]])
                        -> 1+(2+sum([3|[4|[]]]))
                        -> 1+(2+(3+sum([4|[]])))
                        -> 1+(2+(3+(4))).
\end{verbatim}
It looks as if, now, the expressions are of constant size until
clause~\(\alpha\) applies. Moreover, even if (\erlcode{+}) were
instead written \erlcode{plus}, its occurrence should not be
considered as taking more memory than (\erlcode{+}) because names are
only tags. Also, what about the parentheses and the blanks? Should
they be considered meaningful, as far as memory allocation is
concerned? All these considerations bring to the fore the need for a
finer understanding of how \Erlang functions and data are usually
represented at run\hyp{}time but, because these encodings depend
strongly on the compiler and the hardware architecture, we should not
rely on too detailed a description. The adequate model is the
\emph{abstract syntax trees} and the \emph{directed acyclic graphs},
seen in the introduction. These allow us to draw conclusions about
memory usage which hold up in proportion of a constant.

\paragraph{Concatenation of stacks}
\index{memory!concatenation of stacks|(}

Definition in \fig~\vref{def:cat} is
\begin{equation*}
\fun{cat}(        \el,t) \xrightarrow{\smash{\alpha}} t;\qquad
\fun{cat}(\cons{x}{s},t) \xrightarrow{\smash{\beta}}
                                 \cons{x}{\fun{cat}(s,t)}.
\end{equation*}
The relevant measure of memory usage here is the number of cons\hyp{}nodes\index{cons-node} created by rule~\clause{\beta}. Clearly, the call \(\fun{cat}(s,t)\) yields \(n\)~such nodes, where \(n\)~is the length of stack~\(s\).\index{memory!concatenation of stacks|)}

\paragraph{Reversal of stacks}
\index{memory!reversal of stacks|(}

The definition of~\fun{rev\(_0\)/1} is found in
section~\ref{sec:reversal}:
\begin{equation*}
\fun{rev}_0(\el) \xrightarrow{\smash{\gamma}} \el;\qquad
\fun{rev}_0(\cons{x}{s}) \xrightarrow{\smash{\delta}}
                         \fun{cat}(\fun{rev}_0(s),[x]).
\end{equation*}
The empty stack on the right\hyp{}hand side of rule~\clause{\gamma} is
shared with the pattern. Of course, the same would hold for any
constant. We already know how many such nodes are created by calls to
\fun{cat/2} and, since the length of~\(s\) in the recursive call
\(\fun{rev}_0(s)\) decreases by~\(1\) each time, the number of pushes
is \(\sum_{k=1}^{n-1}k = \tfrac{1}{2}n(n-1)\), if the original stack
contains \(n\)~items. We need to add one push for each \([x]\), that
is,~\(n\). In total: \(n(n+1)/2\).

The alternative definition~\eqref{def:rev} of the reversal,
\vpageref{def:rev}, is
\begin{equation*}
\fun{rev}(s) \xrightarrow{\smash{\epsilon}} \fun{rcat}(s,\el).\quad\;\;
\fun{rcat}(\el,t) \xrightarrow{\smash{\zeta}} t;\quad\;\;
\fun{rcat}(\cons{x}{s},t) \xrightarrow{\smash{\eta}}
                          \fun{rcat}(s,\cons{x}{t}).
\end{equation*}
Clearly, the total number of pushes is~\(n\), the length of the input
stack.
\index{memory!reversal of stacks|)}

\paragraph{Merging}
\index{memory!merging|(}

Let us quantify the memory to sort by merging \(n=2^p\)~keys,
bottom\hyp{}up. First, the number of stacks created is the number of
nodes of the merge tree: \(2^p + 2^{p-1} + \ldots + 2^0 = 2^{p+1}-1 =
2n - 1\). There is one cons\hyp{}node\index{cons-node} for each key,
which leads us to determine the sum of the lengths of all the created
stacks: \((p+1)2^p = n\lg n + n\). This is the total number of
cons\hyp{}nodes\index{cons-node}. In the case of top\hyp{}down
mergers, only the first half of the stacks, including the original
one, are reversed, hence allocate cons\hyp{}nodes. As a consequence,
the total number of cons\hyp{}nodes created is \(\tfrac{1}{2}n\lg n\).
\index{memory!merging|)}

%\section{Context of a call}
\paragraph{Context of a call}
\index{functional language!call context|(}
\index{memory!call context|(}

We want now to get a better understanding of how the context of a
recursive call impact the evaluation. As a continued example, let us
define a function \fun{sum/1} such that the call \(\fun{sum}(s)\) is
the sum of the integers in the stack~\(s\):
\begin{equation*}
\fun{sum}([n]) \xrightarrow{\smash{\alpha}} n;\qquad
\fun{sum}(\cons{n}{s}) \xrightarrow{\smash{\beta}} n + \fun{sum}(s).
\end{equation*}
Consider in \fig~\vref{fig:sum_ast} the abstract syntax tree of
\(1+(2+\fun{sum}([3,4]))\) and the function call it contains in
\fig~\ref{fig:sum_call}.
\begin{figure}[b]
\centering
\subfloat[Expression\label{fig:sum_ast}]{%
  \includegraphics[bb=71 627 134 721]{sum_ast}
}
\qquad
\subfloat[Instances of the context \(n +
  \texttt{\textvisiblespace}\)\label{fig:sum_ctxt}]{%
  \includegraphics[bb=55 627 135 721]{sum_ctxt}
}
\quad
\subfloat[Call\label{fig:sum_call}]{%
  \includegraphics[bb=71 663 110 731]{sum_call}
}
\qquad
\subfloat[Argument\label{fig:sum_arg}]{%
  \includegraphics[bb=64 679 118 731]{sum_arg}
}
\caption{\(1+(2+\fun{sum}([3,4]))\)}
\end{figure}
By taking as origin the node \fun{sum}, the abstract syntax tree can
be split into the part below it, that is, the argument in
\fig~\ref{fig:sum_arg}, and the part above, called \emph{instances of
  the context}, in \fig~\ref{fig:sum_ctxt}.

The main interest of abstract syntax trees is that no parentheses are
required, because a sub\hyp{}expression is denoted by a subtree, that
is, a tree embedded into another. Moreover, blank characters are
absent as well. Altogether, this brings the essential to the fore. To
illustrate the gained legibility, consider again the previous
computation in full, from left to right (the node to be rewritten is
boxed), in \fig~\ref{fig:sum1234}.
\begin{figure}
\centering
\includegraphics[bb=72 620 412 721]{sum1234}
\caption{Evaluation of \(\fun{sum}([1,2,3,4])\)\label{fig:sum1234}}
\end{figure}
It is now clear that the instances of the context accumulate so as to
grow in inverse proportion to the argument's length: integers move one
by one from the argument to the context and the associated operation
changes from a cons\hyp{}node\index{cons-node} to an
addition. Therefore, if we use as memory unit a node, the total memory
is indeed constant --~except for the last step.

Consider again the running example and what happens to the context,
step after step, in \fig~\vref{fig:sum1234_stack}.
\begin{figure}[t]
\centering
\includegraphics{sum1234_stack}
\caption{Contexts while computing \(\fun{sum}([1,2,3,4])\)
\label{fig:sum1234_stack}}
\end{figure}
This example shows that the context instances grow, while the argument
size decreases in such a way that the total memory remains constant.
\index{functional language!call context|)}
\index{memory!call context|)}

\paragraph{Tail form}
\label{sec:tail}
\index{functional language!tail form|(}

Let us consider the function \erlcode{sum/1}, which sums the integers
given in a stack:
\begin{verbatim}
sum([N])   -> N;
sum([N|S]) -> N + sum(S).
\end{verbatim}
and look for an equivalent definition in tail form, called
\erlcode{sum0/1}. Very much like with the factorial in
equation~\eqref{eq:fact_tf} \vpageref{eq:fact_tf}, the idea is to use
a supplementary argument which accumulates partial results. This kind
of argument is called an \emph{accumulator}. The new version should
hence look like as follows:
\begin{alltt}
sum0(T)       -> sum0(T,\fbcode{0}).
sum0([M],N)   -> \fbcode{sum0(M+N,S)}\,;
sum0([M|S],N) -> \fbcode{sum0(M+N,S)}\,.
\end{alltt}
or, equivalently,
\begin{alltt}
sum0(T)       -> sum0(\fbcode{0},T).
sum0(N,[M])   -> \fbcode{sum0(M+N,S)}\,;
sum0(N,[M|S]) -> \fbcode{sum0(M+N,S)}\,.
\end{alltt}
Notice that, just as with \erlcode{sum/1}, the call \erlcode{sum0([])}
fails without a warning and we may find this behaviour
questionable. Indeed, it can be considered inappropriate in the
framework of software engineering, where programming large and robust
applications is a requisite, but this book focuses primarily on
programming in the small, therefore the programs introduced here are
purposefully fragile; in other words, they may fail on some
undesirable inputs instead of providing the user with some nice
warnings, error messages or, even better, managing to get the compiler
itself to reject such programs.

Since it was decided that an accumulator is needed, we must be clear
on what kind of data it holds. As said previously, an accumulator
usually contains a part of the final result. From a different
perspective, an accumulator can be regarded as a partial trace of all
the previous rewrite steps. Here, since the final result is an
integer, we bear in mind that the accumulator ought to be a number as
well.

There is no need to fill the above canvas (the boxes) from the first
line to the last: this is a program, not an essay. Perhaps the best
method is to first lie down the left\hyp{}hand sides of the clauses
and make sure none is missing and that none is useless (taking into
account the implicit ordering from first to last). Second, we pick up
the clause whose right\hyp{}hand side seems the easiest to guess. For
instance, the first clause of \erlcode{sum/2} seems simple enough
because it applies when only one number, \erlcode{M}, remains in the
input stack. Since the accumulator \erlcode{N}~holds the partial sum
up to now, only~\erlcode{M} remains to be processed. Therefore, the
answer is \erlcode{M+N} or \erlcode{N+M}:
\begin{alltt}
sum0(T)       -> sum0(\fbcode{0},T).
sum0(N,[M])   -> \textbf{M+N};
sum0(N,[M|S]) -> \fbcode{sum0(M+N,S)}\,.
\end{alltt}
The second clause of~\erlcode{sum0/2} is chosen next. It applies when
the input stack is not empty and its first item is~\erlcode{M} and the
remaining are in~\erlcode{S}. Until now, the partial sum is the
accumulator~\erlcode{N}. It is clear that a recursive call is needed
here, because if the body were \erlcode{M+N} again, then the rest of
the integers, \erlcode{S}, would be useless. So the process must be
resumed with another input:
\begin{alltt}
sum0(T)       -> sum0(\fbcode{0},T).
sum0(N,[M])   -> M+N;
sum0(N,[M|S]) -> \textbf{sum0(}\fbcode{M+N}\textbf{,}\fbcode{S}\textbf{)}.
\end{alltt}
The question now is to find what the new stack and the new accumulator
should be in this last clause. What is known about the stack?
\erlcode{M}~and~\erlcode{S}. What can be done with~\erlcode{M}? Well,
the same that was done before, in the first clause of
\erlcode{sum0/2}, that is, let us add it to the accumulator:
\begin{alltt}
sum0(T)       -> sum0(\fbcode{0},T).
sum0(N,[M])   -> M+N;
sum0(N,[M|S]) -> sum0(\textbf{M+N},\fbcode{S}).
\end{alltt}
This way, the new accumulator is \erlcode{M+N}, which is fine since
the purpose of the accumulator is to hold the partial sum until the
present number, which is~\erlcode{M} now. What new stack of numbers
should be used? It is clear that~\erlcode{M} cannot be reused here,
because it has already been added to the accumulator, and it must not
be added twice. This means that it is not needed
anymore. Remains~\erlcode{S}, which is what is sought, since it
represents all the remaining numbers to be added to the accumulator:
\begin{alltt}
sum0(T)       -> sum0(\fbcode{0},T).
sum0(N,[M])   -> M+N;
sum0(N,[M|S]) -> sum0(M+N,\textbf{S}).
\end{alltt}
The last unfinished business is the initial value of the
accumulator. It is important not to rush and to deal with this value
at the last moment. What kind of operation is being carried out on the
accumulator? Additions. Without knowing anything about the integers
in~\erlcode{T}, as it is the case in the clause of \erlcode{sum0/1},
what integer could be taken as an initial value? It is well known
that, for all \(n\), \(n + 0 = n\), thus \erlcode{0}~appears to be the
only possible value here, since it does not change the total sum:
\begin{alltt}
sum0(T)       -> sum0(\textbf{0},T).
sum0(N,[M])   -> M+N;
sum0(N,[M|S]) -> sum0(M+N,S).
\end{alltt}
The last step consists in trying some examples after the labelling
\begin{alltt}
sum0(T)       \(\xrightarrow{\smash[t]{\alpha}}\) sum0(0,T).
sum0(N,[M])   \(\xrightarrow{\smash[t]{\beta}}\) M+N;
sum0(N,[M|S]) \(\xrightarrow{\smash[t]{\gamma}}\) sum0(M+N,S).
\end{alltt}
Consider our running example again:
\begin{alltt}
sum0([1|[2|[3|[4|[]]]]])
  \(\xrightarrow{\smash[t]{\alpha}}\) sum0(0,[1|[2|[3|[4|[]]]]])
  \(\xrightarrow{\smash[t]{\gamma}}\) sum0(1+0,[2|[3|[4|[]]]])   \(=\) sum0(1,[2|[3|[4|[]]]])
  \(\xrightarrow{\smash[t]{\gamma}}\) sum0(2+1,[3|[4|[]]])       \(=\) sum0(3,[3|[4|[]]])
  \(\xrightarrow{\smash[t]{\gamma}}\) sum0(3+3,[4|[]])           \(=\) sum0(6,[4|[]])
  \(\xrightarrow{\smash[t]{\beta}}\) 4 + 6                      \(=\) 10\textrm{.}
\end{alltt}
By contrast, let us recall here the run
\begin{verbatim}
sum([1|[2|[3|[4|[]]]]]) -> 1+sum([2|[3|[4|[]]]])
                        -> 1+(2+sum([3|[4|[]]]))
                        -> 1+(2+(3+sum([4|[]])))
                        -> 1+(2+(3+(4))).
\end{verbatim}
The difference between \erlcode{sum0/1} and \erlcode{sum/1} lies
not in the result (both functions are indeed equivalent) but in the
way the additions are performed. They are equivalent because
\begin{equation*}
4 + (3 + (2 + (1 + 0))) = 1 + (2 + (3 + 4)).
\end{equation*}
This equality holds because, for all numbers \(x\), \(y\) and
\(z\),\label{proof_sum}
\begin{enumerate*}

  \item \label{add_assoc} the addition is associative: \(x + (y + z) =
    (x + y) + z\),

  \item \label{add_comm} the addition is symmetric: \(x + y = y +
    x\),

  \item \label{add_zero} zero is a right\hyp{}neutral number: \(x+0 =
    x\).

\end{enumerate*}
To show exactly why, let us write (\(\eqn{\ref{add_assoc}}\)),
(\(\eqn{\ref{add_comm}}\)) and (\(\eqn{\ref{add_zero}}\)) to denote,
respectively, the use of associativity, symmetry and neutrality, and
lay out the following equalities:
\begin{align*}
4 + (3 + (2 + (1 + 0)))
  &\eqn{\smash{\ref{add_zero}}}  4 + (3 + (2 + 1))\\
  &\eqn{\smash{\ref{add_comm}}}  (3 + (2 + 1)) + 4\\
  &\eqn{\smash{\ref{add_comm}}}  ((2 + 1) + 3) + 4\\
  &\eqn{\smash{\ref{add_comm}}}  ((1 + 2) + 3) + 4\\
  &\eqn{\smash{\ref{add_assoc}}} (1 + 2) + (3 + 4)\\
  &\eqn{\smash{\ref{add_assoc}}} 1 + (2 + (3 + 4)).\hfill\Box
\end{align*}
This seems a bit heavy for such a small program. Is there a way to
rewrite further \erlcode{sum0/1} so that less hypotheses are needed
to prove the equivalence with \erlcode{sum/1}? Let us start with the
most obvious difference: the use of zero. This zero is the initial
value of the accumulator and its sole purpose is to be added to the
first number in the stack. We could then simply first load the
accumulator with this number, so the neutrality of zero is no more
required:
\begin{alltt}
sum0(\textbf{[N|T]})   -> sum0(\textbf{N},T).
sum0(N,[M])   -> M+N;
sum0(N,[M|S]) -> sum0(M+N,S).
\end{alltt}
But this definition of \erlcode{sum0/1} fails on stacks containing
exactly one number, because \erlcode{T}~can be empty. Therefore, we
must allow the stack to be empty in the definition of
\erlcode{sum0/2}:
\begin{alltt}
sum0([N|T])   -> sum0(N,T).
\textbf{sum0(N,   []) -> N;}
sum0(N,[M|S]) -> sum0(M+N,S).
\end{alltt}
Now, we can easily get rid of the hypothesis that the addition is
symmetric: by replacing \erlcode{M+N} by \erlcode{N+M}:
\begin{alltt}
sum0([N|T])   -> sum0(N,T).
sum0(N,   []) -> N;
sum0(N,[M|S]) -> sum0(\textbf{N+M},S).
\end{alltt}
Let us relabel the arrows
\begin{alltt}
sum0([N|T])   \(\xrightarrow{\smash[t]{\alpha}}\) sum0(N,T).
sum0(N,   []) \(\xrightarrow{\smash[t]{\beta}}\) N;
sum0(N,[M|S]) \(\xrightarrow{\smash[t]{\gamma}}\) sum0(N+M,S).
\end{alltt}
and consider again our running example:
\begin{alltt}
sum0([1|[2|[3|[4|[]]]]])
          \(\xrightarrow{\smash[t]{\alpha}}\) sum0(1,[2|[3|[4|[]]]])
          \(\xrightarrow{\smash[t]{\gamma}}\) sum0(1+2,[3|[4|[]]])   \(=\) sum0(3,[3|[4|[]]])
          \(\xrightarrow{\smash[t]{\gamma}}\) sum0(3+3,[4|[]])       \(=\) sum0(6,[4|[]])
          \(\xrightarrow{\smash[t]{\gamma}}\) sum0(4+6,[])           \(=\) sum0(10,[])
          \(\xrightarrow{\smash[t]{\beta}}\) 10\textrm{.}
\end{alltt}
This time, the series of additions corresponds to \(((1+2)+3)+4\),
which we can prove equal to \(1+(2+(3+4))\) by means of associativity
only:
\begin{equation*}
((1 + 2) + 3) + 4
  \eqn{\ref{add_assoc}} (1 + 2) + (3 + 4)
  \eqn{\ref{add_assoc}} 1 + (2 + (3 + 4)).\hfill\Box
\end{equation*}
What about the speed and the memory usage of \erlcode{sum0/1}? It is
easy to see that each step by means of clauses~\clause{\beta}
and~\clause{\gamma} process exactly one integer from the input stack,
so the total number of rewrite steps is the number of integers plus
one due to the initial rewrite through clause~\clause{\alpha}. In
other words, if the initial input stack contains \(n\)~integers, the
number of rewrites is exactly~\(n+1\).

Let us rename \erlcode{sum0/1} as \fun{sum\(_0\)/1} in
\fig~\vref{fig:sum0}.
\begin{figure}
\begin{equation*}
\boxed{%
\begin{array}{@{}r@{\;}l@{\;}l@{}}
\fun{sum}_0(\cons{n}{t}) & \xrightarrow{\smash{\gamma}}
                         & \fun{sum}_0(n,t).\\
      \fun{sum}_0(n,\el) & \xrightarrow{\smash{\delta}} & n;\\
\fun{sum}_0(n,\cons{m}{s}) & \xrightarrow{\smash{\epsilon}}
                           & \fun{sum}_0(n+m,s).
\end{array}}
\end{equation*}
\caption{Summing integers in a stack with \fun{sum\(_0\)/1}
\label{fig:sum0}}
\end{figure}
Consider the abstract syntax trees of the rewritten
expressions in \fig~\ref{fig:sum0_1234}.
\begin{figure}
\centering
\includegraphics{sum0_1234}
\caption{Abstract syntax trees of \(\fun{sum}_0([1,2,3,4])
  \twoheadrightarrow 10\)\label{fig:sum0_1234}}
\end{figure}
The intermediary trees of~\(m+n\) have been skipped to emphasise that
the size of the trees strictly decreases and the size of the context
is constant.

\paragraph{Multiplication}

Consider this time multiplying all the integers in a stack. The first
thing that should come to the mind is that this problem is very
similar to the previous one, only the arithmetic operator is
different, so the following definition can be written immediately, by
modification of \erlcode{sum/1}:
\begin{verbatim}
mult([N])   -> N;
mult([N|S]) -> N * mult(S).
\end{verbatim}
Similarly, a definition in tail form can be derived from
\erlcode{sum0/1}:
\begin{verbatim}
mult0([N|T])   -> mult0(N,T).
mult0(N,   []) -> N;
mult0(N,[M|S]) -> mult0(N*M,S).
\end{verbatim}
The reason why \erlcode{mult0/1} is equivalent to \erlcode{mult/1} is
the same reason why \erlcode{sum0/1} is equivalent to \erlcode{sum/1}:
the arithmetic operator (\erlcode{*}) is associative, just as
(\erlcode{+}) is.

What may be improved that could not be in \erlcode{sum0/1}? In other
words, what can speed up a long series of multiplications? The
occurrence of at least one zero, for example. In that case, it is not
necessary to keep multiplying the remaining numbers, because the
result is going to be zero anyway. This optimisation can be done by
setting apart the case when \erlcode{N}~is~\erlcode{0}:
\begin{alltt}
mult0([N|T])   -> mult0(N,T).
mult0(N,   []) -> N;
\textbf{mult0(N,[0|S]) -> 0;}\hfill% \emph{Improvement.}
mult0(N,[M|S]) -> mult0(N*M,S).
\end{alltt}
How often a zero occurs in the input? In the worst case, there is no
zero and thus the added clause is useless. But if it is known that
zero is likely to be in the input with a probability higher than for
other numbers, this added clause could be useful in the long term,
that is, on the average time of different runs of the
program. Actually, even if the numbers are uniformly random over an
interval including zero, it makes sense to keep the clause.

\index{functional language!tail form|)}
\index{memory|)}


\label{sec:tail_call_opt}

\mypar{Aliasing}
\index{memory!aliasing|(}
\index{stack!flattening!aliasing|(}

In section~\ref{sec:persistence}, we assumed that the sharing between
the pattern and the right\hyp{}hand side of the same rule is
maximal. In practice, compilers do not enforce that property and the
programmers have to explicit the sharing beyond mere variables. For
example, let us consider again \fig~\vref{fig:flat0} defining function
\fun{flat\(_0\)/1}. In \Erlang, maximum sharing in
rule~\clause{\gamma} is achieved by naming \(\cons{x}{s}\) in the
pattern and reusing that name in the right\hyp{}hand side. This name
is called an \emph{alias}. The syntax is self\hyp{}explanatory:
\begin{alltt}
flat0(         []) -> [];
flat0(     [[]|T]) -> flat0(T);
flat0([\textbf{S=[\_|\_]}|T]) -> cat(flat0(\textbf{S}),flat0(T));\hfill% \emph{Aliasing}
flat0(      [Y|T]) -> [Y|flat0(T)].
\end{alltt}
\index{stack!flattening!aliasing|)}
\index{stack!compression|(}
Another example is the function \fun{red/1} (\emph{reduce}), seen in
\fig~\vref{fig:red}, copying its input stack, but discarding items
which are successively repeated:
\begin{equation*}
\begin{array}{@{}r@{\;}l@{\;}l@{}}
\fun{red}(\el)           & \rightarrow & \el;\\
\fun{red}(\cons{x,x}{s}) & \rightarrow & \fun{red}(\cons{x}{s});\\
\fun{red}(\cons{x}{s})   & \rightarrow & \cons{x}{\fun{red}(s)}.
\end{array}
\end{equation*}
For instance, \(\fun{red}([4,2,2,1,1,1,2]) \twoheadrightarrow
[4,2,1,2]\). The translation to \Erlang with maximum sharing is
\begin{alltt}
red(         []) -> [];
red([X|\textbf{S=[X|\_]}]) -> red(\textbf{S});
red(      [X|S]) -> [X|red(S)].
\end{alltt}
\index{stack!compression|)}

\index{merge sort!aliasing|(}
\noindent Another important example is merging in \fig~\vref{fig:mrg}:
\begin{equation*}
\begin{array}{@{}r@{\;}l@{\;}l@{}}
\fun{mrg}(\el,t)         & \rightarrow & t;\\
\fun{mrg}(s,\el)         & \rightarrow & s;\\
\fun{mrg}(\cons{x}{s},\cons{y}{t}) & \rightarrow
& \cons{y}{\fun{mrg}(\cons{x}{s},t)},\;\text{if \(x \succ y\)};\\
\fun{mrg}(\cons{x}{s},t) & \rightarrow
                         & \cons{x}{\fun{mrg}(s,t)}.
\end{array}
\end{equation*}
The best translation is
\begin{alltt}
mrg(     [],    T)            -> T;
mrg(     S,    [])            -> S;
mrg(\textbf{S=[X|\_]},[Y|T]) when X > Y -> [Y|mrg(\textbf{S},T)];
mrg(  [X|S],    T)            -> [X|mrg(S,T)].
\end{alltt}
We must mind the abbreviations in the notations for stacks. For
instance, \fun{tms/1} in \fig~\vref{fig:tms} should be translated as
\begin{alltt}
tms([X|T=[\_|U]]) -> cutr([X],T,U);
tms(          T) -> T.
\end{alltt}
\index{merge sort!aliasing|)}
\index{memory!aliasing|)}

\index{insertion sort!aliasing|(}
\noindent Yet another example is 2-way insertion in
\fig~\vref{fig:i2w_def}:
\begin{equation*}
\begin{array}{@{}r@{\;}l@{\;}ll@{}}
\fun{i2w}(s)         & \xrightarrow{\smash{\xi}}
                     & \fun{i2w}(s,\el,\el).\\
\fun{i2w}(\el,\el,u) & \xrightarrow{\smash{\pi}}
                     & u;\\
\fun{i2w}(\el,\cons{y}{t},u)
                     & \xrightarrow{\smash{\rho}}
                     & \fun{i2w}(\el,t,\cons{y}{u});\\
\fun{i2w}(\cons{x}{s},t,\cons{z}{u})
                     & \xrightarrow{\smash{\sigma}}
                     & \fun{i2w}(\cons{x}{s},\cons{z}{t},u),
                     & \text{if \(x \succ z\)};\\
\fun{i2w}(\cons{x}{s},\cons{y}{t},u)
                     & \xrightarrow{\smash{\tau}}
                     & \fun{i2w}(\cons{x}{s},t,\cons{y}{u}),
                     & \text{if \(y \succ x\)};\\
\fun{i2w}(\cons{x}{s},t,u)
                     & \xrightarrow{\smash{\upsilon}}
                     & \fun{i2w}(s,t,\cons{x}{u}).
\end{array}
\end{equation*}
In \Erlang, maximum sharing requires an alias in clauses
\clause{\sigma}~and~\clause{\tau}:
\begin{alltt}
i2w(S)                              -> i2w(S,[],[]).
i2w(     [],   [],    U)            -> U;
i2w(     [],[Y|T],    U)            -> i2w([],T,[Y|U]);
i2w(\textbf{V=[X|\_]},    T,[Z|U]) when X > Z -> i2w(\textbf{V},[Z|T],U);
i2w(\textbf{V=[X|\_]},[Y|T],    U) when Y > X -> i2w(\textbf{V},T,[Y|U]);
i2w(  [X|S],    T,    U)            -> i2w(S,T,[X|U]).
\end{alltt}
Note that atoms (constant constructors), including the empty stack
\erlcode{[]}, are automatically shared, so, for example, the
alias~\erlcode{S} in \erlcode{f(S=[]) -> S.} is useless.
\index{insertion sort!aliasing|)}

Andersson's search with a tree candidate also benefits from using
aliases. The definitions of \fig~\vref{fig:mem3} is best translated as
\begin{alltt}
mem3(Y,T) -> mem4(Y,T,T).

mem4(Y,  \{bst,X,T1,_\},        T) when X > Y -> mem4(Y,T1,T);
mem4(Y,\textbf{C=\{bst,\_,\_,T2\}},        \_)            -> mem4(Y,T2,\textbf{C});
mem4(Y,         ext,\{bst,Y,\_,\_\})            -> true;
mem4(\_,           ext,        \_)            -> false.
\end{alltt}

Sometimes, aliasing is crucial. For instance, the whole discussion
about persistence in section~\ref{sec:persistence} hinges on having
maximum sharing in each rewrite rule, but, here, \Erlang needs
aliasing to achieve this goal, so
definitions~\eqref{eq:push_pop_persistence}
\vpageref{eq:push_pop_persistence} \emph{must} be implemented as
follows:
\begin{verbatim}
push(X,H=[S|_]) -> [[X|S]|H].
pop(T=[[X|S]|_]) -> {X,[S|T]}.
\end{verbatim}

\mypar{Control stack and heap}
\index{stack!control $\sim$|see{memory, control stack}}
\index{memory!control stack|(}
\index{memory!heap|(}

The memory\index{memory} is under the exclusive supervision of the \emph{garbage collector}. It is a process which has constantly full access to the directed acyclic graphs and whose task consists in finding the nodes which have become useless during evaluation. It consequently gets rid of them, so that subsequently created nodes can find enough room. This chapter hopefully demonstrates that the concept of \emph{control stack} and \emph{heap} arise naturally when a detailed analysis shows how some nodes can be automatically scheduled for deletion as soon as they become useless, thus relieving the garbage collector and improving the timing of memory management. Further investigation shows that calls to functions defined in tail form can be optimised\index{memory!tail call optimisation} so that the total amount of memory needed to evaluate a call is reduced.

For a better understanding of memory management, we need to make
sharing explicit, as with the definition of \fun{sum/1} in
\fig~\vref{fig:sum_dag}.
\begin{figure}
\centering
\includegraphics{sum_dag}%[bb=71 666 237 721]
\caption{Definition of \fun{sum/1} with maximum sharing
\label{fig:sum_dag}}
\end{figure}
Let us compute \(\fun{sum}([3,7,5])\) and show the first rewrites in
\fig~\vref{fig:sum375_push},
\begin{figure}[b]
\centering
\includegraphics{sum375_push}
\caption{Evaluation of \(\fun{sum}([3,7,5])\) with a DAG (phase 1/2)
\label{fig:sum375_push}}
\end{figure}
where the full state of the memory is given as snapshots between
dashed arrows. (DAG is the abbreviation of \emph{directed acyclic
  graph}.) \index{directed acyclic graph} The arrows are dashed as to distinguish them from the ones in the definition, since rewrite rules apply in general only to parts of the trees, and we wanted to display all the data after each rewrite.

Naturally, a question arises on how the value of the original function
call is finally computed (\(15\)). Examining
\fig~\ref{fig:sum375_push} and comparing memory snapshots from left to
right, we realise that roots~(\(+\)) have been accumulating at the
right of the original call, until a reference to a value has been
reached --~here, the integer~\(5\). This process is analogous to
pushing items on a stack, although a stack containing not only values,
but also expressions as \(7 + \fun{sum}([5])\), and we name it
`pushing phase.' This invites us to perform the inverse operation,
that is, popping the expressions in question, in order to finish the
evaluation, or `popping phase.'  More precisely, we want to compute
\emph{values} from the trees composing the DAG from right to left, the
roots of which are considered an item in a special stack, until the
tree of the original call is reached and associated with its own value
--~which is, by definition, the final result. A value can either be an
\emph{immediate} value like integers, atoms and empty stacks, or a
\emph{constructed} value like non\hyp{}empty stacks and tuples. We may
also deal with \emph{references} to values, which are graphically
represented by edges; for instance, the rightmost tree in the DAG is a
reference to the immediate value~\(5\). \emph{When the rightmost tree
  is an immediate value or a reference to a value, the second phase of
  the evaluation (leftward) can take place.} In the following, for the
sake of conciseness, we shall write `value' when a `reference to a
value' is also acceptable.

While our refined computational model forbids erasing nodes, because this is the exclusive task of the garbage collector, it does allow the edge ending in the latest rewritten call to be overwritten by its value. As explained before, these calls have been successively framed in \fig~\vref{fig:sum375_push}. The popping phase consists here in replacing the edge to the previous node \fun{sum} with an edge to the current value. Then the patched tree is evaluated, perhaps leading to more trees to be pushed and subsequent popping phases until one value remains in the stack.

The popping phase is shown at play in \fig~\vref{fig:sum375_pop},
\begin{figure}[b]
\centering
\includegraphics{sum375_pop}
\caption{Evaluation of \(\fun{sum}([3,7,5])\) (phase 2/2)
\label{fig:sum375_pop}}
\end{figure}
which is to be read from right to left. The rightmost memory state is
the result of the prior pushing phase, on the last state of
\fig~\ref{fig:sum375_push}. Note how all the nodes \fun{sum}
and~(\(+\)) become useless, step by step, that is, they cannot be
reached from the stack (these nodes lie below the horizontal base
line). For illustration purposes, we made all the nodes~(\(+\))
disappear as soon as possible and three nodes \fun{sum} are reclaimed
by the garbage collector, including the original one, that is, the
leftmost. The leftmost dashed arrow has the superscript~\(2\) because
it combines two steps (\(3+12 \rightarrow 15\), and discarding the
original node \fun{sum}) at once, for the sake of room. \emph{A node
  is useful if it can be reached from one of the roots of the DAG.}
Keep in mind that the argument of the original call, that is,
\([3,7,5]\), may or may not be freed by the garbage collector,
depending on it being shared or not (from outside the figure, that is,
by some context). The intermediary node containing the value~\(12\)
has been freed as well along the way, to suggest that garbage
collection is interleaved with the evaluation or, if adhering to a
multiprocessing view, we would say that collection and evaluation run
in parallel, sharing the same memory space but without interferences
from the programmer's point of view --~only nodes that are forever
unreachable from a point onwards during the evaluation are swept away.

Our example is actually worth another, closer look. Indeed, we can
predict exactly when the nodes \fun{sum} can be reclaimed: after each
step backwards (from right to left in \fig~\vref{fig:sum375_pop}), the
rightmost node \fun{sum} becomes useless. Same for the intermediary
value~\(12\): it becomes unreachable from the roots of the DAG as soon
as it has been used to compute~\(15\). The same observation can be
made about the nodes~(\(+\)). All these facts mean that, in our
example, we do not need to rely on the garbage collector to identify
these particular nodes as useless: let us really implement an isolated
stack of expressions as a meta\hyp{}object, instead of solely relying
on an analogy and storing everything in the same space. The memory
managed by the garbage collector is called the \emph{heap}, in
contrast with this special stack, called the \emph{control stack}. The
heap and the control stack are separate and complementary, making up
the whole memory. Also, for implementation reasons, the control stack
never contains constructed data but references to constructed data in
the heap.

Consider how the evaluation in \fig~\vref{fig:sum375_pop} can be
improved with automatic deallocation of nodes based on a
stack\hyp{}based policy (`Last In, First Out') in
\fig~\vref{fig:sum375_pop1}.
\begin{figure}[b]
\centering
\includegraphics{sum375_pop1}
\caption{Evaluation of \(\fun{sum}([3,7,5])\) (phase 2/2, stack and
  heap)
\label{fig:sum375_pop1}}
\end{figure}
Remember that the value \([3,7,5]\) is stored in the heap, not in the
control stack, and that it may be shared. Also, due to space
limitations on the page, the last step is actually twofold, as it was
in \fig~\vref{fig:sum375_pop}.  We can seize the growth of the control
stack in \fig~\vref{fig:sum375_stack_push}.
\begin{figure}
\centering
\includegraphics{sum375_stack_push}
\caption{Control stack while computing \(\fun{sum}([3,7,5])\) (phase
  1/2)
\label{fig:sum375_stack_push}}
\end{figure}
The poppings, from right to left, are presented in
\fig~\ref{fig:sum375_stack_pop}.
\begin{figure}
\centering
\includegraphics{sum375_stack_pop}
\caption{Control stack while computing \(\fun{sum}([3,7,5])\)
(phase 2/2)
\label{fig:sum375_stack_pop}}
\end{figure}

The corresponding algorithm consists in the following steps. Let us
first assume, as a result of the previous steps, that the control
stack is not empty and that the top item is an immediate value or a
reference to a value, although we shall refer to either as values.
\begin{enumerate}

  \item While the control stack contains at least two objects, pop out
    the value, but without losing it, so another tree becomes the top;
    \begin{enumerate}

    \item if the root of the top tree is a node \fun{sum}, then
      pop it and push instead the value;

    \item else, the node \fun{sum} in the tree has an incoming edge:
      \begin{enumerate}

      \item change its destination so it reaches the value and discard
        the node \fun{sum};

      \item evaluate the patched top tree and iterate.

      \end{enumerate}

    \end{enumerate}

  \item The only item remaining in the control stack is the result.

\end{enumerate}
Actually, we allowed integers to be stored in the control stack, so we could replace any tree which consists solely of a reference to such kind of value in the heap by a copy of the value.  We can see in \fig~\ref{fig:sum375_stack_push} that the control stack grows at every step until a value is reached.  \index{memory!heap|)} \index{memory!control stack|)}

\mypar{Tail call optimisation}
\index{memory!tail call optimisation|(}
\index{functional language!tail form|(}

Let us investigate what happens when using an equivalent definition in
tail form like \fun{sum\(_0\)/1} in \fig~\vref{fig:sum0}.
\Fig~\ref{fig:sum0_375} only shows the first phase, which consists in
\begin{figure}[!t]
\centering
\includegraphics[bb=71 630 356 721]{sum0_375_0}

\includegraphics[bb=71 626 352 721]{sum0_375_1}

\includegraphics[bb=71 636 394 721]{sum0_375_2}
\caption{\(\fun{sum}_0([3,7,5]) \twoheadrightarrow 15\)
without tail call optimisation\label{fig:sum0_375}}
\end{figure}
pushing in the control stack the tree newly produced by a rule and
sharing the subtrees denoted by variables (including aliases)
occurring both in the pattern and the right\hyp{}hand side. The second
phase consists in popping the accumulated roots in order to resume
suspended call contexts and, in the end, only the final result remains
in the control stack.

In the case of \fun{sum\(_0\)/1}, we notice that we already found the
result after the first phase:~\(15\). Therefore, in this case, the
second phase does not contribute to build the value, which raises the
question: why keep the previous trees in the first place?

Indeed, they are useless after each push and a common optimisation,
named \emph{tail call optimisation} and implemented by compilers of
functional languages, consists in popping the previous tree (matched
by the pattern of the rule) and pushing the new one (created by the
right\hyp{}hand side of the rule). This way, the control stack
contains only one item at all times. This optimisation is shown in
\fig~\vref{fig:sum0_375tco} and should be contrasted with the series
\begin{figure}[!t]
\centering
\includegraphics[bb=71 628 327 721]{sum0_375tco0}
\includegraphics[bb=71 628 350 721]{sum0_375tco1}
\includegraphics[bb=71 636 375 721]{sum0_375tco2}
\caption{\(\fun{sum}_0([3,7,5]) \twoheadrightarrow 15\) with tail call
  optimisation\label{fig:sum0_375tco}}
\end{figure}
in \fig~\vref{fig:sum0_375}.

\emph{Tail call optimisation can be applied to all functions in tail
  form.}

Let us visualise another example: the evaluation of
\(\fun{cat}([1,2],[3,4])\), using the definition in
\fig~\vref{fig:cat_dag}, which is not in tail form.
\begin{enumerate}

  \item The first phase, consisting in pushing the newly created trees
    in the control stack is shown in \fig~\vref{fig:cat1234_push}.

  \item The second phase of the evaluation of
    \(\fun{cat}([1,2],[3,4])\) is shown in
    \fig~\vref{fig:cat1234_pop}. It consists in replacing from right
    to left the reference to the previous call by the current
    (reference to a) value, until the initial call itself is removed
    and only the final result remains. Notice how the second argument,
    \([3,4]\), is actually shared with the output \([1,2,3,4]\) and
    how no optimisation is possible.
\end{enumerate}
\begin{figure}[!t]
\centering
\subfloat[End of phase 1/2\label{fig:cat1234_push}]{
\includegraphics[bb=71 615 287 734]{cat1234_push}
%\caption{Evaluation of \(\fun{cat}([1,2],[3,4])\) (end of phase 1/2)
%\label{fig:cat1234_push}}
}
\vskip32pt
%\end{figure}
%\begin{figure}
%\centering
\subfloat[Phase 2/2\label{fig:cat1234_pop}]{
\begin{minipage}[l]{0.9\textwidth}
\centering
\includegraphics[bb=71 613 287 714]{cat1234_pop0}
\vskip4.8pt
\includegraphics[bb=71 612 287 724]{cat1234_pop1}
\vskip5pt
\includegraphics[bb=71 612 287 722]{cat1234_pop2}
\end{minipage}
}
\caption{\(\fun{cat}([1,2],[3,4]) \twoheadrightarrow [1,2,3,4]\)} %(phase 2/2)
%\label{fig:cat1234_pop}}
\end{figure}
\index{memory!tail call optimisation|)}

\vspace*{-5mm}

\mypar{Transformation to tail form}
\label{sec:into_tail_form}
\index{stack!flattening!tail form|(}

Our definition of \fun{flat/1} with lifting in \fig~\vref{fig:flat},
is easily translated to \Erlang as follows: \verbatiminput{flat.def}
It is almost in tail form: only the last rule has a call with a
non\hyp{}empty context. By adding an accumulator, this definition can
be transformed into an equivalent one in tail form. The purpose of
this accumulator is to store the variables which occur in the
instances of the context, so these can be rebuilt and computed after
the current call is. So let us add a stack accumulator~\erlcode{A},
unchanged in every clause, and add a new \erlcode{flat\_tf/1}
definition calling the new \erlcode{flat/2} with the initial value of
the accumulator set to the empty stack:
\begin{alltt}
\textbf{flat\_tf(T)        \(\xrightarrow{\smash{\alpha}}\) flat(T,[]).}
flat(       [],\textbf{A}) \(\xrightarrow{\smash{\beta}}\) [];\hfill% A \emph{unused yet}
flat(   [[]|T],\textbf{A}) \(\xrightarrow{\smash{\gamma}}\) flat(T,\textbf{A});
flat([[X|S]|T],\textbf{A}) \(\xrightarrow{\smash{\delta}}\) flat([X,S|T],\textbf{A});
flat(    [X|T],\textbf{A}) \(\xrightarrow{\smash{\epsilon}}\) [X|flat(T,\textbf{A})].
\end{alltt}
Now we must accumulate a value at each call which is not in tail form
(here, clause~\clause{\epsilon}), and use the contents of the
accumulator in all clauses where there is no call (here,
clause~\clause{\alpha}). The technique consists in accumulating in
clause~\clause{\epsilon} the values occurring in the call context,
\erlcode{[X|\textvisiblespace]}; in other words, we
push~\erlcode{X} onto~\erlcode{A}:
\begin{alltt}
flat\_tf(T)        \(\smashedrightarrow{\alpha}\) flat(T,[]).
flat(       [],A) \(\smashedrightarrow{\beta}\) [];
flat(   [[]|T],A) \(\smashedrightarrow{\gamma}\) flat(T,A);
flat([[X|S]|T],A) \(\smashedrightarrow{\delta}\) flat([X,S|T],A);
flat(    [X|T],A) \(\smashedrightarrow{\epsilon}\) flat(T,\textbf{[X|}A\textbf{]}).\hfill% \emph{Here}
\end{alltt}
When the input is fully consummated, in clause~\clause{\beta}, the
accumulator contains all the non\hyp{}stack items (all the
\erlcode{X}s from clause~\clause{\epsilon}) in the reverse order of the
original stack; therefore, they need to be reversed. That is to say:
\begin{alltt}
flat\_tf(T)        \(\smashedrightarrow{\alpha}\) flat(T,[]).
flat(       [],A) \(\smashedrightarrow{\beta}\) \textbf{rev(A)};
flat(   [[]|T],A) \(\smashedrightarrow{\gamma}\) flat(T,A);
flat([[X|S]|T],A) \(\smashedrightarrow{\delta}\) flat([X,S|T],A);
flat(    [X|T],A) \(\smashedrightarrow{\epsilon}\) flat(T,[X|A]).
\end{alltt}
The definition is now completed and in tail form. What about
\erlcode{flat0/1} in \fig~\vref{fig:flat0}?  \verbatiminput{flat0.def}
That definition has the peculiarity that some of its clauses contain
two or more calls --~let us not forget that a call being recursive or
not has nothing to do, in general, with being in tail form or not.

Let us start by adding the accumulative parameter to \erlcode{flat0/1}
and initialise it with the empty stack:
\begin{alltt}
\textbf{flat0\_tf(T)          \(\smashedrightarrow{\alpha}\) flat0(T,[]).}\hfill% \emph{Added}
flat0(         [],\textbf{A}) \(\smashedrightarrow{\gamma}\) [];\hfill% A \emph{unused yet}
flat0(     [[]|T],\textbf{A}) \(\smashedrightarrow{\delta}\) flat0(T,\textbf{A});
flat0([Y=[\_|\_]|T],\textbf{A}) \(\smashedrightarrow{\epsilon}\) cat(flat0(Y,\textbf{A}),flat0(T,\textbf{A}));
flat0(      [Y|T],\textbf{A}) \(\smashedrightarrow{\zeta}\) [Y|flat0(T,\textbf{A})].
cat(   [],T)         \(\smashedrightarrow{\eta}\) T;
cat([X|S],T)         \(\smashedrightarrow{\theta}\) [X|cat(S,T)].
\end{alltt}
Let us decide that, in clause~\clause{\epsilon}, the first call to be
rewritten is the leftmost recursive call \erlcode{flat0(Y,A)}, whose
context is \erlcode{cat(\textvisiblespace,flat0(T,A))}. Therefore, in
clause~\clause{\epsilon}, let us save~\erlcode{T} in~\erlcode{A} so we
can reconstruct the context in the right\hyp{}hand side
of~\clause{\gamma}, where the current stack to process is empty and
thus the stacks saved in the accumulator enable resuming the
flattening:
\begin{alltt}
flat0\_tf(T)              \(\smashedrightarrow{\alpha}\) flat0(T,[]).
flat0(         [],\textbf{[T|}A\textbf{]}) \(\smashedrightarrow{\gamma}\) \textbf{cat(}[]\textbf{,flat0(T,A))};\hfill% \emph{Used}
flat0(     [[]|T],    A) \(\smashedrightarrow{\delta}\) flat0(T,A);
flat0([Y=[\_|\_]|T],    A) \(\smashedrightarrow{\epsilon}\) flat0(Y,\textbf{[T}|A\textbf{]});\hfill% \emph{Saved}
flat0(      [Y|T],    A) \(\smashedrightarrow{\zeta}\) [Y|flat0(T,A)].
cat(   [],T)             \(\smashedrightarrow{\eta}\) T;
cat([X|S],T)             \(\smashedrightarrow{\theta}\) [X|cat(S,T)].
\end{alltt}
But a clause is now missing: what if the accumulator is empty?
Therefore, a clause~\clause{\beta} must be added before
clause~\clause{\gamma} to take care of this situation:
\begin{alltt}
flat0\_tf(T)              \(\smashedrightarrow{\alpha}\) flat0(T,[]).
\textbf{flat0(         [],   []) \(\smashedrightarrow{\beta}\) [];}
flat0(         [],[T|A]) \(\smashedrightarrow{\gamma}\) cat([],flat0(T,A));
flat0(     [[]|T],    A) \(\smashedrightarrow{\delta}\) flat0(T,A);
flat0([Y=[\_|\_]|T],    A) \(\smashedrightarrow{\epsilon}\) flat0(Y,[T|A]);
flat0(      [Y|T],    A) \(\smashedrightarrow{\zeta}\) [Y|flat0(T,A)].
cat(   [],T)             \(\smashedrightarrow{\eta}\) T;
cat([X|S],T)             \(\smashedrightarrow{\theta}\) [X|cat(S,T)].
\end{alltt}
We can simplify the right\hyp{}hand side of clause~\clause{\gamma}
because the definition of \erlcode{cat/2} has become useless:
\begin{alltt}
flat0\_tf(T)              \(\smashedrightarrow{\alpha}\) flat0(T,[]).
flat0(         [],   []) \(\smashedrightarrow{\beta}\) [];
flat0(         [],[T|A]) \(\smashedrightarrow{\gamma}\) \textbf{flat0(T,A)};\hfill% \emph{Simplified}
flat0(     [[]|T],    A) \(\smashedrightarrow{\delta}\) flat0(T,A);
flat0([Y=[\_|\_]|T],    A) \(\smashedrightarrow{\epsilon}\) flat0(Y,[T|A]);
flat0(      [Y|T],    A) \(\smashedrightarrow{\zeta}\) [Y|flat0(T,A)].
\end{alltt}
Clause~\clause{\zeta} is not in tail form. We cannot just
push~\erlcode{Y} onto the accumulator
\begin{alltt}
flat0(      [Y|T],    A) \(\smashedrightarrow{\zeta}\) flat0(T,\textbf{[Y|}A\textbf{]}).\hfill% \emph{Wrong}
\end{alltt}
because the latter contains stacks to be flattened later (see
clause~\clause{\epsilon}) and \erlcode{Y}~is not a stack --~this
modification would lead to a match failure just after
clause~\clause{\gamma} matches, because all patterns only match
stacks. What can we do? Perhaps the first idea which comes to the mind
is to add another accumulator to hold the non\hyp{}stack items,
like~\erlcode{Y}. Basically, this is exactly the same method as
before, except it applies to another accumulator, say~\erlcode{B}. Let
us first add~\erlcode{B} everywhere and initialise it with
\erlcode{[]}:
\begin{alltt}
flat0\_tf(T)                \(\smashedrightarrow{\alpha}\) flat0(T,[],\textbf{[]}).
flat0(         [],   [],\textbf{B}) \(\smashedrightarrow{\beta}\) [];\hfill% B \emph{unused yet}
flat0(         [],[T|A],\textbf{B}) \(\smashedrightarrow{\gamma}\) flat0(T,A,\textbf{B});
flat0(     [[]|T],    A,\textbf{B}) \(\smashedrightarrow{\delta}\) flat0(T,A,\textbf{B});
flat0([Y=[\_|\_]|T],    A,\textbf{B}) \(\smashedrightarrow{\epsilon}\) flat0(Y,[T|A],\textbf{B});
flat0(      [Y|T],    A,\textbf{B}) \(\smashedrightarrow{\zeta}\) [Y|flat0(T,A,\textbf{B})].
\end{alltt}
Now we can save the variables of the call context in
clause~\clause{\zeta} in~\erlcode{B} and erase the context in
question. In clause~\clause{\beta}, we know that \erlcode{B}~contains
all the non\hyp{}stack items in reversed order, so we must
reverse~\erlcode{B}. Since clause~\clause{\beta} contained no further
calls, this is the end:
\begin{alltt}
flat0\_tf(T)                \(\smashedrightarrow{\alpha}\) flat0(T,[],[]).
flat0(         [],   [],B) \(\smashedrightarrow{\beta}\) \textbf{rev(B)};
flat0(         [],[T|A],B) \(\smashedrightarrow{\gamma}\) flat0(T,A,B);
flat0(     [[]|T],    A,B) \(\smashedrightarrow{\delta}\) flat0(T,A,B);
flat0([Y=[\_|\_]|T],    A,B) \(\smashedrightarrow{\epsilon}\) flat0(Y,[T|A],B);
flat0(      [Y|T],    A,B) \(\smashedrightarrow{\zeta}\) flat0(T,A,\textbf{[Y|B]}).
\end{alltt}
Further examination can lead to a simpler program, where the patterns
do not match embedded stacks:
\begin{alltt}
flat0\_tf(T)       -> flat0(T,[],[]).
flat0(   [],[],B) -> rev(B);
flat0(   [], A,B) -> flat0(A,   [],    B);
flat0(  [Y], A,B) -> flat0(Y,    A,    B);\hfill% \emph{Optimisation}
flat0([Y|T], A,B) -> flat0(Y,[T|A],    B);
flat0(    Y, A,B) -> flat0(A,   [],[Y|B]).
\end{alltt}
The shortcoming of this approach is that it requires many accumulators
in general and it is \emph{ad hoc}. Instead of adding one more
accumulator to solve our problem, we can stick to only one but make
sure that values in it are distinguished according to their origin, so
a value from a given context is not confused with a value from another
context. (This was previously implemented by using different
accumulators for different context values.) The way of achieving this
with only one accumulator consists in putting in a tuple the values of
a given context together with an atom, which plays the role of a tag
identifying the original expression containing the call. Let us
backtrack to
\begin{alltt}
flat0\_tf(T)          \(\smashedrightarrow{\alpha}\) flat0(T,[]).
flat0(         [],A) \(\smashedrightarrow{\gamma}\) [];\hfill% A \emph{unused yet}
flat0(     [[]|T],A) \(\smashedrightarrow{\delta}\) flat0(T,A);
flat0([Y=[\_|\_]|T],A) \(\smashedrightarrow{\epsilon}\) cat(flat0(Y,A),flat0(T,A));
flat0(      [Y|T],A) \(\smashedrightarrow{\zeta}\) [Y|flat0(T,A)].
cat(   [],T)         \(\smashedrightarrow{\eta}\) T;
cat([X|S],T)         \(\smashedrightarrow{\theta}\) [X|cat(S,T)].
\end{alltt}
Let us modify clause~\clause{\epsilon} by choosing, as before,
\erlcode{flat0(Y,A)} as the first call to be rewritten. We choose the
atom~\erlcode{k1} to represent that call and we pair it with the sole
value of its context, that is,~\erlcode{T}. We remove the context
\erlcode{cat(\textvisiblespace,flat0(T,A))} and, in the remaining
call, we push~\erlcode{\{k1,T\}} onto the accumulator~\erlcode{A}:
\begin{alltt}
flat0\_tf(T)          \(\smashedrightarrow{\alpha}\) flat0(T,[]).
flat0(         [],A) \(\smashedrightarrow{\gamma}\) [];\hfill% A \emph{unused yet}
flat0(     [[]|T],A) \(\smashedrightarrow{\delta}\) flat0(T,A);
flat0([Y=[\_|\_]|T],A) \(\smashedrightarrow{\epsilon}\) \textbf{flat0(Y,[\{k1,T\}|A])};
flat0(      [Y|T],A) \(\smashedrightarrow{\zeta}\) [Y|flat0(T,A)].
cat(   [],T)         \(\smashedrightarrow{\eta}\) T;
cat([X|S],T)         \(\smashedrightarrow{\theta}\) [X|cat(S,T)].
\end{alltt}
The key point is that \erlcode{k1}~must not be pushed in this
accumulator anywhere else in the program, because it must denote
unambiguously the call in clause~\clause{\epsilon}. Of course, this
program is not correct anymore, as the erased context must be
reconstructed somewhere else and applied to the value of the call
\erlcode{flat0(Y,[\{k1,T\}|A])}. The accumulator~\erlcode{A}
represents, as before, the values of the contexts. Where should we
extract its contents? Clause~\clause{\gamma} does not make any use
of~\erlcode{A} and this is our cue. It means that, at that point,
there are no more stacks to be flattened, so this is the right moment
to wonder if there is some work left to be done, that is, examine the
contents of~\erlcode{A}. In order to implement this task, a dedicated
function should be created, say~\erlcode{appk/2}, so
that~\erlcode{appk(\(V\),\(A\))} will compute whatever remains to be
done with what is found in the accumulator~\(A\), the value~\(V\)
being a partial result, that is, the result up to this point. If there
is nothing left to do, that is, if \(A\)~is empty, then
\erlcode{appk(\(V\),\(A\))} rewrites into~\(V\) and this is it. In
other words:
\begin{alltt}
appk(V,[\{k1,T\}|A]) \(\smashedrightarrow{\kappa}\) \fbcode{cat(\fbcode{HHHHH},flat0(T,A))}\,;
appk(V,        []) \(\smashedrightarrow{\iota}\) V.\hfill% \emph{The end}
\end{alltt}
The empty box must be filled with the reconstruction of the context
which was erased at the point where~\erlcode{k1} was saved in the
accumulator. The context in question was
\erlcode{cat(\textvisiblespace,flat0(T,A))}, in
clause~\clause{\epsilon}, so we have
\begin{alltt}
appk(V,[\{k1,T\}|A]) \(\smashedrightarrow{\kappa}\) cat(\fbcode{HHHHH},flat0(T,A));
appk(V,        []) \(\smashedrightarrow{\iota}\) V.
\end{alltt}
The remaining empty box is meant to be filled with the result of the
function call \erlcode{flat0(Y,[\{k1,T\}|A])}. To make this happen,
two conditions must be fulfilled. Firstly, the accumulator in the
pattern of \erlcode{appk/2} must be the same as at the moment of the
call, that is, it must be matched by \erlcode{[\{k1,T\}|A]}. In
theory, we should prove that the two occurrences of~\erlcode{A} indeed
denote the same value, but this would lead us astray. Finally, we need
to make sure that when a call to \erlcode{flat0/2} is over, a call to
\erlcode{appk/2} is made with the result. When the whole
transformation into tail form will be completed, no context will be
found anymore (by definition), so all calls to \erlcode{flat0/2} will
end in clauses whose right\hyp{}hand sides do not contain any further
call to be processed. A quick examination of the clauses reveals that
clause~\clause{\gamma} is the only clause of concern and that
\erlcode{A}~was unused yet. So let us replace the right\hyp{}hand side
of this clause with a call to \erlcode{appk/2}, whose first argument is
the result of the current call to \erlcode{flat0/2}, that is, the
current right\hyp{}hand side, and whose second argument is the
accumulator which may contain more information about contexts to be
rebuilt and applied. We have
\begin{alltt}
flat0(       [],A) \(\smashedrightarrow{\gamma}\) \textbf{appk(}[],\textbf{A)};
\end{alltt}
Now we understand that~\erlcode{V} in clause~\clause{\kappa} is the
value of the function call \erlcode{flat0(Y,[\{k1,T\}|A])}, so we can
proceed by plugging~\erlcode{V} into the frame of
clause~\clause{\kappa}:
\begin{alltt}
appk(V,[\{k1,T\}|A]) \(\smashedrightarrow{\kappa}\) cat(\textbf{V},flat0(T,A));
\end{alltt}
A glance is enough to realise that clause~\clause{\kappa} is not in tail
form. Therefore, let us repeat the same method. The first call that
must be rewritten is~\erlcode{flat0(T,A)}, whose context is
\erlcode{cat(V,\textvisiblespace)}. Let us associate the
variable~\erlcode{V} in the latter with a new atom~\erlcode{k2} and
push the two of them onto the accumulator:
\begin{alltt}
appk(V,[\{k1,T\}|A]) \(\smashedrightarrow{\kappa}\) flat0(T,\textbf{[\{k2,V\}|}A\textbf{]});
\end{alltt}
We need a new clause for \erlcode{appk/2} which handles the
corresponding case, that is, when the value of the call has been found
and the context has to be reconstructed and resumed:
\begin{alltt}
flat0\_tf(T)          \(\smashedrightarrow{\alpha}\) flat0(T,[]).
flat0(         [],A) \(\smashedrightarrow{\gamma}\) appk([],A);
flat0(     [[]|T],A) \(\smashedrightarrow{\delta}\) flat0(T,A);
flat0([Y=[\_|\_]|T],A) \(\smashedrightarrow{\epsilon}\) flat0(Y,[\{k1,T\}|A]);
flat0(      [Y|T],A) \(\smashedrightarrow{\zeta}\) [Y|flat0(T,A)].
cat(   [],T)         \(\smashedrightarrow{\eta}\) T;
cat([X|S],T)         \(\smashedrightarrow{\theta}\) [X|cat(S,T)].
\textbf{appk(V,[\{k2,W\}|A])   \(\smashedrightarrow{\lambda}\) cat(W,V);}\hfill% A \emph{unused yet}
appk(V,[\{k1,T\}|A])   \(\smashedrightarrow{\kappa}\) flat0(T,[\{k2,V\}|A]);
appk(V,        [])   \(\smashedrightarrow{\iota}\) V.
\end{alltt}
Notice how, in clause~\clause{\lambda}, we renamed~\erlcode{V} (in the
accumulator) into~\erlcode{W}, so as to avoid a clash with the first
argument of \erlcode{appk/2}. Also, why is it \erlcode{cat(W,V)} and
not \erlcode{cat(V,W)}? The reason is found by recollecting that \erlcode{W}~denotes the value of the call \erlcode{flat0(Y)} (in the
original definition), whereas \erlcode{V}~represents the value of
\erlcode{flat0(T)} (in the original definition). Nothing is done yet
with the rest of the accumulator~\erlcode{A}, which entails that we
must pass it to \erlcode{cat/2}, just like the other functions:
\begin{alltt}
flat0\_tf(T)          \(\smashedrightarrow{\alpha}\) flat0(T,[]).
flat0(         [],A) \(\smashedrightarrow{\gamma}\) appk([],A);
flat0(     [[]|T],A) \(\smashedrightarrow{\delta}\) flat0(T,A);
flat0([Y=[\_|\_]|T],A) \(\smashedrightarrow{\epsilon}\) flat0(Y,[\{k1,T\}|A]);
flat0(      [Y|T],A) \(\smashedrightarrow{\zeta}\) [Y|flat0(T,A)].
cat(   [],T,\textbf{A})       \(\smashedrightarrow{\eta}\) T;\hfill% A \emph{unused yet}
cat([X|S],T,\textbf{A})       \(\smashedrightarrow{\theta}\) [X|cat(S,T,\textbf{A})].
appk(V,[\{k2,W\}|A])   \(\smashedrightarrow{\lambda}\) cat(W,V,\textbf{A});\hfill% \emph{Passed} A
appk(V,[\{k1,T\}|A])   \(\smashedrightarrow{\kappa}\) flat0(T,[\{k2,V\}|A]);
appk(V,        [])   \(\smashedrightarrow{\iota}\) V.
\end{alltt}
After clause~\clause{\epsilon}, the first clause not being in tail form is
clause~\clause{\zeta}. Let us pair the variable~\erlcode{Y} of the
context \erlcode{[Y|\textvisiblespace]} with a new atom~\erlcode{k3},
and let us save the pair into the accumulator, while reconstructing
the erased context in a new clause~\clause{\mu} of \erlcode{appk/2}:
\begin{alltt}
flat0\_tf(T)          \(\smashedrightarrow{\alpha}\) flat0(T,[]).
flat0(         [],A) \(\smashedrightarrow{\gamma}\) appk([],A);
flat0(     [[]|T],A) \(\smashedrightarrow{\delta}\) flat0(T,A);
flat0([Y=[\_|\_]|T],A) \(\smashedrightarrow{\epsilon}\) flat0(Y,[\{k1,T\}|A]);
flat0(      [Y|T],A) \(\smashedrightarrow{\zeta}\) flat0(T,\textbf{[\{k3,Y\}|}A\textbf{]}).\hfill% Y \emph{saved}
cat(   [],T,\textbf{A})       \(\smashedrightarrow{\eta}\) T;\hfill% A \emph{unused yet}
cat([X|S],T,\textbf{A})       \(\smashedrightarrow{\theta}\) [X|cat(S,T,\textbf{A})].
\textbf{appk(V,[\{k3,Y\}|A])   \(\smashedrightarrow{\mu}\) [Y|V];}\hfill% A \emph{unused yet}
appk(V,[\{k2,W\}|A])   \(\smashedrightarrow{\lambda}\) cat(W,V,A);
appk(V,[\{k1,T\}|A])   \(\smashedrightarrow{\kappa}\) flat0(T,[\{k2,V\}|A]);
appk(V,        [])   \(\smashedrightarrow{\iota}\) V.
\end{alltt}
Something interesting happens here: the brand\hyp{}new right\hyp{}hand
side of clause~\clause{\mu} makes no use of the remaining
accumulator~\erlcode{A}. We encountered the exact same situation
with~\clause{\gamma}: a right\hyp{}hand side containing no further
calls. In this case, we need to check whether there is more work to be
done with the data saved earlier in~\erlcode{A}. This is the very aim
of \erlcode{appk/2}, therefore a call to it must be set within the
right\hyp{}hand side of clause~\clause{\mu}:
\begin{alltt}
flat0\_tf(T)          \(\smashedrightarrow{\alpha}\) flat0(T,[]).
flat0(         [],A) \(\smashedrightarrow{\gamma}\) appk([],A);
flat0(     [[]|T],A) \(\smashedrightarrow{\delta}\) flat0(T,A);
flat0([Y=[\_|\_]|T],A) \(\smashedrightarrow{\epsilon}\) flat0(Y,[\{k1,T\}|A]);
flat0(      [Y|T],A) \(\smashedrightarrow{\zeta}\) flat0(T,[\{k3,Y\}|A]).
cat(   [],T,A)       \(\smashedrightarrow{\eta}\) T;\hfill% A \emph{unused yet}
cat([X|S],T,A)       \(\smashedrightarrow{\theta}\) [X|cat(S,T,A)].
appk(V,[\{k3,Y\}|A])   \(\smashedrightarrow{\mu}\) \textbf{appk(}[Y|V],\textbf{A)};
appk(V,[\{k2,W\}|A])   \(\smashedrightarrow{\lambda}\) cat(W,V,A);
appk(V,[\{k1,T\}|A])   \(\smashedrightarrow{\kappa}\) flat0(T,[\{k2,V\}|A]);
appk(V,        [])   \(\smashedrightarrow{\iota}\) V.
\end{alltt}
The next clause to be considered is clause~\clause{\eta}, because its
right\hyp{}hand side contains no calls, so it requires now a call to
\erlcode{appk/2} with the right\hyp{}hand side~\erlcode{T}, which is
the result of the current call to \erlcode{flat0/2}, and the
accumulator, that is,~\erlcode{A}:
\begin{alltt}
flat0\_tf(T)          \(\smashedrightarrow{\alpha}\) flat0(T,[]).
flat0(         [],A) \(\smashedrightarrow{\gamma}\) appk([],A);
flat0(     [[]|T],A) \(\smashedrightarrow{\delta}\) flat0(T,A);
flat0([Y=[\_|\_]|T],A) \(\smashedrightarrow{\epsilon}\) flat0(Y,[\{k1,T\}|A]);
flat0(      [Y|T],A) \(\smashedrightarrow{\zeta}\) flat0(T,[\{k3,Y\}|A]).
cat(   [],T,A)       \(\smashedrightarrow{\eta}\) \textbf{appk(}T,\textbf{A)};
cat([X|S],T,A)       \(\smashedrightarrow{\theta}\) [X|cat(S,T,A)].
appk(V,[\{k3,Y\}|A])   \(\smashedrightarrow{\mu}\) appk([Y|V],A);
appk(V,[\{k2,W\}|A])   \(\smashedrightarrow{\lambda}\) cat(W,V,A);
appk(V,[\{k1,T\}|A])   \(\smashedrightarrow{\kappa}\) flat0(T,[\{k2,V\}|A]);
appk(V,        [])   \(\smashedrightarrow{\iota}\) V.
\end{alltt}
Last but not least, clause~\clause{\theta} must be fixed as we did for
the other clauses not in tail form. Let us pick a new atom,
say,~\erlcode{k4}, and tuple it with the sole variable~\erlcode{Y} of
the context \erlcode{[Y|\textvisiblespace]} and push the resulting
pair onto the accumulator~\erlcode{A}. Dually, we need to add a
clause~\clause{\nu} to \erlcode{appk/2} to catch this case, rebuild the
erased context and apply it to the result of the current call to
\erlcode{flat0/2}, that is, its first argument:
\begin{alltt}
flat0\_tf(T)          \(\smashedrightarrow{\alpha}\) flat0(T,[]).
flat0(         [],A) \(\smashedrightarrow{\gamma}\) appk([],A);
flat0(     [[]|T],A) \(\smashedrightarrow{\delta}\) flat0(T,A);
flat0([Y=[\_|\_]|T],A) \(\smashedrightarrow{\epsilon}\) flat0(Y,[\{k1,T\}|A]);
flat0(      [Y|T],A) \(\smashedrightarrow{\zeta}\) flat0(T,[\{k3,Y\}|A]).
cat(   [],T,A)       \(\smashedrightarrow{\eta}\) appk(T,A);
cat([X|S],T,A)       \(\smashedrightarrow{\theta}\) cat(S,T,\textbf{[X|}A\textbf{]}).
\textbf{appk(V,[\{k4,X\}|A])   \(\smashedrightarrow{\nu}\) [X|V];}\hfill% A \emph{unused yet}
appk(V,[\{k3,Y\}|A])   \(\smashedrightarrow{\mu}\) appk([Y|V],A);
appk(V,[\{k2,W\}|A])   \(\smashedrightarrow{\lambda}\) cat(W,V,A);
appk(V,[\{k1,T\}|A])   \(\smashedrightarrow{\kappa}\) flat0(T,[\{k2,V\}|A]);
appk(V,        [])   \(\smashedrightarrow{\iota}\) V.
\end{alltt}
The right\hyp{}hand side of the newly created clause contains no
calls, so we must send it to \erlcode{appk/2} together with the rest
of the accumulator, in order to process any pending contexts:
\begin{alltt}
flat0\_tf(T)          \(\smashedrightarrow{\alpha}\) flat0(T,[]).
flat0(         [],A) \(\smashedrightarrow{\gamma}\) appk([],A);
flat0(     [[]|T],A) \(\smashedrightarrow{\delta}\) flat0(T,A);
flat0([Y=[\_|\_]|T],A) \(\smashedrightarrow{\epsilon}\) flat0(Y,[\{k1,T\}|A]);
flat0(      [Y|T],A) \(\smashedrightarrow{\zeta}\) flat0(T,[\{k3,Y\}|A]).
cat(   [],T,A)       \(\smashedrightarrow{\eta}\) appk(T,A);
cat([X|S],T,A)       \(\smashedrightarrow{\theta}\) cat(S,T,[\{k4,X\}|A]).
appk(V,[\{k4,X\}|A])   \(\smashedrightarrow{\nu}\) \textbf{appk(}[X|V],\textbf{A)};
appk(V,[\{k3,Y\}|A])   \(\smashedrightarrow{\mu}\) appk([Y|V],A);
appk(V,[\{k2,W\}|A])   \(\smashedrightarrow{\lambda}\) cat(W,V,A);
appk(V,[\{k1,T\}|A])   \(\smashedrightarrow{\kappa}\) flat0(T,[\{k2,V\}|A]);
appk(V,        [])   \(\smashedrightarrow{\iota}\) V.
\end{alltt}
The transformation is now finished. It is correct in the sense that
the resulting program is equivalent to the original one, that is,
\erlcode{flat0/1} and \erlcode{flat0\_tf/1} compute the same values
from the same inputs, and all the clauses of the latter are in tail
form. It is also complete in the sense that any definition can be
transformed. As announced, the main interest of this method lies in
its uniformity and must not be expected to generate programs which are
faster than the originals.

It is possible, upon close examination, to shorten a bit the
definition of \erlcode{appk/2}. Indeed, clauses
\clause{\nu}~and~\clause{\mu} are identical, if not the presence of a
different tag, \erlcode{k4}~versus~\erlcode{k3}. Let us fuse them into
a single clause and use a new atom~\erlcode{k34} instead of every
occurrence of \erlcode{k3}~and~\erlcode{k4}.
\begin{alltt}
flat0\_tf(T)          \(\smashedrightarrow{\alpha}\) flat0(T,[]).
flat0(         [],A) \(\smashedrightarrow{\gamma}\) appk([],A);
flat0(     [[]|T],A) \(\smashedrightarrow{\delta}\) flat0(T,A);
flat0([Y=[\_|\_]|T],A) \(\smashedrightarrow{\epsilon}\) flat0(Y,[\{k1,T\}|A]);
flat0(      [Y|T],A) \(\smashedrightarrow{\zeta}\) flat0(T,[\{\textbf{k34},Y\}|A]).
cat(   [],T,A)       \(\smashedrightarrow{\eta}\) appk(T,A);
cat([X|S],T,A)       \(\smashedrightarrow{\theta}\) cat(S,T,[\{\textbf{k34},X\}|A]).
appk(V,[\{\textbf{k34},Y\}|A])  \(\smashedrightarrow{\mu}\) appk([Y|V],A);
appk(V, [\{k2,W\}|A])  \(\smashedrightarrow{\lambda}\) cat(W,V,A);
appk(V, [\{k1,T\}|A])  \(\smashedrightarrow{\kappa}\) flat0(T,[\{k2,V\}|A]);
appk(V,         [])  \(\smashedrightarrow{\iota}\) V.
\end{alltt}
Let us make a short digression and transform \erlcode{flat0\_tf/1}
further so that \erlcode{flat0\_tf(\(T\))} is rewritten into a pair
made of the value of \erlcode{flat0(\(T\))} and its cost. Because the
definition is initially in tail form, we just have to add a counter
and increment it where the clause corresponds to a clause in the original
definition, else the counter is left unchanged. We also have to add a
clause to set the first value of the counter. Let us recall first the
original definition of \erlcode{flat0/1} (we rename the arrows here to
ease the forthcoming steps):
\begin{alltt}
flat0(         []) \(\smashedrightarrow{\gamma}\) [];
flat0(     [[]|T]) \(\smashedrightarrow{\delta}\) flat0(T);
flat0([Y=[\_|\_]|T]) \(\smashedrightarrow{\epsilon}\) cat(flat0(Y),flat0(T));
flat0(      [Y|T]) \(\smashedrightarrow{\zeta}\) [Y|flat0(T)].
cat(   [],T)       \(\smashedrightarrow{\eta}\) T;
cat([X|S],T)       \(\smashedrightarrow{\theta}\) [X|cat(S,T)].
\end{alltt}
Let us identify and name identically in the tail form version
\erlcode{flat0\_tf/1} the clauses that have their counterpart in the
definition of \erlcode{flat0/1}:
\begin{alltt}
flat0\_tf(T)          \(\smashedrightarrow{\phantom{\gamma}}\) flat0(T,[]).
flat0(       [],A)   \(\smashedrightarrow{\gamma}\) appk([],A);
flat0(   [[]|T],A)   \(\smashedrightarrow{\delta}\) flat0(T,A);
flat0([Y=[\_|\_]|T],A) \(\smashedrightarrow{\epsilon}\) flat0(Y,[\{k1,T\}|A]);
flat0(    [Y|T],A)   \(\smashedrightarrow{\zeta}\) flat0(T,[\{k34,Y\}|A]).
cat(   [],T,A)       \(\smashedrightarrow{\eta}\) appk(T,A);
cat([X|S],T,A)       \(\smashedrightarrow{\theta}\) cat(S,T,[\{k34,X\}|A]).
appk(V,[\{k34,Y\}|A])  \(\smashedrightarrow{\phantom{\theta}}\) appk([Y|V],A);
appk(V, [\{k2,W\}|A])  \(\smashedrightarrow{\phantom{\theta}}\) cat(W,V,A);
appk(V, [\{k1,T\}|A])  \(\smashedrightarrow{\phantom{\theta}}\) flat0(T,[\{k2,V\}|A]);
appk(V,         [])  \(\smashedrightarrow{\phantom{\theta}}\) V.
\end{alltt}
\index{stack!flattening!tail form|)}

Drawing from our practical understanding of the new, systematic
transformation, we can try to summarise it as follows.
\begin{enumerate*}

  \item Consider all the definitions involved, that is, the one of
    immediate concern, but also all which it depends upon;

  \item add a stack accumulator to all these definitions and add a
    definition setting the empty stack as the initial value of the
    accumulator;

  \item for each body made of a call in tail form, just pass the
    accumulator unchanged;

  \item replace each body containing no call by a call to a new
    function \erlcode{appk/2}, with the body expression and the
    accumulator unchanged;

  \item for each body not in tail form, including those
    of \erlcode{appk/2},
    \begin{enumerate*}

      \item identify or choose the first call to be evaluated;

      \item select all the values and variables in the call context
        which are parameters, except the accumulator, and group them
        in a tuple, together with a unique atom;

      \item replace the body in question with the call to be done
        first and pass to it the accumulator on top of which the tuple
        of the previous step has been pushed;

      \item \label{add_appk1} create a clause for \erlcode{appk/2}
        matching this case, whose body is the previously mentioned
        context;

      \item \label{add_appk2} replace the
        place\hyp{}holder~\erlcode{\textvisiblespace} in the context
        by the first argument of \erlcode{appk/2} and make sure that
        there is no clash of variables;

    \end{enumerate*}

  \item add the clause \erlcode{appk(V,[]) -> V} to \erlcode{appk/2}.

\end{enumerate*}
This algorithm is said to be \emph{global}, insofar as \emph{all} the steps must be achieved before a program equivalent to the original input is reached, because intermediary steps may not lead to correct definitions. It is possible to dynamically rearrange the order in which some steps are applied so the algorithm becomes \emph{incremental}, but it is probably not worth the complication (based on the analysis of the call graph).

\index{Fibonacci's function!tail form|(}
Let us apply the same methodological steps to another difficult
definition like that of the Fibonacci function~\erlcode{fib/1}:
\begin{alltt}
fib(0)            \(\smashedrightarrow{\beta}\) 1;
fib(1)            \(\smashedrightarrow{\gamma}\) 1;
fib(N) when N > 1 \(\smashedrightarrow{\delta}\) fib(N-1) + fib(N-2).
\end{alltt}
The steps are as follows.
\begin{enumerate}

  \item This definition is self\hyp{}contained.

  \item Let us rename~\erlcode{fib/1} into \erlcode{fib/2}, then add a
    stack accumulator to it so it becomes \erlcode{fib/2}, next create
    a clause~\clause{\alpha} defining \erlcode{fib\_tf/1} as a single
    call to \erlcode{fib/2} where the initial value of the accumulator
    is the empty stack:
\begin{alltt}
\textbf{fib\_tf(N)           \(\smashedrightarrow{\alpha}\) fib(N,[]).}\hfill% \emph{New}
fib(0,\textbf{A})            \(\smashedrightarrow{\beta}\) 1;
fib(1,\textbf{A})            \(\smashedrightarrow{\gamma}\) 1;
fib(N,\textbf{A}) when N > 1 \(\smashedrightarrow{\delta}\) fib(N-1,\textbf{A}) + fib(N-2,\textbf{A}).
\end{alltt}

  \item There is no body in tail form which contains a call.

  \item Clauses \clause{\beta}~and~\clause{\gamma} are in tail form
    and contain no call, so we must replace the bodies with a call to
    function \erlcode{appk/2}, whose first argument is the original
    body (here, both are the value~\erlcode{1}) and the second
    argument is the accumulator unchanged:
\begin{alltt}
fib\_tf(N)           \(\smashedrightarrow{\alpha}\) fib(N,[]).
fib(0,A)            \(\smashedrightarrow{\beta}\) \textbf{appk(}1,\textbf{A)};
fib(1,A)            \(\smashedrightarrow{\gamma}\) \textbf{appk(}1,\textbf{A)};
fib(N,A) when N > 1 \(\smashedrightarrow{\delta}\) fib(N-1,A) + fib(N-2,A).
\end{alltt}

  \item Clause~\clause{\delta} is not in tail form and contains two
  calls, so we must choose which one we want to compute first. Let us
  arbitrarily choose the rightmost call, that is,
  \erlcode{fib(N-2,A)}. Therefore, its context is \erlcode{fib(N-1,A)
    + \textvisiblespace}. The values in the context, excluding the
  accumulator, are reduced to the sole value of~\erlcode{N}. Let us
  create a unique atom identifying this call,~\erlcode{k1}, and form
  the pair \erlcode{\{k1,N\}}.  Let us replace the body of
  clause~\clause{\delta} with
  \erlcode{fib(N-2,\textbf{[\{k1,N\}|A]})}. Next, let us create a
  clause for \erlcode{appk/2} matching this tuple. Its body is the
  context we just removed from the body of clause~\clause{\delta}. In
  it, let us fill the hole~\erlcode{\textvisiblespace} with the first
  parameter.
\begin{alltt}
fib\_tf(N)           \(\smashedrightarrow{\alpha}\) fib(N,[]).
fib(0,A)            \(\smashedrightarrow{\beta}\) appk(1,A);
fib(1,A)            \(\smashedrightarrow{\gamma}\) appk(1,A);
fib(N,A) when N > 1 \(\smashedrightarrow{\delta}\) fib(N-2,\textbf{[\{k1,N\}|A]}).
\textbf{appk(V,[\{k1,N\}|A])  \(\smashedrightarrow{\epsilon}\) fib(N-1,A) + V.}
\end{alltt}

    The body of~\(\epsilon\) is not in tail form, as
    it contains a function call not located at the root of the
    abstract syntax tree. The context of this call
    is~\erlcode{\textvisiblespace{} + V} and all the values it
    contains are limited to the one denoted by the
    variable~\erlcode{V}. Let us generate a new unique
    atom~\erlcode{k2} and pair it with~\erlcode{V}. We then replace
    the body of clause~\clause{\epsilon} with the call to be computed
    first and we pass to it the accumulator~\erlcode{A} on top of
    which the pair has been pushed. We make a new clause of
    \erlcode{appk/2} matching this case and in its body we put the
    context we just mentioned. We substitute the first parameter to
    the place\hyp{}holder~\erlcode{\textvisiblespace}. We have
\begin{alltt}
\textbf{appk(V,[\{k2,W\}|A]) \(\smashedrightarrow{\zeta}\) V + W;}
appk(V,[\{k1,N\}|A]) \(\smashedrightarrow{\epsilon}\) fib(N-1,\textbf{[\{k2,V\}|A]}).
\end{alltt}
    Note that we carefully renamed the variable~\erlcode{V} in the
    accumulator into~\erlcode{W} in order to avoid a clash with the
    first parameter~\erlcode{V}. This new body \erlcode{V+W} is in
    tail form and contains no further function calls, so it must be
    embedded into a recursive call because the accumulator~\erlcode{A}
    may not be empty --~so further calls may be waiting. We
    pass~\erlcode{A} to that call. Finally, all the clauses are in
    tail form:
\begin{alltt}
fib\_tf(N)           \(\smashedrightarrow{\alpha}\) fib(N,[]).
fib(0,A)            \(\smashedrightarrow{\beta}\) appk(1,A);
fib(1,A)            \(\smashedrightarrow{\gamma}\) appk(1,A);
fib(N,A) when N > 1 \(\smashedrightarrow{\delta}\) fib(N-2,[\{k1,N\}|A]).
appk(V,[\{k2,W\}|A])  \(\smashedrightarrow{\zeta}\) \textbf{appk(}V+W,\textbf{A)};
appk(V,[\{k1,N\}|A])  \(\smashedrightarrow{\epsilon}\) fib(N-1,[\{k2,V\}|A]).
\end{alltt}

  \item We must make sure to add a clause to match the case of the
    empty accumulator and rewrite to the first parameter:
\begin{alltt}
fib\_tf(N)           \(\smashedrightarrow{\alpha}\) fib(N,[]).
fib(0,A)            \(\smashedrightarrow{\beta}\) appk(1,A);
fib(1,A)            \(\smashedrightarrow{\gamma}\) appk(1,A);
fib(N,A) when N > 1 \(\smashedrightarrow{\delta}\) fib(N-2,[\{k1,N\}|A]).
\textbf{appk(V,        [])  \(\smashedrightarrow{\eta}\) V;}\hfill% \emph{Do not forget!}
appk(V,[\{k2,W\}|A])  \(\smashedrightarrow{\zeta}\) appk(V+W,A);
appk(V,[\{k1,N\}|A])  \(\smashedrightarrow{\epsilon}\) fib(N-1,[\{k2,V\}|A]).
\end{alltt}
\index{Fibonacci's function!tail form|)}
\index{stack!flattening!tail form|(}
Let us apply now our general method to \erlcode{flat/1}. Let us pick
up here:
\begin{alltt}
flat\_tf(T)        -> flat(T,[]).
flat(       [],A) -> [];\hfill% A \emph{unused yet}
flat(   [[]|T],A) -> flat(T,A);
flat([[X|S]|T],A) -> flat([X,S|T],A);
flat(    [Y|T],A) -> [Y|flat(T,A)].
\end{alltt}
The only body containing no calls is in the first clause of
\erlcode{flat/2}, so it must be passed to a call to \erlcode{appk/2},
together with the accumulator. Only the last body is not in tail
form. The only call to be performed has the context
\erlcode{[Y|\textvisiblespace]}, whose only values are reduced to the
sole~\erlcode{Y}. So we generate a unique atom~\erlcode{k1} and we
pair it with~\erlcode{Y}. We replace the body not in tail form with
the call to which we pass the accumulator on top of which the pair has
been pushed. We consequently create a clause for \erlcode{appk/2}
matching this case. Its body is the just erased context. The
hole~\erlcode{\textvisiblespace} is filled with the first parameter:
\begin{alltt}
flat\_tf(T)         -> flat(T,[]).
flat(       [],A)  -> \textbf{appk(}[],\textbf{A)};
flat(   [[]|T],A)  -> flat(T,A);
flat([[X|S]|T],A)  -> flat([X,S|T],A);
flat(    [Y|T],A)  -> \textbf{flat(T,[\{k1,Y\}|A])}.
\textbf{appk(V,[\{k1,Y\}|A]) -> [Y|V].}
\end{alltt}
Since the body of the newly created clause of \erlcode{appk/2} is a
value, it has to be wrapped into a recursive call because the
accumulator~\erlcode{A} may not be empty, so perhaps some more calls
have to be computed:
\begin{alltt}
flat\_tf(T)         -> flat(T,[]).
flat(       [],A)  -> appk([],A);
flat(   [[]|T],A)  -> flat(T,A);
flat([[X|S]|T],A)  -> flat([X,S|T],A);
flat(    [Y|T],A)  -> flat(T,[\{k1,Y\}|A]).
appk(V,[\{k1,Y\}|A]) -> \textbf{appk(}[Y|V],\textbf{A)}.
\end{alltt}
Finally, the definition of \erlcode{appk/2} must be completed by a
clause corresponding to the case when the accumulator is empty and its
body simply is the first argument, that is, by design, the result:
\begin{alltt}
flat\_tf(T)         -> flat(T,[]).
flat(       [],A)  -> appk([],A);
flat(   [[]|T],A)  -> flat(T,A);
flat([[X|S]|T],A)  -> flat([X,S|T],A);
flat(    [Y|T],A)  -> flat(T,[\{k1,Y\}|A]).
\textbf{appk(V,        []) -> V;}
appk(V,[\{k1,Y\}|A]) -> appk([Y|V],A).
\end{alltt}
If we compare this version with
\begin{verbatim}
flat_tf(T)        -> flat(T,[]).
flat(       [],A) -> rev(A);
flat(   [[]|T],A) -> flat(T,A);
flat([[X|S]|T],A) -> flat([X,S|T],A);
flat(    [Y|T],A) -> flat(T,[Y|A]).
\end{verbatim}
\index{stack!flattening!tail form|)}
we understand that the latter can be derived from the former if the
pair \erlcode{\{k1,Y\}} is replaced by~\erlcode{Y}. This is possible
because it is the only atom which was generated. The definition of
\erlcode{appk/2} then is equivalent to \erlcode{rcat/2}
(section~\vref{sec:reversal}):
\verbatiminput{rev.def}
\end{enumerate}
The philosophy underlying our general method to transform a given
group of definitions into an equivalent in tail form consists in
adding a parameter which is a stack accumulating the values of the
different contexts and creating a function (\erlcode{appk/2}) to
reconstruct these when the call they contained is over. These rebuilt
contexts are in turn transformed into tail form until all the clauses
are in tail form. As a result, the number of clauses is larger than in
the original source and the algorithm is obscured because of all the
administrative work about the accumulator. In order to save time and
efforts, it is wise to consider tail forms useful \emph{a posteriori},
when we run afoul of the maximum stack size because, except if very
large inputs are, from the design phase, likely. Another reason may be
to compile to low\hyp{}level \Clang (only using \texttt{goto} jumps).

Let us transform straight insertion (section~\ref{sec:straight_ins}
\vpageref{sec:straight_ins}) and analyse the cost of the resulting
definition. We start from
\begin{alltt}
isrt(   [])             \(\smashedrightarrow{\beta}\) [];
isrt([X|S])             \(\smashedrightarrow{\gamma}\) ins(isrt(S),X).
ins([Y|S],X) when X > Y \(\smashedrightarrow{\delta}\) [Y|ins(S,X)];
ins(    S,X)            \(\smashedrightarrow{\epsilon}\) [X|S].
\end{alltt}
(the clauses have been renamed) and we add a stack accumulator to our
functions and initialise it with the empty stack (new
clause~\clause{\alpha}):
\begin{alltt}
\textbf{isrt\_tf(S)                \(\smashedrightarrow{\alpha}\) isrt(S,[]).}
isrt(   [],\textbf{A})             \(\smashedrightarrow{\beta}\) [];\hfill% A \emph{unused yet}
isrt([X|S],\textbf{A})             \(\smashedrightarrow{\gamma}\) ins(isrt(S,\textbf{A}),X,\textbf{A}).
ins([Y|S],X,\textbf{A}) when X > Y \(\smashedrightarrow{\delta}\) [Y|ins(S,X,\textbf{A})];
ins(    S,X,\textbf{A})            \(\smashedrightarrow{\epsilon}\) [X|S].\hfill% A \emph{unused yet}
\end{alltt}
We can now inspect each clause and, depending on its body shape (that
is: expression in tail form, either with or without a call, or not in
tail form), some transformation is done. First, the body of
clause~\clause{\beta} is in tail form and does not contain any
function call. Thus, we transform it by calling the auxiliary
function \erlcode{appk/2}:
\begin{alltt}
isrt(   [],\textbf{A})             \(\smashedrightarrow{\beta}\) \textbf{appk(}[],\textbf{A)};
\end{alltt}
Next is clause~\clause{\gamma}, which is not in tail form. The first
call to be evaluated is \erlcode{isrt(S,A)}, whose control context is
\erlcode{ins(\textvisiblespace,X,A)}. Let us keep the call whilst
saving into the accumulator~\erlcode{A} the variable~\erlcode{X}
needed to rebuild the control context later, in a new clause of
function \erlcode{appk/2}. This variable needs a priori to be tagged
by some unique atom, say~\erlcode{k1}:
\begin{alltt}
isrt([X|S],\textbf{A})             \(\smashedrightarrow{\gamma}\) isrt(S,\textbf{[\{k1,X\}|A]}).
...
\textbf{appk(V,[\{k1,X\}|A])        \(\rightarrow\) ins(V,X,A).}
\end{alltt}
The following clause is~\clause{\delta}, which is not in tail
form. The only call to be evaluated is \erlcode{ins(S,X,A)}, whose
control context is \erlcode{[Y|\textvisiblespace]}. Let us
associate~\erlcode{Y} with a unique atom~\erlcode{k2}, then save both
of them in the accumulator~\erlcode{A} and, dually, add a clause to
\erlcode{appk/2} to reconstruct the erased control context:
\begin{alltt}
ins([Y|S],X,A) when X > Y \(\smashedrightarrow{\delta}\) ins(S,X,\textbf{[\{k2,Y\}|A]});
...
\textbf{appk(V,[\{k2,Y\}|A])        \(\rightarrow\) appk([Y|V],A);}
\end{alltt}
The last clause is~\clause{\epsilon}, which is in tail form and
contains no call, so we must pass its body to \erlcode{appk/2} in
order to check whether there are pending control contexts to rebuild
and evaluate:
\begin{alltt}
ins(    S,X,A)            \(\smashedrightarrow{\epsilon}\) \textbf{appk(}[X|S],\textbf{A)}.
\end{alltt}
In order to complete the transformation, we must add a clause
to \erlcode{appk/2} to process the case when the accumulator is empty,
so the final result is found. Finally, the resulting program is (last
step in bold typeface)\label{isrt_tf_appk}
\begin{alltt}
isrt\_tf(S)                \(\smashedrightarrow{\alpha}\) isrt(S,[]).
isrt(   [],A)             \(\smashedrightarrow{\beta}\) appk([],A);
isrt([X|S],A)             \(\smashedrightarrow{\gamma}\) isrt(S,[\{k1,X\}|A]).
ins([Y|S],X,A) when X > Y \(\smashedrightarrow{\delta}\) ins(S,X,[\{k2,Y\}|A]);
ins(    S,X,A)            \(\smashedrightarrow{\epsilon}\) appk([X|S],A).
\textbf{appk(V,        [])        \(\smashedrightarrow{\zeta}\) V;}
appk(V,[\{k2,Y\}|A])        \(\smashedrightarrow{\eta}\) appk([Y|V],A);
appk(V,[\{k1,X\}|A])        \(\smashedrightarrow{\theta}\) ins(V,X,A).
\end{alltt}
We can remark that the atom~\erlcode{k1} is not necessary in the
definition of \erlcode{isrt\_tf/1}, since all other values in the
accumulator are tagged~\erlcode{k2}:
\begin{alltt}
isrt\_tf(S)                \(\smashedrightarrow{\alpha}\) isrt(S,[]).
isrt(   [],A)             \(\smashedrightarrow{\beta}\) appk([],A);
isrt([X|S],A)             \(\smashedrightarrow{\gamma}\) isrt(S,\textbf{[X|A]}).\hfill% \emph{Here}
ins([Y|S],X,A) when X > Y \(\smashedrightarrow{\delta}\) ins(S,X,[\{k2,Y\}|A]);
ins(    S,X,A)            \(\smashedrightarrow{\epsilon}\) appk([X|S],A).
appk(V,        [])        \(\smashedrightarrow{\zeta}\) V;
appk(V,[\{k2,Y\}|A])        \(\smashedrightarrow{\eta}\) appk([Y|V],A);
appk(V,     \textbf{[X|A]})        \(\smashedrightarrow{\theta}\) ins(V,X,A).\hfill% \emph{and here}
\end{alltt}
It becomes obvious now that \erlcode{isrt/2} reverses its first
argument in the accumulator, which is initialised in
clause~\clause{\alpha} to the empty stack. Then, in
clause~\clause{\beta},
\erlcode{appk/2} is called with the same arguments. For instance,
\erlcode{isrt([3,8,2],[])}~\(\smashedrightarrow{3}\)
\erlcode{appk([],[2,8,3])}. Hence, we conclude that
\begin{equation*}
\erlcode{isrt(\(S\),[])} \equiv \erlcode{appk([],rev(\(S\)))},
\end{equation*}
which allows us to cut out the definition of \erlcode{isrt/2}
entirely as follows:
\begin{alltt}
isrt\_tf(S)                \(\smashedrightarrow{\alpha}\) \textbf{appk([],rev(S))}.
ins([Y|S],X,A) when X > Y \(\smashedrightarrow{\delta}\) ins(S,X,[\{k2,Y\}|A]);
ins(    S,X,A)            \(\smashedrightarrow{\epsilon}\) appk([X|S],A).
appk(V,        [])        \(\smashedrightarrow{\zeta}\) V;
appk(V,[\{k2,Y\}|A])        \(\smashedrightarrow{\eta}\) appk([Y|V],A);
appk(V,     [X|A])        \(\smashedrightarrow{\theta}\) ins(V,X,A).
\end{alltt}
We expect that sorting a stack or the same stack reversed is the same:
\begin{equation*}
\erlcode{isrt\_tf(\(S\))} \equiv \erlcode{isrt\_tf(rev(\(S\)))}.
\end{equation*}
By clause \clause{\alpha}, and remarking that \(\erlcode{rev(rev(\(S\)))} \equiv S\), we draw
\begin{equation*}
\erlcode{isrt\_tf(\(S\))} \equiv \erlcode{appk([],rev(rev(\(S\))))}
                          \equiv \erlcode{appk([],\(S\))}.
\end{equation*}
Therefore, we can simplify the body of clause~\clause{\alpha}:
\begin{alltt}
isrt\_tf(S)                \(\smashedrightarrow{\alpha}\) appk([],\textbf{S}).
ins([Y|S],X,A) when X > Y \(\smashedrightarrow{\delta}\) ins(S,X,[\{k2,Y\}|A]);
ins(    S,X,A)            \(\smashedrightarrow{\epsilon}\) appk([X|S],A).
appk(V,        [])        \(\smashedrightarrow{\zeta}\) V;
appk(V,[\{k2,Y\}|A])        \(\smashedrightarrow{\eta}\) appk([Y|V],A);
appk(V,     [X|A])        \(\smashedrightarrow{\theta}\) ins(V,X,A).
\end{alltt}
We can get a shorter program at the expense of more
comparisons. Remark that when clause~\clause{\eta} applies,
\erlcode{Y}~is lower than the head of~\erlcode{V}, which exists
because this clause is only used to compute the bodies of clauses~\clause{\epsilon} and~\clause{\eta}, where the first argument is not
the empty stack. Therefore,
\(\erlcode{appk([Y|V],A)} \equiv \erlcode{ins(V,Y,A)}\), because
clause~\clause{\epsilon} would apply. Accordingly, let us change
clause~\clause{\eta}:
\begin{alltt}
isrt\_tf(S)                \(\smashedrightarrow{\alpha}\) appk([],S).
ins([Y|S],X,A) when X > Y \(\smashedrightarrow{\delta}\) ins(S,X,[\{k2,Y\}|A]);
ins(    S,X,A)            \(\smashedrightarrow{\epsilon}\) appk([X|S],A).
appk(V,        [])        \(\smashedrightarrow{\zeta}\) V;
appk(V,[\{k2,Y\}|A])        \(\smashedrightarrow{\eta}\) \textbf{ins(V,Y,A)};
appk(V,     [X|A])        \(\smashedrightarrow{\theta}\) ins(V,X,A).
\end{alltt}
We can see clearly now that \erlcode{appk/2} calls \erlcode{ins/3}
in the same way in clauses \clause{\eta}~and~\clause{\theta}, which
means that it is useless to tag~\erlcode{Y} with~\erlcode{k2} and we
can get rid of clause~\clause{\theta} (\erlcode{Z} can either be a~\erlcode{X} or a~\erlcode{Y}):
\begin{alltt}
isrt\_tf(S)                \(\smashedrightarrow{\alpha}\) appk([],S).
ins([Y|S],X,A) when X > Y \(\smashedrightarrow{\delta}\) ins(S,[\textbf{Y}|A]);\hfill% \emph{Here}
ins(    S,X,A)            \(\smashedrightarrow{\epsilon}\) appk([X|S],A).
appk(V,   [])             \(\smashedrightarrow{\zeta}\) V;
appk(V,[\textbf{Z}|A])             \(\smashedrightarrow{\eta}\) ins(V,Z,A).\hfill% \emph{and here}
\end{alltt}
Perhaps it is clearer to get rid of \erlcode{appk/2} by integrating
its two operations in \erlcode{isrt\_tf/1} and
\erlcode{ins/3}. Let us split clauses
\clause{\alpha}~and~\clause{\epsilon} to manifest the cases where,
respectively, \erlcode{S}~and~\erlcode{A} are empty:
\newlength\duparrow \settowidth\duparrow{\(\alpha\sb{0}\)}
\begin{alltt}
isrt\_tf(   [])                \(\MyArrow{\duparrow}{\alpha\sb{0}}\) appk([],[]);
isrt\_tf([X|S])                \(\MyArrow{\duparrow}{\alpha\sb{1}}\) appk([],[X|S]).
ins([Y|S],X,    A) when X > Y \(\MyArrow{\duparrow}{\delta}\) ins(S,X,[Y|A]);
ins(    S,X,[Y|A])            \(\MyArrow{\duparrow}{\epsilon\sb{0}}\) appk([X|S],[Y|A]);
ins(    S,X,   [])            \(\MyArrow{\duparrow}{\epsilon\sb{1}}\) appk([X|S],[]).
appk(V,   [])                 \(\MyArrow{\duparrow}{\zeta}\) V;
appk(V,[Z|A])                 \(\MyArrow{\duparrow}{\eta}\) ins(V,Z,A).
\end{alltt}
We can now replace the bodies of clauses~\clause{\alpha_0} and~\clause{\epsilon_1} by their value, as given by clause~\clause{\zeta}, and we can remove~\clause{\zeta}:
\begin{alltt}
isrt\_tf(   [])                \(\MyArrow{\duparrow}{\alpha\sb{0}}\) \textbf{[]};
isrt\_tf([X|S])                \(\MyArrow{\duparrow}{\alpha\sb{1}}\) appk([],[X|S]).
ins([Y|S],X,    A) when X > Y \(\MyArrow{\duparrow}{\delta}\) ins(S,X,[Y|A]);
ins(    S,X,[Y|A])            \(\MyArrow{\duparrow}{\epsilon\sb{0}}\) appk([X|S],[Y|A]);
ins(    S,X,   [])            \(\MyArrow{\duparrow}{\epsilon\sb{1}}\) \textbf{[X|S]}.
appk(V,[Z|A])                 \(\MyArrow{\duparrow}{\eta}\) ins(V,Z,A).
\end{alltt}
We saved one rewrite in case the input stack is empty. Lastly, the
bodies of clauses \clause{\alpha_1}~and~\clause{\epsilon_0} can be
replaced by their value, as given by clause~\clause{\eta}, which can
be, finally, erased. We rename the accumulator~\erlcode{A} as~\erlcode{T}.
\begin{alltt}
isrt\_tf(   [])                \(\MyArrow{\duparrow}{\alpha\sb{0}}\) [];
isrt\_tf([X|S])                \(\MyArrow{\duparrow}{\alpha\sb{1}}\) \textbf{ins([],X,S)}.
ins([Y|S],X,    T) when X > Y \(\MyArrow{\duparrow}{\delta}\) ins(S,X,[Y|S]);
ins(    S,X,[Y|T])            \(\MyArrow{\duparrow}{\epsilon\sb{0}}\) \textbf{ins([X|S],Y,T)};
ins(    S,X,   [])            \(\MyArrow{\duparrow}{\epsilon\sb{1}}\) [X|S].
\end{alltt}
It is important to remember that these last steps, relative to the
removal of tag~\erlcode{k2} and so forth, make sense only because, in
assessing the cost, we take into account only the number of function
calls, not the number of comparisons, which is now greater for not
using the control context \erlcode{[Y|\textvisiblespace]} in the
original clause~\clause{\delta} of \erlcode{ins/3}. In other words,
the keys saved in the accumulator in the new clause~\clause{\delta}
have to be re\hyp{}inserted in clause~\clause{\epsilon_0}.

The same analysis used for assessing the cost of \erlcode{isrt/1}
applies here as well, except that the keys are inserted in their
original order. So when the keys are sorted increasingly, the cost
is here \emph{maximum} (that is, clause~\clause{\delta} is used
maximally) and when it is sorted non\hyp{}increasingly, the cost is
\emph{minimum} (that is, clause~\clause{\delta} is never used).
\emph{If keys are not repeated, the best case of \erlcode{isrt/1} is
  the worst case of \erlcode{isrt\_tf/1} and the worst case
  of \erlcode{isrt/1} is the best case of \erlcode{isrt\_tf/1}.}
This is true because `nondecreasing' means the same as `increasing'
when there is no repetition.

In order to find the minimum cost of the final version of
\erlcode{isrt\_tf/1}, it is helpful to get first a better
understanding of the computational process by unfolding a simple
example like sorting \erlcode{[4,3,2,1]}, which is a stack sorted in
decreasing order:
\begin{alltt}
isrt\_tf([4,3,2,1]) \(\MyArrow{\duparrow}{\alpha\sb{1}}\) ins(     [],4,[3,2,1])
                   \(\MyArrow{\duparrow}{\epsilon\sb{0}}\) ins(    [4],3,  [2,1])
                   \(\MyArrow{\duparrow}{\epsilon\sb{0}}\) ins(  [3,4],2,    [1])
                   \(\MyArrow{\duparrow}{\epsilon\sb{0}}\) ins([2,3,4],1,     [])
                   \(\MyArrow{\duparrow}{\epsilon\sb{1}}\) [1,2,3,4]\textrm{.}
\end{alltt}
Let us note \(\B{\fun{isrt\_tf}}{n}\) the minimum cost of sorting
\(n\)~keys. Then \(\B{\fun{isrt\_tf}}{0} = 1\), by
clause~\clause{\alpha_0}. Let us assume next that \(n>0\). Then
\begin{itemize*}

  \item clause~\clause{\alpha\sb{1}} is used once;

  \item clause~\clause{\delta} is not used, since we assume here that
    the keys are already sorted non\hyp{}increasingly;

  \item clause~\clause{\epsilon\sb{0}} is used once for each key in
    its third argument, which, by clause~\clause{\alpha\sb{1}}, means
    all keys except the first, that is \(n-1\) times;

  \item clause~\clause{\epsilon\sb{1}} is used once.

\end{itemize*}
In sum, the evaluation trace is \(\alpha_1\epsilon_0^{n-1}\epsilon_1\), so
the total cost is
\begin{equation*}
\B{\fun{isrt\_tf}}{n} = \len{\alpha_1\epsilon_0^{n-1}\epsilon_1} = n+1,
\end{equation*}
if~\(n>0\). Since we found that \(\B{\fun{isrt\_tf}}{0} = 1 = 0 + 1\),
we can extend the previous formula to \(n=0\). This result can be
related directly to \(\W{\fun{isrt}}{n} = (n^2 + 3n + 2)/2\), because
the best case of \erlcode{isrt\_tf/1} corresponds to the worst case of
\erlcode{isrt/1} when the keys are not repeated. We can further reckon
that this minimum cost for \erlcode{isrt\_tf/1} is also an absolute
minimum for a sorting algorithm when the input is sorted
non\hyp{}increasingly, because it is simply the cost needed to reverse
the input.

Let \(\W{\fun{isrt\_tf}}{n}\) be the maximum
cost of \erlcode{isrt\_tf(\(S\))},  where the stack~\(S\) contains
\(n\)~keys (in increasing order). For the empty stack, the evaluation
trace is~\(\alpha_0\). For singletons, for example, \erlcode{[5]}, it
is~\(\alpha_1\epsilon_1\). To understand the general case \(n>1\), we
can try
\begin{alltt}
isrt\_tf([1,2,3,4]) \(\MyArrow{\duparrow}{\alpha\sb{1}}\) ins(     [],1,[2,3,4])
                   \(\MyArrow{\duparrow}{\epsilon\sb{0}}\) ins(    [1],2,  [3,4])
                   \(\MyArrow{\duparrow}{\delta}\) ins(     [],2,[1,3,4])
                   \(\MyArrow{\duparrow}{\epsilon\sb{0}}\) ins(    [2],1,  [3,4])
                   \(\MyArrow{\duparrow}{\epsilon\sb{0}}\) ins(  [1,2],3,    [4])
                   \(\MyArrow{\duparrow}{\delta}\) ins(    [2],3,  [1,4])
                   \(\MyArrow{\duparrow}{\delta}\) ins(     [],3,[2,1,4])
                   \(\MyArrow{\duparrow}{\epsilon\sb{0}}\) ins(    [3],2,  [1,4])
                   \(\MyArrow{\duparrow}{\epsilon\sb{0}}\) ins(  [2,3],1,    [4])
                   \(\MyArrow{\duparrow}{\epsilon\sb{0}}\) ins([1,2,3],4,     [])
                   \(\MyArrow{\duparrow}{\delta}\) ins(  [2,3],4,    [1])
                   \(\MyArrow{\duparrow}{\delta}\) ins(    [3],4,  [2,1])
                   \(\MyArrow{\duparrow}{\delta}\) ins(     [],4,[3,2,1])
                   \(\MyArrow{\duparrow}{\epsilon\sb{0}}\) ins(    [4],3,  [2,1])
                   \(\MyArrow{\duparrow}{\epsilon\sb{0}}\) ins(  [3,4],2,    [1])
                   \(\MyArrow{\duparrow}{\epsilon\sb{0}}\) ins([2,3,4],1,     [])
                   \(\MyArrow{\duparrow}{\epsilon\sb{1}}\) [1,2,3,4]\textrm{.}
\end{alltt}
Notice the interplay of clauses~\clause{\delta}
and~\clause{\epsilon\sb{0}}. A series of applications of
clause~\clause{\delta} ends with the first argument to be the empty
stack. This is because the effect of clause~\clause{\delta} is to save
the contents of this argument by reversing it on top of the third
argument. In other words, in the worst case, clause~\clause{\delta} is
equivalent to
\begin{alltt}
ins([Y|S],X,T) when X > Y \(\rightarrow\) ins(\textbf{[]},X,\textbf{rcat(S,[Y|T])});
\end{alltt}
A sequence of~\clause{\delta} is followed by a series
of~\clause{\epsilon\sb{0}} \emph{of same length}, followed by another
\clause{\epsilon\sb{0}}~or~\clause{\epsilon\sb{1}}. The reason is that
clause~\clause{\epsilon\sb{0}} restores on top of the first argument
the keys saved previously by clause~\clause{\delta}. Then, if there
are some keys left in the last argument (to be sorted), one more
application of clause~\clause{\epsilon\sb{0}} is required, otherwise
the program ends with clause \clause{\epsilon\sb{1}}, that is, the
evaluation trace when \(n>1\) is
\begin{equation*}
\alpha_1\prod_{p=0}^{n-2}{\left(\delta^p\epsilon_0^{p+1}\right)} \cdot
\delta^{n-1}\epsilon_0^{n-1} \cdot \epsilon_1
= \alpha_1\prod_{p=0}^{n-2}{\left((\delta\epsilon_0)^p\epsilon_0\right)}
\cdot (\delta\epsilon_0)^{n-1} \cdot \epsilon_1.
\end{equation*}
This observation is the key for finding the maximum cost as it hints at counting the rewrite steps of clause~\clause{\delta} and of clause~\clause{\epsilon\sb{0}}
\emph{together}, as evinced in the right\hyp{}hand side of the
equality. We can now directly derive the maximum cost:
\begin{align*}
  \W{\fun{isrt\_tf}}{n}
  &= \left|\alpha_1\prod_{p=0}^{n-2}{\left((\delta\epsilon_0)^p\epsilon_0\right)}
     \cdot (\delta\epsilon_0)^{n-1} \cdot \epsilon_1\right|\\
  &= \len{\alpha_1} +
     \left|\prod_{p=0}^{n-2}{\left((\delta\epsilon_0)^p\epsilon_0\right)}\right|
     + \left|(\delta\epsilon_0)^{n-1}\right| + \len{\epsilon_1}\\
  &= 1 + \sum_{p=0}^{n-2}{\left|(\delta\epsilon_0)^p\epsilon_0\right|}
     + (n-1)\len{\delta\epsilon_0} + 1\\
\W{\fun{isrt\_tf}}{n}
  &= 1 + \sum_{p=0}^{n-2}{(2p+1)} + 2(n-1) + 1 = n^2 + 1.
\end{align*}
Since the worst case of \erlcode{isrt\_tf/1} and \erlcode{isrt/1} are
identical, we can compare their cost in this case,
for~\(n\geqslant{}0\):
\begin{equation*}
\W{\fun{isrt}}{n}     = (n^2 + 3n + 2)/2\quad\text{and}\quad
\W{\fun{isrt\_tf}}{n} = 2 \cdot \W{\fun{isrt}}{n} + 3n + 1.
\end{equation*}
Let us relate now the best and worst cases for \erlcode{isrt/1} and
\erlcode{isrt\_tf/1}. We have, for~\(n>3\),
\(\B{\smash[t]{\fun{isrt\_tf}}}{n} < \B{\fun{isrt}}{n} <
\W{\fun{isrt}}{n} < \W{\smash[t]{\fun{isrt\_tf}}}{n}\).  If we note
\(\C{\fun{isrt}}{n}\) the cost of \erlcode{isrt/1} on an input of
length~\(n\), these inequalities are equivalent to say
\(\B{\smash[t]{\fun{isrt\_tf}}}{n} < \C{\fun{isrt}}{n} <
\W{\smash[t]{\fun{isrt\_tf}}}{n}\).  This is the best we can do
because we only have the obvious inequalities
\(\B{\smash[t]{\fun{isrt\_tf}}}{n} \leqslant
\C{\smash[t]{\fun{isrt\_tf}}}{n} \leqslant
\W{\smash[t]{\fun{isrt\_tf}}}{n}\), which do not allow us to compare
\(\C{\fun{isrt}}{n}\) and \(\C{\smash[t]{\fun{isrt\_tf}}}{n}\). In
order to obtain a stronger result, we need an average cost analysis so
we can tell apart \erlcode{isrt\_tf/1} from \erlcode{isrt/1}. Indeed,
it might be that, for a given input stack of length~\(n\), most
configurations of the input lead to a cost for \erlcode{isrt\_tf/1}
which is actually lower than for \erlcode{isrt/1}. Let us note
\(\M{\smash[t]{\fun{isrt\_tf}}}{n}\) the average number of rewrites
needed to compute \erlcode{isrt\_tf}{(\(S\))}, where the length of
stack~\(S\) is~\(n\). Similarly, we note \(\M{\fun{ins}}{p,q}\) for
the average cost of the call \erlcode{ins(\(P\),\(X\),\(Q\))}, where
stack~\(P\) has length~\(p\) and stack~\(Q\) has length~\(q\). The
short story is this: because the keys are random, the average number
of times clause~\clause{\delta} is used is~\(p/2\). Since the aim of
clause \clause{\epsilon_0}, as observed before, is to put back the
keys previously moved by clause~\clause{\delta}, we expect, in
average, the same number \(p/2\), plus~\(1\), because
clause~\clause{\epsilon_0} also prepares the possible following use of
clause~\clause{\delta}. In other words, the difference with the
longest evaluation trace defining \(\W{\fun{isrt}}{n}\) is that the
subsequences~\(\delta\epsilon_0\) are expected to be 50\%~shorter in
average, so the evaluation trace is, in average,
\begin{equation*}
\alpha_1\prod_{p=0}^{n-2}{\left((\delta\epsilon_0)^{p/2}\epsilon_0\right)}
\cdot (\delta\epsilon_0)^{(n-1)/2} \cdot \epsilon_1,
\end{equation*}
from which we deduce the average cost for \(n>1\):
\begin{equation*}
  \M{\fun{isrt\_tf}}{n} = 1 +
  \sum_{p=0}^{n-2}{\left(2\cdot\frac{p}{2}+1\right)}
                       + \left(2\cdot\frac{n-1}{2}\right) + 1
                     = \frac{1}{2}{n^2} + \frac{1}{2}{n} + 1.
\end{equation*}
Elegantly, this formula extends to cope with~\(n=0,1\) and we can
compare now \(\M{\fun{isrt\_tf}}{n}\) to \(\M{\fun{isrt}}{n}\),
for~\(n\geqslant{}0\):
\begin{equation*}
\M{\fun{isrt\_tf}}{n}
  = \tfrac{1}{2}{n^2} + \tfrac{1}{2}{n} + 1
  \sim \tfrac{1}{2}{n^2} \sim 2 \cdot \M{\fun{isrt}}{n}.
\end{equation*}
In other words, \erlcode{isrt\_tf/1}, in spite of being optimised, is
nevertheless 50\%~slower than the original function, \emph{in average
  for large values of~\(n\)}. This should not be too surprising, as
a transformation to tail form should only be undertaken for the sake
of the control stack, not efficiency.
\index{functional language!tail form|)}

\paragraph{Exercise}

Consider the variation
{\small
\begin{verbatim}
isrt0(L)                            -> isrt0(    L,   [],   []).
isrt0(   [],   [],    Q)            -> Q;
isrt0(   [],[J|P],    Q)            -> isrt0(   [],    P,[J|Q]);
isrt0([I|L],    P,[K|Q]) when K > I -> isrt0([I|L],[K|P],    Q);
isrt0([I|L],[J|P],    Q) when I > J -> isrt0([I|L],    P,[J|Q]);
isrt0([I|L],    P,    Q)            -> isrt0(    L,    P,[I|Q]).
\end{verbatim}
}
Here, one rewrite involves moving exactly one key, so the cost of
\erlcode{isrt0/3} is the number of key movements to sort the original
stack. Analyse the minimum, maximum and average number of key
movements.

\paragraph{Light encoding of stack accumulators}
\index{stack!encoding with tuples|(}

The accumulators used to transform definitions into tail form are, in
their most general instance, stacks of tuples. While using a stack
brings to the fore the very nature of the accumulator, it incurs a
penalty in the size of the memory required because, in the abstract
syntax trees, a push corresponds to a node, just as a tuple. \emph{By
  nesting tuples in tuples, we can get rid of the stack altogether.}
For instance, instead of writing
\erlcode{[\{k3,\(X_1\)\},\{k1,\(V\),\(E\)\},\{k3,\(X_2\)\}]}, we would
write the nested tuples
\erlcode{\{k3,\(X_1\),\{k1,\(V\),\(E\),\{k3,\(X_2\),\{\}\}\}\}}. Both
abstract syntax trees are easily compared in
\fig~\ref{fig:tuple_vs_stack}.
\begin{figure}
\centering
\subfloat[With a stack of tuples\label{fig:acc_stack}]{
  \includegraphics[bb=72 642 220 721]{acc_stack}
}
\subfloat[With nested tuples\label{fig:acc_tup}]{
  \includegraphics[bb=71 659 195 721]{acc_tup}
}
\caption{Two implementations of the same linear accumulator
\label{fig:tuple_vs_stack}}
\end{figure}
The encoding of a stack accumulator by means of tuples only supposes
to add a component to each tuple, which holds what was the `next'
tuple in the stack. The memory saving consists in one edge for each
initial tuple, plus all the push nodes, that is, if there were
\(n\)~tuples, we save \(n\)~edges (often called \emph{pointers} in
imperative languages) and \(n\)~nodes. This is a very significant
amelioration. As an illustration, let us improve on the following code
we derived earlier: \index{stack!flattening|(}
\begin{alltt}
flat0\_tf(T)          \(\smashedrightarrow{\alpha}\) flat0(T,[]).
flat0(         [],A) \(\smashedrightarrow{\gamma}\) appk([],A);
flat0(     [[]|T],A) \(\smashedrightarrow{\delta}\) flat0(T,A);
flat0([Y=[\_|\_]|T],A) \(\smashedrightarrow{\epsilon}\) flat0(Y,[\{k1,T\}|A]);
flat0(    [Y|T],A)   \(\smashedrightarrow{\zeta}\) flat0(T,[\{k34,Y\}|A]).
cat(   [],T,A)       \(\smashedrightarrow{\eta}\) appk(T,A);
cat([X|S],T,A)       \(\smashedrightarrow{\theta}\) cat(S,T,[\{k34,X\}|A]).
appk(V,[\{k34,Y\}|A])  \(\smashedrightarrow{\mu}\) appk([Y|V],A);
appk(V, [\{k2,W\}|A])  \(\smashedrightarrow{\lambda}\) cat(W,V,A);
appk(V, [\{k1,T\}|A])  \(\smashedrightarrow{\kappa}\) flat0(T,[\{k2,V\}|A]);
appk(V,         [])  \(\smashedrightarrow{\iota}\) V.
\end{alltt}
It results in the more economical
\begin{alltt}
flat0\_tf(T)          \(\smashedrightarrow{\alpha}\) flat0(T,\{\}).
flat0(         [],A) \(\smashedrightarrow{\gamma}\) appk([],A);
flat0(     [[]|T],A) \(\smashedrightarrow{\delta}\) flat0(T,A);
flat0([Y=[\_|\_]|T],A) \(\smashedrightarrow{\epsilon}\) flat0(Y,\{k1,T,A\});
flat0(      [Y|T],A) \(\smashedrightarrow{\zeta}\) flat0(T,\{k34,Y,A\}).
cat(   [],T,A)       \(\smashedrightarrow{\eta}\) appk(T,A);
cat([X|S],T,A)       \(\smashedrightarrow{\theta}\) cat(S,T,\{k34,X,A\}).
appk(V,\{k34,Y,A\})    \(\smashedrightarrow{\mu}\) appk([Y|V],A);
appk(V, \{k2,W,A\})    \(\smashedrightarrow{\lambda}\) cat(W,V,A);
appk(V, \{k1,T,A\})    \(\smashedrightarrow{\kappa}\) flat0(T,\{k2,V,A\});
appk(V,       \{\})    \(\smashedrightarrow{\iota}\) V.
\end{alltt}
\index{stack!flattening|)}
\index{stack!encoding with tuples|)}

\paragraph{Improvements}
\label{par:fib_improvements}
\index{Fibonacci's function!cost|(}
Just to illustrate the point that improvements on a definition which
is not in tail form are much more beneficial than a mere
transformation to tail form, let us consider again the Fibonacci
function:
\begin{alltt}
fib(0)            -> 1;
fib(1)            -> 1;
fib(N) when N > 1 -> fib(N-1) + fib(N-2).
\end{alltt}
The equations defining the cost of this function are simply:
\begin{equation*}
\C{\fun{fib}}{0} := 1;\quad
\C{\fun{fib}}{1} := 1;\quad
\C{\fun{fib}}{n} := 1 + \C{\fun{fib}}{n-1} +
\C{\fun{fib}}{n-2},\,\; \text{with} \,\; n > 1.
\end{equation*}
Adding~\(1\) on both sides of the last equation and reordering the
terms:
\begin{equation*}
\C{\fun{fib}}{n} + 1
  = (\C{\fun{fib}}{n-1} + 1) + (\C{\fun{fib}}{n-2} + 1).
\end{equation*}
This gives us the idea to set \(D_n := \C{\fun{fib}}{n} + 1 \),
yielding, for~\(n > 1\),
\begin{equation*}
D_0 = \C{\fun{fib}}{0} + 1 = 2,\quad
D_1 = \C{\fun{fib}}{1} + 1 = 2,\quad
D_n = D_{n-1} + D_{n-2}.
\end{equation*}
The recurrence is the same as the Fibonacci sequence (third clause
of \erlcode{fib/1}), except for \(D_0\)~and~\(D_1\) whose values
are~\(2\) instead of~\(1\). In order to make it coincide with the
values of \erlcode{fib/1}, we need to set \(F_n := D_n/2\):
\begin{equation*}
\C{\fun{fib}}{n} = 2 \cdot F_n - 1.
\end{equation*}
Now we have \(F_0 = F_1 = 1\) and \(F_n = F_{n-1} + F_{n-2}\), for
all~\(n > 1\); importantly, \(F_n\)~computes the same values
as~\erlcode{fib/1}, that is, \(F_n \equiv
\erlcode{fib(\(n\))}\). The \emph{generating
  function}\index{enumerative combinatorics!generating function}
associated to the sequence \((F_n)_{n \geqslant 0}\) is
\begin{equation}
f(x) := \sum_{k \geqslant 0}{F_kx^k}.
\label{eq:Fib}
\end{equation}
Let us set aside for a moment the issue of the convergence and let us
work out a closed form for \(f(x)\). We have \(xf(x) = \sum_{k >
  0}{F_{k-1}x^k}\) and \(x^2f(x) = \sum_{k>1}{F_{k-2}x^k}\), therefore
\begin{equation*}
f(x) - xf(x) - x^2f(x) = F_0 + F_1x - F_0x
                         + \sum_{k>1}{(F_k - F_{k-1} - F_{k-2})x^k}
                       = x.
\end{equation*}
Thus
\begin{equation*}
\abovedisplayskip=0pt
\belowdisplayskip=2pt
f(x) = \frac{x}{1 - x - x^2}.
\end{equation*}
Now, let us expand \(f(x)\) back into a power series by naming \(\phi
:= \frac{1+\sqrt{5}}{2}\) and \(\hat\phi := \frac{1-\sqrt{5}}{2}\) the
roots of \(1 - x - x^2\) and factoring the denominator:
\begin{equation*}
\abovedisplayskip=4pt
\belowdisplayskip=4pt
f(x) = \frac{x}{(1-\phi x)(1-\hat\phi x)}
     = \frac{1}{\sqrt{5}}\left(\frac{1}{1 - \phi x}
                              - \frac{1}{1 - \hat\phi x}\right).
\end{equation*}
We can now use the geometric power series \(\frac{1}{1-\alpha x} =
\sum_{k \geqslant 0}{\alpha^kx^k}\) to derive
\begin{equation*}
\abovedisplayskip=4pt
\belowdisplayskip=4pt
f(x) = \sum_{k \geqslant 0}{\frac{\phi^k-\hat\phi^k}{\sqrt{5}}}x^k,
\end{equation*}
so, by identification with the coefficients in
equation~\eqref{eq:Fib}, we conclude
\begin{equation*}
\abovedisplayskip=4pt
\belowdisplayskip=4pt
F_n = \frac{1}{\sqrt{5}}(\phi^n - \hat\phi^n).
\end{equation*}
(See \cite{GrahamKnuthPatashnik_1994}, \S~6.6, for more details.)  Of
course, we may very well doubt the result, as the method neglects
convergence issues, therefore let us prove\index{induction!example|(}
now by means of complete induction on~\(n > 0\) that
\begin{equation*}
\abovedisplayskip=4pt
\belowdisplayskip=4pt
F_0 = 1;\quad F_n = \frac{1}{\sqrt{5}}(\phi^n - \hat\phi^n).
\end{equation*}
First, let us verify that the formula works for the smallest value of~\(n\):
\begin{equation*}
\abovedisplayskip=4pt
\belowdisplayskip=4pt
F_1 = \frac{1}{\sqrt{5}}(\phi - \hat\phi)
    = \frac{1}{\sqrt{5}}(\phi - (1 - \phi)) = 1,
\end{equation*}
where we used the fact \(\hat\phi = 1 - \phi\). Let us suppose now
that the equation to establish is valid for all values ranging
from~\(1\) to~\(n\) (this is the complete induction hypothesis) and
let us prove that it holds for \(n+1\). We have \( F_{n+1} := F_n +
F_{n-1}\). We can use the complete induction hypothesis for the cases
\(n-1\)~and~\(n\):
\begin{align*}
\abovedisplayskip=4pt
\belowdisplayskip=0pt
F_{n+1} &= \frac{1}{\sqrt{5}}(\phi^n - \hat\phi^n) +
          \frac{1}{\sqrt{5}}(\phi^{n-1} - \hat\phi^{n-1})\\
       &= \frac{1}{\sqrt{5}}(\phi^{n-1}(\phi + 1) -
          \hat\phi^{n-1}(\hat\phi + 1)).
\end{align*}
The key is that \(\phi\)~and~\(\hat\phi\) are the roots of
\(x^2 = x + 1\), therefore
\begin{equation*}
  F_{n+1} = \frac{1}{\sqrt{5}}(\phi^{n-1} \cdot \phi^2 -
          \hat\phi^{n-1} \cdot \hat\phi^2)
       = \frac{1}{\sqrt{5}}(\phi^{n+1} - \hat\phi^{n+1}),
\end{equation*}
which was the statement to be proved. The complete induction principle
then implies that the equation holds for
all~\(n>0\). \index{induction!example|)} Now that we derived a closed
form for~\(F_n\), let us study its asymptotic behaviour. This is
straightforward if we start by noticing that \(\hat\phi <
1\), therefore \(\hat\phi^n \rightarrow 0\), as \(n\)~gets large and,
because~\(\phi > 1\),
\begin{equation*}
F_n \sim \frac{1}{\sqrt{5}}\phi^n,\,\; \text{implying}
\,\; \C{\fun{fib}}{n} \sim \frac{2}{\sqrt{5}}\phi^n.
\end{equation*}
That is, this cost is \emph{exponential}\index{cost!exponential
  $\sim$} and, because~\(\phi > 1\), it will always be greater than
any polynomial cost, except perhaps for a finite number of some small
values of~\(n\). In other words, this is hopelessly slow.

How can we improve this definition?

We must resist the temptation to transform it into tail form because
being in tail form only benefits the control stack, not the cost in
general. By looking at the call tree of \erlcode{fib(5)} in
\fig~\vref{fig:fib5tree},
\begin{figure}
\centering
\includegraphics{fib5tree}
\caption{Call tree of \texttt{fib(5)}\label{fig:fib5tree}}
\end{figure}
we realise that some small subtrees are duplicated, like the ones
rooted at~\erlcode{fib(2)} and, even larger ones,
like~\erlcode{fib(3)}. Let us examine the leftmost branch, from the
leaf to the root. It is made of the successive nodes \erlcode{fib(1)},
\erlcode{fib(2)}, \erlcode{fib(3)}, \erlcode{fib(4)} and
\erlcode{fib(5)}, that is, all the values of \erlcode{fib(N)}
for~\erlcode{N} ranging from~\erlcode{1} to~\erlcode{5}. Generalising
this observation, we can say that the series
\((\erlcode{fib(N)})_{\erlcode{N}}\) is entirely described,
except~\erlcode{fib(0)}, by the leftmost branch in the call tree of
\erlcode{fib(N)}. Therefore, starting from the small tree
\begin{center}
\includegraphics[bb=72 693 156 721]{fib2tree}
\end{center}
we can obtain the complete call tree for \erlcode{fib(5)} by growing
the tree from the root, whilst sharing some subtrees, that is, reusing
them instead of recomputing them, so the call tree looks now like in
\fig~\vref{fig:fib5shared} (technically, it is a directed acyclic
graph), where the arrowed edges implement the reuse of subtrees.
\begin{figure}[b]
\centering
\includegraphics[bb=71 671 349 718]{fib5shared}
\caption{Call tree of \erlcode{fib(5)} with maximal sharing
\label{fig:fib5shared}}
\end{figure}
This graph representation leads us to think that if two successive
Fibonacci numbers are kept at all times, we can achieve this maximal
sharing. Let us denote by~\(F_n\) the \(n\)th~Fibonacci number in the
series. Then each computational step is \((F_{n-1},F_{n}) \rightarrow
(F_{n}, F_{n+1}) := (F_{n},F_{n}+F_{n-1})\). Let \(f\)~be the function
such that \(f(x,y) := (y,x+y)\), then \((F_{n},F_{n+1}) =
f(F_{n-1},F_{n})\) and
\begin{equation*}
(F_n,F_{n+1}) = f(F_{n-1},F_{n}) = f(f(F_{n-2},F_{n-1})) =
f^2(F_{n-2},F_{n-1})
\end{equation*}
etc. till we reach \((F_{n},F_{n+1}) = f^n(F_0,F_1) := f^{n}(1,1)\),
for all \(n \geqslant 0\). Let \(\pi_1\) be the function such that
\(\pi_1(x,y) = x\), that is, it projects the first component of a
pair, then \(F_n = \pi_1 \circ f^n(1,1)\), for all \(n \geqslant
0\). The iteration of~\(f\) is easy to define by the recurrences
\begin{equation*}
f^0(x,y) = (x,y),\quad
f^n(x,y) = f^{n-1}(f(x,y)) := f^{n-1}(y,x+y).
\end{equation*}
The \Erlang code is now straightforward:
\begin{verbatim}
fib_opt(N) -> pi1(f(N,{1,1})).
pi1({X,_}) -> X.
f(0,{X,Y}) -> {X,Y};
f(N,{X,Y}) -> f(N-1,{Y,X+Y}).
\end{verbatim}
A tail form definition is extremely easy to obtain, without even applying
the general method:
\begin{alltt}
fib\_opt\_tf(N) -> f(N,\{1,1\}).
f(0,\{X,\_\})    -> X;\hfill% \emph{Projection done here}
f(N,\{X,Y\})    -> f(N-1,\{Y,X+Y\}).
\end{alltt}
We deduce that its cost is \(n + 2\). This is a tremendous improvement over~\erlcode{fib/1} and, as an unexpected bonus, the definition is in tail form and is made of the same number of clauses as the original.  \index{Fibonacci's function!cost|)}

The general algorithm we presented in this section transforms all the
definitions of the functions used by a given definition. Assuming that
the size of the control stack is a real issue, is it possible not to
transform all the functions involved? Consider again
\erlcode{slst0/2}, defined in equation~\eqref{eq:slst0} \vpageref{eq:slst0}:
\begin{verbatim}
slst0(S,X) -> rev(sfst(rev(S),X)).
\end{verbatim}
If we use the alternative definition \erlcode{sfst0/2}, which is
in tail form, instead of \erlcode{sfst/2}, and,
since~\erlcode{rev/1} is already in tail form, we reach
\begin{verbatim}
slst0(S,X) -> rev(sfst0(rev(S),X)).
\end{verbatim}
where all the composed functions are in tail form. Of course, a function composition, like~\erlcode{sfst0/2}, is not, by definition, in tail form, but it is not a problem. The size of control stack needed to compute the calls to \erlcode{slst0/2} will be bounded by a small constant, because it is not recursive.


\section{Higher-order functions}
\index{functional language!higher-order $\sim$|(}

\mypar{Polymorphic sorting}
\index{insertion sort!polymorphic $\sim$|(}

There is an aspect of straight insertion sort with \erlcode{isrt/1}
(section~\ref{sec:straight_ins} \vpageref{sec:straight_ins}) which
deserves a second thought. \Erlang functions are
\emph{polymorphic}\index{functional language!polymorphism}, that is,
they may process some of their arguments in a uniform manner,
irrespective of its type. For example, reversing a stack does not
depend on the nature of the keys it contains --~it is a purely
structural algorithm. By contrast, our definition of \erlcode{isrt/1}
relies on the usage of the predefined comparison operator
(\erlcode{>}) in a guard. This implies that all keys in the stack must
be pairwise comparable --~for example, they can be integers. But what
if we want to sort other kinds of values, like stacks? Consider the
very practical need to sort a set of bills: each bill can be
represented by a stack of prices rounded to the closest integer and
the set in question by a stack itself; we would then want to sort by
insertion the bills by, say, nondecreasing total amounts. If we set on
writing a version of \erlcode{isrt/1} tailored to work only with keys
which are stacks of integers, we are duplicating code and we would
have to write a different instance every time a different kind of
values to be sorted presents itself. As a consequence, what is needed
here is polymorphism on \emph{function parameters}, more precisely,
the possibility of a function to be a value, thus suitable as an
argument. \Erlang provides this facility in a natural way and many
functional languages do as well. In our case, we need the caller of
\erlcode{isrt/1} to provide an additional argument which is a
comparison function between the keys. Then the new \erlcode{isrt/2}
would make use of this caller\hyp{}defined comparison, instead of
always applying the default operator (\erlcode{>}) which works only
(or mostly) on integers. Here is again the definition of
\erlcode{isrt/1}: \input{isrt_alpha} Then, our first attempt at
modification leads us straightforwardly to
\begin{alltt}
isrtf(   [],\textbf{\_})             \(\smashedrightarrow{\beta}\) [];
isrtf([X|S],\textbf{F})             \(\smashedrightarrow{\gamma}\) ins(isrtf(S,\textbf{F}),X,\textbf{F}).
ins([Y|S],X,\textbf{F}) when \textbf{F(X,Y)} \(\smashedrightarrow{\delta}\) [Y|ins(S,X,\textbf{F})];
ins(    S,X,\textbf{\_})             \(\smashedrightarrow{\epsilon}\) [X|S].
\end{alltt}
But the compiler would reject this program because \Erlang does not
allow a user\hyp{}defined function to be called in a guard. The
rationale is that the call \erlcode{F(X,Y)} above may not terminate
and \Erlang guarantees that pattern matching always ends.  Because it
is impossible to automatically check whether any function call
terminates on all inputs (this problem is equivalent to the famous
\emph{halting problem of a Turing machine}, which is
\emph{undecidable}), the compiler does not even try and prefers to
reject all guards made of function calls. Thus we must move the call
\erlcode{F(X,Y)} inside the body of the same clause, which begets the
question as how to merge clauses \clause{\delta}~and~\clause{\epsilon}
into a new clause~\clause{\delta\sb{0}}. A simple way out is to create
another function, \erlcode{triage/4}, whose task is to take the result
of the comparison and proceed with the rest of the evaluation. Of
course, this means that \erlcode{triage/4} must also receive all
necessary information to carry on:
\newlength\dblarrow\settowidth\dblarrow{\(\delta\sb{0}\)}
\begin{alltt}
isrtf(   [],\_)          \(\MyArrow{\dblarrow}{\beta}\) [];
isrtf([X|S],F)          \(\MyArrow{\dblarrow}{\gamma}\) ins(isrtf(S,F),X,F).
ins([Y|S],X,F)          \(\MyArrow{\dblarrow}{\delta\sb{0}}\) triage(F(X,Y),[Y|S],X,F).
triage(\fbcode{CCCCC},[Y|S],X,F) \(\MyArrow{\dblarrow}{\zeta}\) [Y|ins(S,X,F)];
triage(\fbcode{CCCCC},[Y|S],X,F) \(\MyArrow{\dblarrow}{\eta}\) [X|S].
\end{alltt}
The empty boxes must be filled with the result of a comparison. In our
case, we want a comparison with two possible outputs, depending on the
first argument being lower or greater than the second. By definition,
the result of \erlcode{X~>~Y} is the atom \erlcode{true} if the value
of~\erlcode{X} is greater than the value of~\erlcode{Y} and
\erlcode{false} otherwise. Let us follow the same convention
for~\erlcode{F} and impose that the value of \erlcode{F(X,Y)} is the
atom \erlcode{true} if~\erlcode{X} is greater than~\erlcode{Y} and
\erlcode{false} otherwise. It is even better to rename the
parameter~\erlcode{F} into something more intuitive according to its
behaviour, like~\erlcode{Gt} (\emph{\textbf{G}reater \textbf{t}han}):
\begin{alltt}
triage( \textbf{true},[Y|S],X,Gt) \(\MyArrow{\dblarrow}{\zeta}\) [Y|ins(Gt,X,S)];
triage(\textbf{false},[Y|S],X,Gt) \(\MyArrow{\dblarrow}{\eta}\) [X|S].
\end{alltt}
We notice that clause~\clause{\eta} makes no use of~\erlcode{Y}, which
means we actually lose a key. What went wrong and when? The mistake
came from not realising that clause~\clause{\epsilon} covered two
cases, \erlcode{S}~is empty or not, therefore we should have untangled
these two cases before merging clause~\clause{\epsilon} with
clause~\clause{\delta}, because in~\clause{\delta} we have the pattern
\erlcode{[Y|S]}, that is, the non\hyp{}empty case. Let us rewind and
split~\clause{\epsilon} into
\clause{\epsilon\sb{0}}~and~\clause{\epsilon\sb{1}}:
\begin{alltt}
isrtf(   [], \_)              \(\MyArrow{\dblarrow}{\beta}\) [];
isrtf([X|S],Gt)              \(\MyArrow{\dblarrow}{\gamma}\) ins(isrtf(S,Gt),X,Gt).
ins([Y|S],X,Gt) when Gt(X,Y) \(\MyArrow{\dblarrow}{\delta}\) [Y|ins(S,X,Gt)];
ins(\textbf{[Y|S]},X,\textbf{Gt})              \(\MyArrow{\dblarrow}{\epsilon\sb{0}}\) [X|\textbf{[Y|S]}];
ins(   \textbf{[]},X, \_)              \(\MyArrow{\dblarrow}{\epsilon\sb{1}}\) \textbf{[X]}.
\end{alltt}
Notice that we did not write
\begin{alltt}
ins([],X,\_) \(\MyArrow{\dblarrow}{\epsilon\sb{1}}\) [X];
ins( S,X,\_) \(\MyArrow{\dblarrow}{\epsilon\sb{0}}\) [X|S].
\end{alltt}
even though it would have been correct, because we had in mind the
fusion with clause~\clause{\delta}, so we needed to make the pattern
\erlcode{[Y|S]} conspicuous in~\clause{\epsilon\sb{0}}. For even more
clarity, we made apparent the parameter~\erlcode{Gt}: the patterns of
clauses \clause{\delta}~and~\clause{\epsilon\sb{0}} are now identical
and ready to merge into a new clause~\clause{\delta\sb{0}}:
\begin{alltt}
isrtf(   [], \_)          \(\MyArrow{\dblarrow}{\beta}\) [];
isrtf([X|S],Gt)          \(\MyArrow{\dblarrow}{\gamma}\) ins(isrtf(S,Gt),X,Gt).
ins([Y|S],X,Gt)          \(\MyArrow{\dblarrow}{\delta\sb{0}}\) triage(Gt(X,Y),[Y|S],X,Gt).
ins(   [],X, \_)          \(\MyArrow{\dblarrow}{\epsilon\sb{1}}\) [X].
triage( true,[Y|S],X,Gt) \(\MyArrow{\dblarrow}{\zeta}\) [Y|ins(S,X,Gt)];
triage(false,[Y|S],X, \_) \(\MyArrow{\dblarrow}{\eta}\) [X|[Y|S]].
\end{alltt}
We can improve a little bit clause~\clause{\eta} by not distinguishing
\erlcode{Y}~and~\erlcode{S}:
\begin{alltt}
triage(false,    \textbf{S},X, \_) \(\MyArrow{\dblarrow}{\eta}\) [X|\textbf{S}].
\end{alltt}
This transformation is correct because \erlcode{S}~is never empty.
Instead of using an auxiliary function like \erlcode{triage/4}, which
takes many arguments and serves no purpose other than performing a
test on the value of \erlcode{Gt(X,Y)} and proceed accordingly, we can
make good use of a \erlcode{case} construct:
\begin{alltt}
isrtf(   [], \_) \(\MyArrow{\dblarrow}{\beta}\) [];
isrtf([X|S],Gt) \(\MyArrow{\dblarrow}{\gamma}\) ins(isrtf(S,Gt),X,Gt).
ins([Y|S],X,Gt) \(\MyArrow{\dblarrow}{\delta\sb{0}}\) \textbf{case Gt(X,Y) of
                      true  \(\smashedrightarrow{\zeta}\) [Y|ins(S,X,Gt)];
                      false \(\smashedrightarrow{\eta}\) [X|[Y|S]]
                    end;}
ins(   [],X, \_) \(\MyArrow{\dblarrow}{\epsilon\sb{1}}\) [X].
\end{alltt}
We can decrease the memory usage again in the clause~\clause{\eta}
(case \erlcode{false}), this time by means of an alias for the pattern
\erlcode{[Y|S]}, so the best version of the code is
\begin{alltt}
isrtf(   [], \_)   \(\MyArrow{\dblarrow}{\beta}\) [];
isrtf([X|S],Gt)   \(\MyArrow{\dblarrow}{\gamma}\) ins(isrtf(S,Gt),X,Gt).
ins(\textbf{T=}[Y|S],X,Gt) \(\MyArrow{\dblarrow}{\delta\sb{0}}\) case Gt(X,Y) of
                        true  \(\smashedrightarrow{\zeta}\) [Y|ins(S,X,Gt)];
                        false \(\smashedrightarrow{\eta}\) [X|\textbf{T}]
                      end;
ins(     [],X, \_) \(\MyArrow{\dblarrow}{\epsilon\sb{1}}\) [X].
\end{alltt}

How would we call \erlcode{isrtf/2} so the resulting value is the same
as calling \erlcode{isrt/1}? First, we need a comparison function
which behaves exactly like the operator~(\erlcode{>}):
\begin{alltt}
gt\_int(X,Y) when X > Y -> true;
gt\_int(\_,\_)            -> false.
\end{alltt}
If we try now to form the call
\begin{center}
\erlcode{isrtf([5,3,1,4,2],gt\_int)},
\end{center}
we find that an error occurs at run\hyp{}time because
\erlcode{gt\_int} is an atom, not a function. That is why \Erlang
provides a special syntax for denoting functions used as values:
\begin{center}
\erlcode{isrtf([5,3,1,4,2],\textbf{fun gt\_int/2})}.
\end{center}
Notice the new keyword~\erlcode{fun} and the usual indication of the
number of arguments the function is expected to operate on (here,
two).

What would happen if we passed as an argument the function
\erlcode{lt\_int/2} defined as follows?
\begin{alltt}
lt\_int(X,Y) when X < Y -> true;
lt\_int(X,Y)            -> false.
\end{alltt}
The consequence is that the result is sorted non\hyp{}increasingly and
all we had to do was to change the comparison function, \emph{not the
  sorting function itself.}

It may seem a burden to have to name even simple comparison functions
like \erlcode{lt\_int/2}, which is none other than the predefined
operator~(\erlcode{<}). Fortunately, \Erlang provides a way to define
functions without giving them a name. The syntax consists in using the
keyword \erlcode{fun} together with the keyword \erlcode{end} and put
the usual definition in\hyp{}between, without a function
name. Reconsider for example the previous calls but using such
anonymous functions (sometimes called \emph{lambdas}):
\begin{center}
\erlcode{isrtf([5,3,1,4,2],\textbf{fun}(X,Y) -> X > Y \textbf{end})}
\end{center}
results in \erlcode{[1,2,3,4,5]} and
\begin{center}
\erlcode{isrtf([5,3,1,4,2],\textbf{fun}(X,Y) -> X < Y \textbf{end})}
\end{center}
results in \erlcode{[5,4,3,2,1]}.

Let us now use \erlcode{isrtf/2} to sort stacks of stacks of integers,
according to the sum of the integers in each stack --~this is the
practical application of sorting bills we previously mentioned. As the
example of sorting in non\hyp{}increasing order hints at, we only need
here to write how to compare two stacks of integers by means of the
\erlcode{sum0/1} function in \fig~\vref{fig:sum0}). We have
\(\C{\fun{sum}_0}{n} = n + 2\). Now we can define the comparison
function \erlcode{gt\_bill/2}, based upon the operator~(\erlcode{>}):
\begin{alltt}
gt\_bill(P,Q) -> sum0(P) > sum0(Q).
\end{alltt}
Notice in passing that the predefined \Erlang comparison
operator~(\erlcode{>}) results in either the atom \erlcode{true} or
\erlcode{false}, so there is no need to use a \erlcode{case}
construct. Then we can sort our bills by calling
\begin{center}
\erlcode{isrtf([[1,5,2,9],[7],[2,5,11],[4,3]],\textbf{fun} gt\_bill/2)}
\end{center}
or, simply,
\begin{alltt}
      isrtf([[1,5,2,9],[7],[2,5,11],[4,3]],
            \textbf{fun}(P,Q) -> sum0(P) < sum0(Q) \textbf{end}).
\end{alltt}
(By the way, do we expect~\erlcode{[7]} to appear before or after
\erlcode{[4,3]} in the answer? What would we have to modify so the
relative order of these two stacks is reversed?) It is just as easy to
sort the bills in non\hyp{}increasing order. This great easiness in
passing around functions as any other kind of values is what justifies
the adjective \emph{functional} for a language like \Erlang and many
others. A function taking another function as an argument is said to
be a \emph{higher\hyp{}order function}.  \index{insertion
  sort!polymorphic $\sim$|)}

\mypar{Sorted association lists}
\label{sorted_association_lists}
\index{association list|(}

There is something that we could improve in the previous definition of
\erlcode{isrtf/2}. Sorting by comparison may imply that some keys are
compared more than once, as the worst case of insertion sort
demonstrates eloquently. It may be that one comparison has a small
cost but, compounded over many uses, it leads to a significant
cost. In the case of sorting bills, it is more efficient to compute
all the total amounts first and then only use these amounts during the
sort process, because comparing one integer to another is much faster
than recomputing the sum of many integers in a stack. So, what is
sorted is a stack of pairs whose first component, called the
\emph{key}, is a simple and small representative of the second
component, called the \emph{value} (improperly, as keys are \Erlang
values as well, but such is the traditional nomenclature). This data
structure is sometimes called an \emph{association list}. Only the key
is used for sorting, not the value, therefore, if the key is an
integer, the comparison on the key is likely to be faster than on the
values. The only penalty is that all the keys must be precomputed in a
first pass over the initial data and they must be stripped from the
final result in an additional post\-processing. This time we shall
design these first and last passes in the most general fashion by
parameterisation upon the evaluation~\erlcode{Mk} of the keys:
\begin{verbatim}
% Computing the keys
mk_keys( _,        []) -> [];
mk_keys(Mk,[V|Values]) -> [{Mk(V),V}|mk_keys(Mk,Values)].

% Eliminating the keys
rm_keys(            []) -> [];
rm_keys([{_,V}|KeyVal]) -> [V|rm_keys(KeyVal)].
\end{verbatim}
The cost of \erlcode{mk\_keys/2} depends on the cost
of~\erlcode{Mk}. The cost of calling \erlcode{rm\_keys(\(S\))} is~\(n+1\) if
\(S\)~contains \(n\)~pairs key\hyp{}value. Now we can sort by calling
\erlcode{isrtf/2} with a comparison on two keys and with the function
to build the keys, \erlcode{sum0/1}. For instance:
\begin{alltt}
 rm\_keys(isrtf(mk\_keys(\textbf{fun} sum0/1,
                       [[1,5,2,9],[7],[2,5,11],[4,3]]),
               \textbf{fun}(\{K1,\_\},\{K2,\_\}) -> K1 > K2 \textbf{end}))
\end{alltt}
It is very important to notice that we did not need to redefine
\erlcode{isrtf/2}. Actually, \erlcode{isrtf/2}, \erlcode{mk\_keys/2}
and \erlcode{rm\_keys/1} would very well constitute a library by
grouping their definitions in the same module. The client, that is,
the user of the library, would then provide the comparison function
fitted to their data to be sorted and the function making the keys. This
\emph{modularisation} is enabled by polymorphism and higher\hyp{}order
functions.

As a last example proving the versatility of our program, let us sort
stacks by their non\hyp{}increasing lengths:
\begin{alltt}
 rm\_keys(isrtf(mk\_keys(\textbf{fun} len0/1,
                       [[1,5,2,9],[7],[2,5,11],[4,3]]),
               \textbf{fun}(\{K1,\_\},\{K2,\_\}) -> K1 < K2 \textbf{end}))
\end{alltt}
where \erlcode{len0/1} is specified in \fig~\vref{fig:len0}.
\begin{figure}
\begin{equation*}
\boxed{
\begin{array}{@{}r@{\;}l@{\;}l@{}}
\fun{len}_0(s) & \rightarrow & \fun{len}_0(s,0).\\
\fun{len}_0(\el,n) & \rightarrow & n;\\
\fun{len}_0(\cons{x}{s},n) & \rightarrow & \fun{len}_0(s,n+1).
\end{array}}
\end{equation*}
\caption{Computing the length of a stack (tail form)
\label{fig:len0}}
\end{figure}
The result:
\begin{center}
\erlcode{[[1,5,2,9],[2,5,11],[4,3],[7]]}.
\end{center}
Notice that \erlcode{[4,3]} occurs before~\erlcode{[7]} because the
former is longer.

Let us specialise further \erlcode{isrtf/1}. Here is the definition
again:
\begin{alltt}
isrtf(   [], \_)   \(\MyArrow{\dblarrow}{\beta}\) [];
isrtf([X|S],Gt)   \(\MyArrow{\dblarrow}{\gamma}\) ins(isrtf(S,Gt),X,Gt).
ins(T=[Y|S],X,Gt) \(\MyArrow{\dblarrow}{\delta\sb{0}}\) \textbf{case} Gt(X,Y) \textbf{of}
                        true  \(\smashedrightarrow{\zeta}\) [Y|ins(S,X,Gt)];
                        false \(\smashedrightarrow{\eta}\) [X|T]
                      \textbf{end};
ins(     [],X, \_) \(\MyArrow{\dblarrow}{\epsilon\sb{1}}\) [X].
\end{alltt}
It is clear that if keys are repeated in the input stack, the call
\erlcode{Gt(X,Y)} is expected to be rewritten to \erlcode{false} at
least once, therefore duplicates are kept by clause~\clause{\eta} and
their relative order is preserved, that is, the sorting algorithm is
stable. What if we do not want to preserve such duplicates in the
output? We need to rewrite the definition to support this choice. The
choice itself, that is, to keep them or not, would naturally be
implemented as an additional functional parameter,
say~\erlcode{Eq}. Also, we would need a 3-way comparison, so the
equality case is explicit. Let us modify the variable~\erlcode{Gt} to
reflect this increase in detail and call it more
generally~\erlcode{Cmp} (\emph{compare}). Its arguments should be
values amongst the user\hyp{}defined atoms~\erlcode{lt} (\emph{lower
  than}), \erlcode{gt} (\emph{greater than}) and~\erlcode{eq}
(\emph{equal}). We have
\begin{alltt}
isrtf(   [],  \_, \_)   \(\MyArrow{\dblarrow}{\beta}\) [];
isrtf([X|S],Cmp,\textbf{Eq})   \(\MyArrow{\dblarrow}{\gamma}\) ins(isrtf(S,Cmp,\textbf{Eq}),X,Cmp,\textbf{Eq}).
ins(T=[Y|S],X,Cmp,\textbf{Eq}) \(\MyArrow{\dblarrow}{\delta\sb{0}}\) \textbf{case} Cmp(X,Y) \textbf{of}
                            gt \(\smashedrightarrow{\zeta}\) [Y|ins(S,X,Cmp,\textbf{Eq})];
                            lt \(\smashedrightarrow{\eta}\) [X|T];
                            eq \(\smashedrightarrow{\theta}\) \textbf{Eq(X,T)}\hfill% \emph{New case}
                          \textbf{end};
ins(     [],X,  \_, \_) \(\MyArrow{\dblarrow}{\epsilon\sb{1}}\) [X].
\end{alltt}
Now, let us say that we want to sort nondecreasingly a stack of
integers and retain possible redundant numbers, just as the previous
version allowed. We have (novelty in boldface type):
\begin{alltt}
isrtf([5,3,1,4,3],fun(X,Y) -> X>Y end,\textbf{fun(X,T) ->\! [X|T] end})
\end{alltt}
which results in \erlcode{[1,3,3,4,5]}. If we do not want the numbers
repeated, we form instead the call
\begin{alltt}
isrtf([5,3,1,4,3],fun(X,Y) -> X>Y end,\textbf{fun(\_,T) -> T end})
\end{alltt}
resulting in \erlcode{[1,3,4,5]}. In passing, this technique solves
the problem of removing duplicates in a stack of keys for which there
is a total order. However, if only successive duplicates have to be
removed from a stack, the function \erlcode{red/1}, defined in
\fig~\vref{fig:red}, is more efficient because its cost is linear in
the input size.

We would be remiss if we do not mention that a higher\hyp{}order
function is not only a function whose at least one parameter is a
function, but it also can be a function whose calls evaluate in a
function. This kind of function is said to be
\emph{curried},\index{functional language!higher-order $\sim$!curried
  function} as an homage to the logician Haskell Curry. The
possibility was already there when we introduced the keywords
\erlcode{fun}~and~\erlcode{end}, because they allow us to define an
anonymous function and use it just like another value, so nothing
impeded us from using such a functional value as the result of a named
function, like in the following function mathematically composing two
functions:
\begin{alltt}
compose(F,G) -> (\textbf{fun}(X) -> F(G(X)) \textbf{end}).
\end{alltt}
Actually, the parentheses around the functional value are useless if
we remember that \emph{the keywords \erlcode{fun}~and~\erlcode{end} play the
role of parentheses when the anonymous function is not called}:
\begin{alltt}
compose(F,G) -> \textbf{fun}(X) -> F(G(X)) \textbf{end}.
\end{alltt}
The higher\hyp{}order function \erlcode{compose/2} can be used to
compute the composition of two other functions, the result being a
function, of course.
\index{association list|)}

\medskip

\paragraph{Functional iterators}
\addcontentsline{toc}{subsection}{Map and folds}
\label{par:maps}

We may desire a function which sums the images of a stack~\(S\) of
integers by a given function~\(f\). In mathematical notation, the
final result would be expressed as
\begin{equation*}
\sum_{k \in S}{f(k)}.
\end{equation*}
In order to implement this in \Erlang, we must proceed in two steps:
firstly, we need a higher\hyp{}order function which computes the
images of the items of a stack by a function; secondly, we need a
function summing the integers of a stack. We already have the latter,
known from \fig~\ref{fig:sum0} page~\pageref{fig:sum0} as
\erlcode{sum0/1}. The former is traditionally called \erlcode{map/2},
such that the call \erlcode{map(\(F\),\(S\))} applies function~\(F\)
to all the items of stack~\(S\) and evaluates into the stack of the
results. That is,
\begin{equation*}
\erlcode{map(\(F\),[\(X_1\),\(X_2\),\(\ldots\),\(X_{n}\)])}
\equiv
\erlcode{[\(F\)(\(X_1\)),\(F\)(\(X_2\)),\(\ldots\),\(F\)(\(X_{n}\))]}.
\end{equation*}
With this goal in mind, it is straightforward to define
\erlcode{map/2}:
\begin{verbatim}
map(_,   []) -> [];
map(F,[X|S]) -> [F(X)|map(F,S)].
\end{verbatim}
The function we were looking for is now compactly defined as the
composition of \erlcode{map/2} and \erlcode{sum0/1} as follows:
\begin{alltt}
sumf(F) -> \textbf{fun}(S) -> sum0(map(F,S)) \textbf{end}.
\end{alltt}
For instance, the function call
\begin{center}
\erlcode{sumf(\textbf{fun}(X) -> X*X \textbf{end})}
\end{center}
denotes the function which sums the squares of the numbers in a stack
to be provided. It is equivalent to the value
\begin{center}
\erlcode{\textbf{fun}(S) -> sum0(map(\textbf{fun}(X) -> X*X \textbf{end},S)) \textbf{end}.}
\end{center}
It is possible to call this function just after it has been computed
by \erlcode{sumf/1}, but \emph{parentheses must be added around a
  function being called when it is anonymous}. For instance, see the
boldface type and underlining in
\begin{center}
\erlcode{\underline{\textbf{(}}sumf(fun(X) -> X*X end)\underline{\textbf{)}}([1,2,3])}.
\end{center}
The function \erlcode{map/2} is often used because it captures a
common operation on stacks. For example,
\begin{verbatim}
push(_,       []) -> [];
push(X,[P|Perms]) -> [[X|P]|push(X,Perms)].
\end{verbatim}
is equivalent to
\begin{alltt}
push(X,Perms) -> map(\textbf{fun}(P) -> [X|P] \textbf{end},Perms).
\end{alltt}
This style leads to clearer programs as it shows the underlying
recursive evaluation without having to read or write a definition for
it. In other words, using a higher\hyp{}order function like
\erlcode{map/2} allows us to identify a common recursive pattern and
let the programmer focus instead on the specific processing of the
items. We shall encounter other examples in the next sections but,
before we move on, imagine we typed instead
\begin{alltt}
push(X,Perms) -> map(fun(\textbf{Perms}) -> [X|\textbf{Perms}] end,Perms).
\end{alltt}
The \Erlang compiler would issue the following warning:
\begin{center}
\emph{\texttt{Warning: variable 'Perms' shadowed in
    'fun'.}}\label{shadowing}
\end{center}
What happens is that the parameter \textbf{\erlcode{Perms}} (in
boldface type) `hides' the parameter \erlcode{Perms} of
\erlcode{push/2} in the sense that, in the body of the anonymous
function, the occurrence of \textbf{\erlcode{Perms}} refers to
\erlcode{fun(\textbf{Perms})}, but not \erlcode{push(X,Perms)}. In
this case, it is not an error, but the compiler designers worried
about programmers walking on the shadowy side of the street. For
example,
\begin{alltt}
push(X,Perms) -> map(fun(X) -> [X|X] end,Perms).\hfill% \emph{Capture}
\end{alltt}
is definitely wrong because the two variables~\erlcode{X} in
\erlcode{[X|X]}, which is the body of the anonymous function, are the
parameter of the anonymous function. A faulty shadowing is called a
\emph{capture}. Here, the parameter~\erlcode{X} bound by
\erlcode{push(X,Perms)} has been captured to mean instead the
parameter of \erlcode{fun(X)}. As a guideline, it is best to avoid
shadowing a parameter, as the \Erlang compiler reminds us for our own
sake. Note that
\begin{center}
\erlcode{sumf(fun(S) -> S*S end)}
\end{center}
is fine because it is equivalent to
\begin{center}
\erlcode{fun(S) -> sum0(map(fun(S) -> S*S end,S)) end}
\end{center}
which is a correct shadowing.

\paragraph{Folds}
\label{par:folds}

Some other useful and frequently recursive schemas can be conveniently
reified into some other higher\hyp{}order functions. Consider a
function which traverses completely a stack while processing an
accumulator depending or not on the current visited item. In the end,
the result is the final value of the accumulator, or else another
function is called to finalise it. A simple example is \fun{len0/1} in
\fig~\vref{fig:len0}. In this case, the accumulator is an integer and
the operation on it consists in incrementing it, whatever the current
item is. Another function reverses a stack (equation~\eqref{def:rev}
\vpageref{def:rev}): \verbatiminput{rev.def} Here, the accumulator is
a stack and the operation on it consists in pushing the current item
on top of it. Let us abstract separately these two concerns in a
higher\hyp{}order function
\begin{enumerate}

  \item which takes as input the function creating a new accumulator
    from the current item and the previous accumulator and

  \item which applies it successively to all the items of a parameter
    stack.

\end{enumerate}
One famous function doing exactly this is called \erlcode{foldl/3} in
\Erlang, which stands for `fold left,' because once the new
accumulator for some item has been computed, the prefix of the stack
up to it can be folded back, as if the stack were written down on a
sheet of paper, because it is no longer useful. So the name should be
better read as `fold from left to right' or \emph{rightward fold}. We
want
\begin{equation*}
\erlcode{foldl(\(F\),\(A\),[\(X_1\),\(X_2\),\(\ldots\),\(X_{n}\)])}
\equiv
\erlcode{\(F\)(\(X_{n}\),\(\ldots\),\(F\)(\(X_2\),\(F\)(\(X_1\),\(A\)))\(\ldots\))},
\end{equation*}
where \(A\)~is the initial value of the
accumulator. \Fig~\vref{fig:foldl} shows the corresponding abstract
syntax trees.
\begin{figure}[b]
\centering
\subfloat[Stack \erlcode{S}]{%
  \includegraphics{a_stack}%[bb=71 650 148 721]
}
\qquad
\subfloat[\erlcode{foldl(F,A,S)}]{%
  \includegraphics[bb=62 660 135 721]{foldl}%[bb=69 651 164 721]
}
\caption{The result of \erlcode{foldl/3} on a non\hyp{}empty stack
\label{fig:foldl}}
\end{figure}
The following definition implements the desired effect:
\begin{verbatim}
foldl(_,A,   []) -> A;
foldl(F,A,[X|S]) -> foldl(F,F(X,A),S).
\end{verbatim}
Now we can rewrite new definitions of \erlcode{len0/1} and
\erlcode{rev/1}:
\begin{alltt}
lenl(S) -> foldl(\textbf{fun}(\_,A) ->   A+1 \textbf{end}, 0,S).
revl(S) -> foldl(\textbf{fun}(X,A) -> [X|A] \textbf{end},[],S).
\end{alltt}
Function \erlcode{foldl/3} is not in tail form because of the embedded
call \erlcode{F(X,A)}, but only a constant amount of control stack is
used for the recursion of \erlcode{foldl/3} itself (one node). In our
two examples, \erlcode{F}~is in tail form, therefore these new
definitions are \emph{almost} in tail form and can stand against the
originals. More definitions almost in tail form are
\begin{alltt}
suml([N|S]) -> foldl(\textbf{fun}(X,A) ->      X+A \textbf{end}, N,S).
rcatl(S,T)  -> foldl(\textbf{fun}(X,A) ->    [X|A] \textbf{end}, T,S).
rmap(F,S)   -> foldl(\textbf{fun}(X,A) -> [F(X)|A] \textbf{end},[],S).
\end{alltt}
Again, the reason why these definitions are not exactly in tail form
is due to the call \erlcode{F(X,A)} in the definition of
\erlcode{foldl/3}, \emph{not} because of the functional arguments
\erlcode{fun(X,A) -> ... end} in the calls to \erlcode{foldl/3}: these
are not function calls but anonymous function definitions, that is,
pieces of data. The main advantage of using \erlcode{foldl/3} is that
it allows the programmer to focus exclusively on the processing of the
accumulator, whilst \erlcode{foldl/3} itself provides the ride for
free. Moreover, we can easily compare different functions defined by
means of \erlcode{foldl/3}.

When the accumulator is a stack on which values are pushed, the result
is in reverse order with respect to the input. That is why
\erlcode{rmap/2}, above, is not equivalent to \erlcode{map/2}. The
former is to be preferred over the latter if the order of the items is
not relevant, because \erlcode{map/2} requires a control stack as long
as the input stack. This leads us quite naturally to introduce another
higher\hyp{}order function: \erlcode{foldr/3}, meaning `fold from
right to left,' or \emph{leftward fold}. We expect
\begin{equation*}
\erlcode{foldr(\(F\),\(A\),[\(X_1\),\(X_2\),\(\ldots\),\(X_{n}\)])}
\equiv
\erlcode{\(F\)(\(X_1\),\(F\)(\(X_2\),\(\ldots\),\(F\)(\(X_{n}\),\(A\)))\(\ldots\))}.
\end{equation*}
\Fig~\vref{fig:foldr} shows the corresponding abstract syntax trees.
\begin{figure}[b]
\centering
\subfloat[Stack \erlcode{S}]{%
  \includegraphics{a_stack}
}
\qquad
\subfloat[\erlcode{foldr(F,A,S)}]{%
  \includegraphics[bb=62 660 132 721]{foldr}
}
\caption{The result of \erlcode{foldr/3} on a non\hyp{}empty stack
\label{fig:foldr}}
\end{figure}
We achieve this behaviour with the following definition:
\begin{verbatim}
foldr(_,A,   []) -> A;
foldr(F,A,[X|S]) -> F(X,foldr(F,A,S)).
\end{verbatim}
This definition, like \erlcode{foldl/3}, is not in tail form but,
unlike \erlcode{foldl/3}, it requires a control stack as long as the
input stack.

With the help of \erlcode{foldr/3}, we can redefine \erlcode{map/2}
and \erlcode{cat/2} as
\begin{alltt}
mapr(F,S) -> foldr(\textbf{fun}(X,A) -> [F(X)|A] \textbf{end},[],S).
catr(S,T) -> foldr(\textbf{fun}(X,A) ->    [X|A] \textbf{end}, S,T).
\end{alltt}
Compare \erlcode{rcatl/2}, defined above, with \erlcode{catr/2}: the
role of the accumulator and of the input stack have been exchanged, as
well as \erlcode{foldl/3} and \erlcode{foldr/3}. It is also possible
to define
\begin{alltt}
lenr(S)     -> foldr(\textbf{fun}(\_,A) -> 1+A \textbf{end}, 0,S).\hfill% \emph{Bad}
sumr([N|S]) -> foldr(\textbf{fun}(X,A) -> X+A \textbf{end}, N,S).\hfill% \emph{Bad}
isrtr(S)    -> foldr(\textbf{fun}(X,A) -> ins(A,X) \textbf{end},[],S).\hfill% \emph{Bad}
\end{alltt}
but that would be unwise because \erlcode{foldr/3} does not use a
bounded amount of control stack, contrary to \erlcode{foldl/3}. In the
case of \erlcode{isrt/1}, it is best to call \erlcode{foldl/3} instead
because the order of insertion does not matter in average (although it
swaps the best and worst cases if the items are not repeated). Note
also, in the case of \erlcode{isrtr/1}, how the order of the arguments
of the mapped function \erlcode{ins/2} matters.

This leads us to formulate some guidelines about the transformation
into tail form. We already know that a definition in tail form is
worth having or even necessary if the maximum size of the control
stack is smaller than that of some input recursively traversed in its
greatest extension. No speed\hyp{}up should be expected a priori from
turning a definition into tail form --~although this may happen
sometimes. Usually,
\begin{itemize}

  \item it is preferable, if possible, to use \erlcode{foldl/3} instead
  of \erlcode{foldr/3} because, provided the functional parameter is
  defined in tail form, the call will use a small limited amount of
  control stack (if the parameter is not in tail form, at least
  \erlcode{foldl/3} won't burden further the control stack, contrary
  to \erlcode{foldr/3});

  \item when writing our own recursion, that is, without resorting to
  folds, it is best to have it in tail form if the accumulator is an
  integer, otherwise, the maximum size of the control stack may need
  to be proportional to the size of the input, despite the output
  being a single integer. (Contrast \erlcode{sum/1} and
  \erlcode{sum0/2}, as well as \erlcode{len/1} and \erlcode{len0/1}.)

\end{itemize}
Independently of stack allocation, there can be a significant
difference in cost when using one fold instead of the other. Take for
example the following two calls:
\begin{center}
\erlcode{foldl(fun cat/2,[],S)}
\(\equiv\)
\erlcode{foldr(fun cat/2,[],S)}.
\end{center}
The first one will be slower than the second, as shown by
inequality~\eqref{ineq:cat_assoc} \vpageref{ineq:cat_assoc}.

What cannot be programmed with folds? As the defining properties show,
folds traverse the input stack in its entirety, hence there is no way
to get off the bus while it is running. For instance, \fun{sfst/2} in
section~\ref{sec:skipping}, \vpageref{sec:skipping}, is, in \Erlang,
\begin{alltt}
sfst(   [],X) \(\smashedrightarrow{\theta}\) [];
sfst([X|S],X) \(\smashedrightarrow{\iota}\) S;
sfst([Y|S],X) \(\smashedrightarrow{\kappa}\) [Y|sfst(S,X)].
\end{alltt}
and cannot be implemented by means of a fold because there are two
ends to the function calls: either the item has not been found and we
ran afoul the end of the stack in clause~\clause{\theta}, or it has
been found somewhere inside the stack in
clause~\clause{\iota}. However, in theory, if we accept a full
traversal of the stack for every call, then \erlcode{sfst/2} can be
programmed by means of a rightward fold. The usual technique is to
have an accumulator which is either an atom meaning `not found' or a
pair with an atom meaning `found' and the rebuilt stack. If the result
is `not found,' then we just give back the original stack. The
following function makes a stronger case because it is really
impossible to express by means of a fold, even inefficiently. It is
the `complete tail' function:\label{code:ctail}
\verbatiminput{ctail.def} In general, a function~\(F\) can be
equivalently expressed by a call to a fold if, and only if, for all
stacks \(S\)~and~\(T\), for all item~\(X\), we have
\begin{equation*}
\erlcode{\(F\)(\(S\))} \equiv \erlcode{\(F\)(\(T\))}
\Rightarrow
\erlcode{\(F\)([\(X\)|\(S\)])}
\equiv
\erlcode{\(F\)([\(X\)|\(T\)])}.
\end{equation*}
(See \cite{GibbonsHuttonAltenkirch_2001,WeberCaldwell_2004}.) For
instance, we have \(\erlcode{ctail([])} \equiv
\erlcode{ctail([2])}\) but \(\erlcode{ctail([1])} \mathrel{\not\equiv}
\erlcode{ctail([1,2])}\).

One positive side\hyp{}effect of using maps and folds is that they sometimes allow the programmer to recognise some compositions that can be optimised by means of some equivalence. As an example, we have, for all functions~\(F\) and~\(G\):
\begin{equation*}
\erlcode{map(\(F\),map(\(G\),\(S\)))}
\equiv
\erlcode{map(compose(\(F\),\(G\)),\(S\))}.
\end{equation*}
Without counting in the costs of \(F\)~and~\(G\), the left\hyp{}hand
side induces the cost \(2n+2\), if \(S\)~contains \(n\)~items, whereas
the right\hyp{}hand side incurs the cost \(n+2\), so it is much
preferable to the former. Another interesting equation is
\begin{equation}
\erlcode{foldl(\(F\),\(A\),\(S\))}
\equiv
\erlcode{foldr(\(F\),\(A\),\(S\))}\label{eq:foldlr}
\end{equation}
\emph{if \(F\)~is associative and symmetric.} Let us prove it. The
first clauses of the definitions of \erlcode{foldl/3} and
\erlcode{foldr/3} imply that, for all \(F\)~and~\(A\),
\begin{equation*}
\erlcode{foldl(\(F\),\(A\),[])} \equiv A
\equiv \erlcode{foldr(\(F\),\(A\),[])}.
\end{equation*}
For non\hyp{}empty stacks, this equation means:
\begin{equation*}
F(X_{n},\ldots,F(X_2,F(X_1,A))\ldots)
\equiv
F(X_1,F(X_2,\ldots,F(X_{n},A))\ldots).
\end{equation*}
Although the ellipses in the previous equation are intuitive, they are
not a valid foundation for a rigorous mathematical argument. Instead,
by definition, we have
\begin{align*}
  \erlcode{foldl(\(F\),\(A\),[\(X\)|\(S\)])}
  &\equiv
   \erlcode{foldl(\(F\),\(F\)(\(X\),\(A\)),\(S\))}.
\intertext{Dually, by definition, we also have}
\erlcode{foldr(\(F\),\(A\),[\(X\)|\(S\)])}
  &\equiv
   \erlcode{\(F\)(\(X\),foldr(\(F\),\(A\),\(S\)))}.
\intertext{The original equation would thus be established for all
  stacks if we prove}
\erlcode{foldl(\(F\),\(F\)(\(X\),\(A\)),\(S\))}
  &\equiv
   \erlcode{\(F\)(\(X\),foldr(\(F\),\(A\),\(S\)))}.
\end{align*}
Let us call this conjecture~\(\predName{Fold}\) and prove it by
structural induction. In general terms, this principle states that,
given a finite data structure~\(S\), a property~\(\pred{Fold}{S}\) to
be proved about it, then
\begin{enumerate}

  \item if \(\pred{Fold}{S}\)~is provable for all the atomic~\(S\),
    that is, configurations of~\(S\) that cannot be decomposed;

  \item if, assuming \(\pred{Fold}{T}\) for all immediate
    substructures~\(T\) of~\(S\), then \(\pred{Fold}{S}\)~is proved;

  \item then \(\pred{Fold}{S}\)~is proved for \emph{all}~\(S\).

\end{enumerate}
Here, the data structure~\(S\) being a stack, there is a unique atomic
stack: the empty stack. So we must first prove
\(\pred{Fold}{\erlcode{[]}}\). The first clauses of the definitions of
\erlcode{foldl/3} and \erlcode{foldr/3} imply that, for all
\(F\)~and~\(A\),
\begin{equation*}
\erlcode{foldl(\(F\),\(F\)(\(X\),\(A\)),[])}
\equiv \erlcode{\(F\)(\(X\),\(A\))}
\equiv
\erlcode{\(F\)(\(X\),foldr(\(F\),\(A\),[]))},
\end{equation*}
which is \(\pred{Fold}{\erlcode{[]}}\). Next, let us consider a
non\hyp{}empty stack \erlcode{[\(Y\)|\(S\)]}. What are its immediate
substructures? By construction of stacks, there is only one immediate
substack of \erlcode{[\(X\)|\(S\)]}, namely~\(S\). Therefore, let us
assume \(\pred{Fold}{S}\) for a given stack~\(S\) and suppose that
\(F\)~is associative and symmetric (this is the structural induction
hypothesis) and let us prove \(\pred{Fold}{\erlcode{[\(Y\)|\(S\)]}}\),
for all~\(Y\). For~\(F\) to be associative means that, for all
values~\(I\), \(J\)~and~\(K\), we have
\begin{align*}
F(I,F(J,K)) &\equiv F(F(I,J),K).
\intertext{The symmetry of~\(F\) means that, for all~\(I\) and~\(J\),
  we have}
F(I,J) &\equiv F(J,I).
\end{align*}
Let us start with the left\hyp{}hand side of
\(\pred{Fold}{\erlcode{[\(Y\)|\(S\)]}}\):
\begin{equation*}
\begin{array}{@{}ll@{}}
\erlcode{foldl(\(F\),\(F\)(\(X\),\(A\)),[\(Y\)|\(S\)])}\\
\;\equiv
   \erlcode{foldl(\(F\),\(F\)(\(Y\),\(F\)(\(X\),\(A\))),\(S\))}
& \text{(definition of \erlcode{foldl/3})}\\
\;\equiv
   \erlcode{foldl(\(F\),\(F\)(\(F\)(\(Y\),\(X\)),\(A\)),\(S\))}
& \text{(associativity of~\(F\))}\\
\;\equiv
   \erlcode{foldl(\(F\),\(F\)(\(F\)(\(X\),\(Y\)),\(A\)),\(S\))}
& \text{(symmetry of~\(F\))}\\
\;\equiv
   \erlcode{\(F\)(\(F\)(\(X\),\(Y\)),foldr(\(F\),\(A\),\(S\)))}
& \text{(induction hypothesis \(\pred{Fold}{S}\))}\\
\;\equiv
   \erlcode{\(F\)(\(X\),\(F\)(\(Y\),foldr(\(F\),\(A\),\(S\))))}
& \text{(associativity of~\(F\))}\\
\;\equiv
   \erlcode{\(F\)(\(X\),foldr(\(F\),\(A\),[\(Y\)|\(S\)]))}
& \text{(definition of \erlcode{foldr/3}).}\hfill\Box
\end{array}
\end{equation*}
This proves \(\pred{Fold}{\erlcode{[\(Y\)|\(S\)]}}\). The principle of
structural induction then implies that \(\pred{Fold}{S}\) is proved
for all stacks~\(S\), hence the original equation~\eqref{eq:foldlr}
\vpageref{eq:foldlr}. The previous derivation suggests a variation in
the definition of \erlcode{foldl/3}:
\begin{alltt}
foldl\_alt(\_,A,   []) -> A;
foldl\_alt(F,A,[X|S]) -> foldl\_alt(F,F(\underline{\textbf{A,X}}),S).
\end{alltt}
The difference lies in the order of the parameters of~\erlcode{F}. We
would then have to prove a slightly different conjecture:
\begin{equation*}
\erlcode{foldl\_alt(\(F\),\(F\)(\(A\),\(X\)),\(S\))}
\equiv
\erlcode{\(F\)(\(X\),foldr(\(F\),\(A\),\(S\)))}.
\end{equation*}
The previous derivation would read now as follows:
\begin{equation*}
\begin{array}{@{}ll@{}}
\erlcode{foldl\_alt(\(F\),\(F\)(\(A\),\(X\)),[\(Y\)|\(S\)])}\\
\;\equiv
\erlcode{foldl\_alt(\(F\),\(F\)(\(F\)(\(A\),\(X\)),\(Y\)),\(S\))}
& \text{(definition)}\\
\;\equiv
   \erlcode{foldl\_alt(\(F\),\(F\)(\(A\),\(F\)(\(X\),\(Y\))),\(S\))}
& \text{(associativity of~\(F\))}\\
\;\equiv
   \erlcode{\(F\)(\(F\)(\(X\),\(Y\)),foldr(\(F\),\(A\),\(S\)))},
& \text{(induction hypothesis \(\pred{Fold}{S})\)}\\
\;\equiv
   \erlcode{\(F\)(\(X\),\(F\)(\(Y\),foldr(\(F\),\(A\),\(S\))))}
& \text{(associativity of~\(F\))}\\
\;\equiv
   \erlcode{\(F\)(\(X\),foldr(\(F\),\(A\),[\(Y\)|\(S\)]))},
& \text{(definition of \erlcode{foldr/3}).}\hfill\Box
\end{array}
\end{equation*}
We see that the symmetry of~\(F\) is not required anymore but in only
one remaining place: when the stack is empty. Indeed, we have
\begin{align*}
\erlcode{foldl\_alt(\(F\),\(F\)(\(A\),\(X\)),[])}
  &\equiv \erlcode{\(F\)(\(A\),\(X\))},
  &&\text{by definition;}\\
\erlcode{\(F\)(\(X\),foldr(\(F\),\(A\),[]))}
  &\equiv \erlcode{\(F\)(\(X\),\(A\))},
  &&\text{by definition of \erlcode{foldr/3}.}
\end{align*}
Therefore, in order to prove the variant conjecture about
\erlcode{foldl\_alt/3} and \erlcode{foldr/3}, we must have
\begin{equation*}
\erlcode{\(F\)(\(A\),\(X\))}
\equiv
\erlcode{\(F\)(\(X\),\(A\))},
\end{equation*}
that is,~\(A\), which is the initial value of the accumulator, must
commute with all items~\(X\) under~\(F\). This is not a clear
improvement over the first theorem about \erlcode{foldl/3}, which
required all pairs of successive items commute. Nevertheless, there is
an interesting special case, which is when \(A\)~is a neutral element
for~\(F\), that is, for all~\(X\),
\begin{equation*}
\erlcode{\(F\)(\(A\),\(X\))}
\equiv
\erlcode{\(F\)(\(X\),\(A\))} \equiv A.
\end{equation*}
Then symmetry altogether is no more required. Therefore,
\erlcode{foldl\_alt/3} is preferable over \erlcode{foldl/3}, because
it provides more opportunities when transforming applications of
\erlcode{foldr/3}. But, since the standard library of \Erlang offers
the definition \erlcode{foldl/3}, we shall stick to it. The standard
library of \OCaml, however, proposes the function
\erlcode{fold\_left}, which corresponds to \erlcode{foldl\_alt/3}.

Anyway, theorem~\eqref{eq:foldlr} allows us to transform immediately
some calls to \erlcode{foldr/3}, which requires an amount of control
stack at least proportional to the size of the input stack, into calls
to \erlcode{foldl/3}, whose parameter~\(F\) is the only function
possibly not using a constant amount of control stack (if it does, the
gain is all the more obvious). This is why the following definitions
are equivalent:
\begin{alltt}
lenl(S) -> foldl(\textbf{fun}(\_,A) -> A+1 \textbf{end},0,S).
lenr(S) -> foldr(\textbf{fun}(\_,A) -> A+1 \textbf{end},0,S).
\end{alltt}
Proving that the following two definitions are equivalent happens to
be a bit trickier:
\begin{alltt}
suml([N|S]) -> foldl(\textbf{fun}(X,A) -> X+A \textbf{end},N,S).
sumr([N|S]) -> foldr(\textbf{fun}(X,A) -> X+A \textbf{end},N,S).
\end{alltt}
The reason is that the first item of the stack serves as the initial
value of the accumulator in both cases, despite the order of traversal
of the stack being reversed (rightward versus leftward).  It is much
more obvious to see that the following definitions are equivalent:
\begin{alltt}
sum1(S=[\_|\_]) -> foldl(\textbf{fun}(X,A) -> X+A \textbf{end},0,S).
sum2(S=[\_|\_]) -> foldr(\textbf{fun}(X,A) -> X+A \textbf{end},0,S).
\end{alltt}
only because addition is associative and symmetric.

\paragraph{Functional encoding of maps}
\addcontentsline{toc}{subsection}{Functional encodings}
%\mypar{Mappings without association lists}

In order to illustrate further the expressive power of
higher\hyp{}order functions, let us muse about a small, albeit
unlikely, example. We mentioned \vpageref{sorted_association_lists}
association lists being a collection of pairs key\hyp{}value,
straightforwardly implemented as stacks, for instance,
\erlcode{[\{a,0\},\{b,1\},\{a,5\}]}. A \emph{mapping} is an
association list which is searched based on the first component of the
pairs. Typically, we have
\begin{alltt}
find(\_,       []) -> absent;
find(X,[\{X,V\}|\_]) -> V;\hfill% \emph{Associated value found}
find(X,    [\_|S]) -> find(X,S).\hfill% \emph{Keep searching}
\end{alltt}
Notice that if a key is repeated, only the first pair will be
considered, for instance, \erlcode{find(a,[\{a,0\},\{b,1\},\{a,5\}])}
evaluates to~\erlcode{0}, not~\erlcode{5}. These pairs are called
\emph{bindings}. Let us assume now that we want to present formally
what a mapping is but without relying upon any particular programming
language. In this case, we must count on mathematics to convey the
concept, more precisely, on mathematical functions. We would say that
a mapping~\(M\) is a function from some finite set of
values~\(\mathcal{K}\) to some finite set of
values~\(\mathcal{V}\). Therefore, what was previously the conjunction
of a data type (a stack) and a lookup function (\erlcode{find/2}) is
now a single function, representing the mapping \emph{and} the lookup
at the same time. A binding \(x \mapsto y\) is just another notation
for the pair \((x, y)\), where \(x \in \mathcal{K}\) and \(y \in
\mathcal{V}\). We need now to express how a mapping is updated, that
is, how a mapping is extended with a new binding. With a stack, this
is simply done by pushing a new pair but, without a stack, we would
say that an update is a function itself, taking a mapping and a
binding as arguments and returning a new mapping. \emph{An update is
  thus a higher\hyp{}order function.}  Let the function~\((\oplus)\)
be such that \(M \mathrel{\oplus} x_1 \mapsto y\) is the \emph{update}
of the mapping~\(M\) by the binding \(x_1 \mapsto y\), as defined by
\begin{equation*}
(M \mathrel{\oplus} x_1 \mapsto y)(x_2) :=
\begin{cases}
  y      & \text{if} \; x_1 = x_2,\\
  M(x_2) & \text{otherwise.}
\end{cases}
\end{equation*}
We can check that we return the value associated to the first key
matching the input, as expected. The empty mapping would be a special
function returning a special symbol meaning `not found,' like
\(M_\varnothing(x) = \bot\), for all~\(x\). The mapping containing the
binding \((1,5)\) would be \(M_\varnothing \mathrel{\oplus} 1 \mapsto
5\). This is very abstract and independent of any programming
language, whilst being totally precise. If now the need arises to show
how this definition can be programmed, this is when functional
languages can shine. An update would be directly written in \Erlang as
\begin{alltt}
update(M,\{X1,Y\}) -> \textbf{fun}(X2) -> \textbf{case} X2 \textbf{of} X1 -> Y;
                                           \_ -> M(X2)
                               \textbf{end}
                    \textbf{end}.
\end{alltt}
The correspondence with the formal definition is almost immediate,
there is no need to introduce a data structure and its interpretation,
nor prove its correctness. The empty mapping is simply
\begin{verbatim}
empty(_) -> absent.
\end{verbatim}
For example, the mapping as stack \erlcode{[\{a,0\},\{b,1\},\{a,5\}]}
can be modelled with higher\hyp{}order functions only as
\begin{center}
\erlcode{update(update(update(\textbf{fun} empty/1,\{a,5\}),\{b,1\}),\{a,0\})}.
\end{center}
Perhaps what needs to be learnt from all this is that stacks in
functional languages, despite having a distinctive syntax and being
used pervasively, are not a fundamental data type: functions are.

\paragraph{Functional encodings of tuples}

Let us start by abstracting the tuple into its essence and, because in
a functional language functions are the main feature, we should ask
ourselves what is \emph{done} with something we think of as a
tuple. Actually, we rushed because we should have realised first that
all tuples can be expressed in terms of the empty tuple and pairs. For
instance, \erlcode{\{5,foo,\{\textbf{fun}(X) -> X*X \textbf{end}\}\}}
can be rewritten with embedded pairs as
\erlcode{\{5,\{foo,\{\textbf{fun}(X) -> X*X
  \textbf{end},\{\}\}\}\}}. So let us rephrase the question in terms
of pairs only. Basically, a pair is constructed (or \emph{injected})
and matched, that is, deconstructed (or \emph{projected}). This
analysis leads to the conclusion that the functional encoding of pairs
requires three functions: one for making, \erlcode{mk\_pair/2}, and
two for unmaking, \erlcode{fst/1} and \erlcode{snd/1}. Once a pair is
built, it is represented as a function, therefore functions extracting
the components take as an argument another function denoting the pair,
thus they are of higher order. Consider
\begin{alltt}
mk\_pair(X,Y) \(\smashedrightarrow{\alpha}\) \textbf{fun}(Pr) \(\smashedrightarrow{\beta}\) Pr(X,Y) \textbf{end}.\hfill% Pr \emph{is a projection}
fst(P) \(\smashedrightarrow{\gamma}\) P(\textbf{fun}(X,\_) \(\smashedrightarrow{\delta}\) X \textbf{end}).\hfill% P \emph{denotes a pair}
snd(P) \(\smashedrightarrow{\epsilon}\) P(\textbf{fun}(\_,Y) \(\smashedrightarrow{\zeta}\) Y \textbf{end}).
\end{alltt}
We have the following expected behaviour:
\begin{alltt}
fst(mk\_pair(3,5)) \(\smashedrightarrow{\alpha}\) fst(\textbf{fun}(Pr) \(\smashedrightarrow{\beta}\) Pr(3,5) \textbf{end})
                  \(\smashedrightarrow{\gamma}\) (\textbf{fun}(Pr) \(\!\smashedrightarrow{\beta}\!\) Pr(3,5) \textbf{end})(\textbf{fun}(X,\_) \(\!\smashedrightarrow{\delta}\!\) X \textbf{end})
                  \(\smashedrightarrow{\beta}\) (\textbf{fun}(X,\_) \(\smashedrightarrow{\delta}\) X \textbf{end})(3,5)
                  \(\smashedrightarrow{\delta}\) 3.
\end{alltt}
To proof the versatility of this encoding, let us define a function
\erlcode{add/1} which adds the components of the pair passed to it:
\begin{alltt}
add(P) \(\smashedrightarrow{\eta}\) fst(P) + snd(P).
\end{alltt}
A call to \erlcode{add/1} would unravel as follows, assuming that
arguments are evaluated rightward:
\begin{alltt}
add(mk\_pair(3,5))
  \(\smashedrightarrow{\alpha}\) add(\textbf{fun}(Pr) \(\smashedrightarrow{\beta}\) Pr(3,5) \textbf{end})
  \(\smashedrightarrow{\eta}\)   fst(\textbf{fun}(Pr) \(\smashedrightarrow{\beta}\) Pr(3,5) \textbf{end})
     + snd(\textbf{fun}(Pr) \(\smashedrightarrow{\beta}\) Pr(3,5) \textbf{end})
  \(\smashedrightarrow{\gamma}\)   (\textbf{fun}(Pr) \(\smashedrightarrow{\beta}\) Pr(3,5) \textbf{end})(\textbf{fun}(X,\_) \(\smashedrightarrow{\delta}\) X \textbf{end})
     + snd(\textbf{fun}(Pr) \(\smashedrightarrow{\beta}\) Pr(3,5) \textbf{end})
  \(\smashedrightarrow{\beta}\)   (\textbf{fun}(X,\_) \(\smashedrightarrow{\delta}\) X \textbf{end})(3,5)
     + snd(\textbf{fun}(Pr) \(\smashedrightarrow{\beta}\) Pr(3,5) \textbf{end})
  \(\smashedrightarrow{\delta}\) 3 + snd(\textbf{fun}(Pr) \(\smashedrightarrow{\beta}\) Pr(3,5) \textbf{end})
  \(\smashedrightarrow{\epsilon}\) 3 + (\textbf{fun}(Pr) \(\smashedrightarrow{\beta}\) Pr(3,5) \textbf{end})(\textbf{fun}(\_,Y) \(\smashedrightarrow{\zeta}\) Y \textbf{end})
  \(\smashedrightarrow{\beta}\) 3 + (\textbf{fun}(\_,Y) \(\smashedrightarrow{\zeta}\) Y \textbf{end})(3,5)
  \(\smashedrightarrow{\zeta}\) 3 + 5 \(=\) 8.
\end{alltt}
The keen reader may feel cheated, though, because we could have simply
defined \erlcode{add/2} as
\begin{verbatim}
add(X,Y) -> X + Y.
\end{verbatim}
Indeed, this critique is valid. The ability of functions to receive
various arguments at once amounts to them receiving \emph{one} tuple
exactly, whose components are these various values. Therefore, we have
to retry and make sure that our functions are \emph{nullary} or
\emph{unary}, that is, take zero or one argument. This is achieved by
taking one value as argument and rewrite the call into a function
which will take in turn the next value as argument etc. This
translation is called \emph{currying}.
\begin{alltt}
mk\_pair(X) \(\smashedrightarrow{\alpha}\) \textbf{fun}(Y) \(\smashedrightarrow{\beta}\) \textbf{fun}(Pr) \(\smashedrightarrow{\gamma}\) (Pr(X))(Y) \textbf{end} \textbf{end}.
fst(P) \(\smashedrightarrow{\delta}\) P(\textbf{fun}(X) \(\smashedrightarrow{\epsilon}\) \textbf{fun}(\_) \(\smashedrightarrow{\zeta}\) X \textbf{end} \textbf{end}).
snd(P) \(\smashedrightarrow{\eta}\) P(\textbf{fun}(\_) \(\smashedrightarrow{\theta}\) \textbf{fun}(Y) \(\smashedrightarrow{\iota}\) Y \textbf{end} \textbf{end}).
add(P) \(\smashedrightarrow{\kappa}\) fst(P) + snd(P).
\end{alltt}
Let us recall that \erlcode{\textbf{fun}(X) -> \textbf{fun}(P) -> ...} is equivalent to
the expression \erlcode{\textbf{fun}(X) -> (\textbf{fun}(P) -> ...)} The parentheses
around \erlcode{Pr(X)} are necessary in \Erlang because this call is
in the place of a function being called itself. Now
\begin{alltt}
add((mk\_pair(3))(5))
  \(\smashedrightarrow{\alpha}\) add((\textbf{fun}(Y) \(\smashedrightarrow{\beta}\) \textbf{fun}(Pr) \(\smashedrightarrow{\gamma}\) (Pr(3))(Y) \textbf{end} \textbf{end})(5))
  \(\smashedrightarrow{\beta}\) add(\textbf{fun}(Pr) \(\smashedrightarrow{\gamma}\) (Pr(3))(5) \textbf{end})
  \(\smashedrightarrow{\kappa}\)   fst(\textbf{fun}(Pr) \(\smashedrightarrow{\gamma}\) (Pr(3))(5) \textbf{end})
     + snd(\textbf{fun}(Pr) \(\smashedrightarrow{\gamma}\) (Pr(3))(5) \textbf{end})
  \(\smashedrightarrow{\delta}\)   (\textbf{fun}(Pr)\(\smashedrightarrow{\gamma}\)(Pr(3))(5) \textbf{end})(\textbf{fun}(X) \(\smashedrightarrow{\epsilon}\) \textbf{fun}(\_) \(\smashedrightarrow{\zeta}\)X \textbf{end} \textbf{end})
     + snd(\textbf{fun}(Pr) \(\smashedrightarrow{\gamma}\) (Pr(3))(5) \textbf{end})
  \(\smashedrightarrow{\gamma}\)   ((\textbf{fun}(X) \(\smashedrightarrow{\epsilon}\) \textbf{fun}(\_) \(\smashedrightarrow{\zeta}\) X \textbf{end} \textbf{end})(3))(5)
     + snd(\textbf{fun}(Pr) \(\smashedrightarrow{\gamma}\) (Pr(3))(5) \textbf{end})
  \(\smashedrightarrow{\epsilon}\) (\textbf{fun}(\_) \(\smashedrightarrow{\zeta}\) 3 \textbf{end})(5) + snd(\textbf{fun}(Pr) \(\smashedrightarrow{\gamma}\) (Pr(3))(5) \textbf{end})
  \(\smashedrightarrow{\zeta}\) 3 + snd(\textbf{fun}(Pr) \(\smashedrightarrow{\gamma}\) (Pr(3))(5) \textbf{end})
  \(\smashedrightarrow{\eta}\) 3 +(\textbf{fun}(Pr)\(\smashedrightarrow{\gamma}\)(Pr(3))(5) \textbf{end})(\textbf{fun}(\_) \(\smashedrightarrow{\theta}\) \textbf{fun}(Y) \(\smashedrightarrow{\iota}\) Y \textbf{end} \textbf{end})
  \(\smashedrightarrow{\gamma}\) 3 + ((\textbf{fun}(\_) \(\smashedrightarrow{\theta}\) \textbf{fun}(Y) \(\smashedrightarrow{\iota}\) Y \textbf{end} \textbf{end})(3))(5)
  \(\smashedrightarrow{\theta}\) 3 + (\textbf{fun}(Y) \(\smashedrightarrow{\iota}\) Y \textbf{end})(5)
  \(\smashedrightarrow{\iota}\) 3 + 5 \(=\) 8.
\end{alltt}
Of course, this encoding is usually not worth doing because the number
of function calls is much greater than when using a data
structure. Its main interest is to show the theoretical expressive
power of higher\hyp{}order functions.

\paragraph{Functional encoding of stacks}

To gain more insight into the nature of stacks as data structures, we
can encode stacks only with higher\hyp{}order functions. In the former
view, a stack is an infrastructure, a kind of inert container for data
and functions are expected to operate on it. In the latter view, a
stack is a composition of functions containing data as arguments and
waiting to be called to \emph{do} something with them. The difference
between both points of view is not an imagined dichotomy between data
and functions, which is blurred in object\hyp{}oriented languages too,
but the fact that higher\hyp{}order functions \emph{alone} can make up
a stack.

Since we already know how to encode pairs with higher\hyp{}order
functions, a first approach to encoding stacks with functions simply
consists in encoding them with pairs. Abstractly, a stack can either
be empty or constructed by pushing an item into another stack, so all
we need is to translate these two concepts. The empty stack can
readily be represented by the empty tuple~\erlcode{\{\}} and pushing
becomes pairing:
\begin{verbatim}
push(X,S) -> {X,S}.
\end{verbatim}
This encoding was introduced in \fig~\vref{fig:tuple_vs_stack} to save
memory on linear accumulators. Here, we want to go one step further
and get rid of the pairs themselves by means of their functional
interpretation seen above, so \erlcode{push/2} becomes a renaming of
\erlcode{mk\_pair/2}:
\begin{alltt}
push(X,S) -> \textbf{fun}(Pr) -> Pr(X,S) \textbf{end}.\hfill% \emph{See} mk_pair/2
\end{alltt}
To understand the status of the empty stack, we must consider
projections on stacks. These are usually called \emph{head} and
\emph{tail}. We implement them as the original versions of
\erlcode{fst/2} and \erlcode{snd/2}, where~\erlcode{S},
\erlcode{H}~and~\erlcode{T} denote, respectively, an encoding of a
stack, a head and a tail:
\begin{alltt}
head(S) -> S(\textbf{fun}(H,\_) -> H \textbf{end}).\hfill% \emph{See} fst/2
tail(S) -> S(\textbf{fun}(\_,T) -> T \textbf{end}).\hfill% \emph{See} snd/2
\end{alltt}
Let us now think \emph{how} the empty stack is used. It is a stack
such that any projection of its putative contents fails, that is,
projecting the first component (the head) fails, as well as projecting
the second component (the tail). A trick consists in defining
\begin{alltt}
empty() -> fail.\hfill% \emph{The atom} fail \emph{is arbitrary}
\end{alltt}
The point is that \erlcode{empty/0} is nullary, so calling it with an
argument fails, as in \erlcode{head(\textbf{fun} empty/0)}. For
example, the stack \erlcode{[a,b,c]} is encoded
\begin{center}
\erlcode{push(a,push(b,push(c,\textbf{fun} empty/0))).}
\end{center}
This solution relies on the \emph{arity}, that is, the number of
parameters, of \erlcode{empty/0} to lead to failure. This failure is
consistent with the way classic stacks, that is, stacks as data
structures, are used: \erlcode{tail([\_|S]) -> S} and the call
\erlcode{tail([])} fails to match a clause. The limit of this encoding
is that, being based on functions, the encoded stacks cannot be
matched by the heads of the clauses. For example, the function
\erlcode{ctail/1} \vpageref{code:ctail} \verbatiminput{ctail.def}
cannot not be encoded because we would need a way to check whether an
encoded stack is empty without crashing the program if it is not. If
we prefer the caller to be gently informed of the problem instead,
that is, we want the definition of the projections to be complete, we
could allow \erlcode{empty/1} to take a projection which is then
discarded:
\begin{verbatim}
empty(_) -> fail.
\end{verbatim}
We would have the rewrite \erlcode{head(fun empty/1) \(\rightarrow\)
  fail}, which is not a failure insofar the run\hyp{}time system is
concerned, but is interpreted by the application as a \emph{logical}
failure. Of course, it becomes the burden of the caller to check
whether the atom \erlcode{fail} is returned and the burden of the
stack maker to make sure not to push this atom in the encoded stack,
otherwise a caller would confuse the empty stack with a regular
item. (A better solution consists in using \emph{exceptions}.)

\mypar{Fixed\hyp{}point combinators}
\index{functional language!Erlang@\Erlang!local recursion|(}
\index{functional language!higher-order $\sim$!fixed-point combinator|(}

Many functions need some other auxiliary functions to carry out
subtasks. For example, consider \fig~\vref{fig:len0} where
\erlcode{len0/2} is the auxiliary function. To forbid its usage
outside the scope of the module, it would be omitted in the
\erlcode{-export} clause at the beginning, but it still could be
called from within the module it is defined. How could we avoid this
as well? This is where anonymous functions comes handy:
\begin{alltt}
len0(S) ->
  Len = fun(   [],N) -> N;
           ([\_|S],N) -> \textbf{Len}(S,N+1)\hfill% \emph{Does not compile}
        end,
  Len(S,0).
\end{alltt}
This limits the visibility of the anonymous function bound to variable
\erlcode{Len} to the body of \erlcode{len0/1}, which is exactly what
we wanted. The problem here is that this definition is rejected by the
\Erlang compiler because the binding construct~(\erlcode{=}) does not
make the variable on its left\hyp{}hand side visible to the
right\hyp{}side, hence \erlcode{Len}~is unknown in the call
\erlcode{Len(S,N+1)}. In some other functional languages, there is a
specific construct to allow recursion on local definitions, for
instance, \erlcode{let~rec} in \OCaml, but the following hypotyposis
is nevertheless theoretically relevant. The original problem becomes
another one: how can we define anonymous \emph{recursive} functions? A
workaround is to pass an additional function parameter, which is used
in stead of the recursive call:
\begin{alltt}
len1(S) -> Len = fun(\_,   [],N) -> N;
                    (\textbf{F},[\_|S],N) -> \textbf{F}(\textbf{F},S,N+1)
                 end,
           Len(\textbf{Len},S,0).
\end{alltt}
Notice that we renamed \erlcode{len0/1} into \erlcode{len1/1}
because we are going to envisage several variants. Moreover, the
anonymous function is not equivalent to \erlcode{len/2} because it
takes three arguments. Also, the compiler emits the following warning
(we removed the line number):
\begin{center}
\emph{\texttt{Warning: variable 'S' shadowed in 'fun'.}}
\end{center}
We have seen this before, \vpageref{shadowing}. Here, the shadowing is
harmless, because inside the anonymous function denoted
by~\erlcode{Len}, the original value of~\erlcode{S}, that is, the
argument of \erlcode{len1/1}, is not needed. Nevertheless, for the
sake of tranquillity, a simple renaming will get rid of the warning:
\begin{alltt}
len1(S) -> Len = fun(\_,   [],N) -> N;
                    (F,[\_|\textbf{T}],N) -> F(F,\textbf{T},N+1)\hfill% \emph{Renaming}
                 end,
           Len(Len,S,0).
\end{alltt}
We can alter this definition by currying the anonymous function and
renaming it so \erlcode{Len}~now is equivalent to \erlcode{fun len/2}:
\begin{alltt}
len2(S) -> H = \textbf{fun(F) ->} fun(   [],N) -> N;
                            ([\_|T],N) -> \textbf{(F(F))}(T,N+1)
                         end
               \textbf{end},
           Len = H(H),\hfill% \emph{Equivalent to} fun len/2
           Len(S,0).
\end{alltt}
Let us define a function \erlcode{u/1} which auto\hyp{}applies its
functional argument and let us make use of it in stead of
\erlcode{F(F)}:
\begin{alltt}
u(F) -> fun(X,Y) -> (F(F))(X,Y) end.\hfill% \emph{Self\hyp{}application}

len3(S) -> H = fun(F) -> fun(   [],N) -> N;
                            ([\_|T],N) -> \textbf{(u(F))}(T,N+1)
                         end
               end,
           (H(H))(S,0).\hfill% \emph{Expanded} Len
\end{alltt}
Let us replace now \erlcode{u(F)} by~\erlcode{F}. This transformation
does not preserve the semantics of~\erlcode{H}, so let us rename the
resulting function~\erlcode{G} and we redefine~\erlcode{H} to be
equivalent to its prior instance:
\begin{alltt}
len3(S) -> G = fun(F) -> fun(   [],N) -> N;
                            ([\_|T],N) -> F(T,N+1)
                         end
               end,
           \textbf{H = fun(F) -> G(u(F)) end},
           (H(H))(S,0).
\end{alltt}
The interesting point is that the anonymous function referred to by
variable~\erlcode{G} is very similar to~\erlcode{Len} at the
beginning. (It may sound paradoxical to speak of anonymous functions
with names, but, in \Erlang, variables and function names are two
distinct syntactic categories, so there is no contradiction in terms.)
Here it is again:
\begin{alltt}
len0(S) ->
  Len = fun(   [],N) -> N;
           ([\_|S],N) -> Len(S,N+1)\hfill% \emph{Unfortunately invalid}
        end,
  Len(S,0).
\end{alltt}
The difference is that \erlcode{G}~abstracts over~\erlcode{F} instead
of having a (problematic) recursive call. Let us expand back the call
\erlcode{u(F)} and get rid of~\erlcode{u/1} altogether:
\begin{alltt}
len4(S) ->
  G = fun(F) -> fun(   [],N) -> N;
                   ([\_|T],N) -> F(T,N+1)
                end
      end,
  H = fun(F) -> G(\textbf{fun(X,Y) -> (F(F))(X,Y) end}) end,
  (H(H))(S,0).
\end{alltt}
To gain some generality, we can extract the assignments to
\erlcode{H}~and~\erlcode{Len}, put them into a new function
\erlcode{x/1} and expand~\erlcode{Len} in place:
\begin{alltt}
\textbf{x(G) -> H=fun(F) -> G(fun(X,Y)->(F(F))(X,Y) end) end, H(H).}

len5(S) -> G = fun(F) -> fun(   [],N) -> N;
                            ([\_|T],N) -> F(T,N+1)
                         end
               end,
           (x(G))(S,0).
\end{alltt}
By putting the definition of function \erlcode{x/1} into a dedicated
module, we can now easily define recursive anonymous functions. There
is a limitation, though, which is that \erlcode{x/1} is tied to the
arity of~\erlcode{F}. For instance, we cannot use it for the
factorial:
\begin{alltt}
fact(N) -> G = fun(F) -> fun(0) -> 1;
                            (N) -> N * F(N-1)
                         end
               end,
           (x(G))(S,0).\hfill% \emph{Arity mismatch}
\end{alltt}
Therefore, if we really want a general scheme, we should work with
fully curried functions, so all functions are unary:
\begin{alltt}
x(G) -> H = fun(F) -> G(fun(\textbf{X}) -> (F(F))(\textbf{X}) end) end, H(H).

len6(S) -> G=fun(F) -> fun(N) -> fun(   []) -> N;
                                    ([\_|T]) -> (F(N+1))(T)
                                 end
                       end
             end,
           ((x2(G))(0))(S).
\end{alltt}
Notice that we swapped the order of the stack and the integer, since
there is no pattern matching to be done on the latter. The grammar of
\Erlang obliges us to put parentheses around every function call
resulting in a function being immediately called, so calling fully
curried functions with all their arguments, like
\erlcode{((x2(G))(0))(S)}, ends up being a bit fastidious, although a
good text editor can help us in paring properly the parentheses.

\emph{The theoretical point of this derivation is that we can always
  write a non\hyp{}recursive function equivalent to a recursive one,}
since even \erlcode{x/1}~is not recursive. In fact, nothing special is
required as long as we have unrestricted function calls. Some strongly
and statically typed languages like \OCaml reject the definition
of~\erlcode{x/1} above, but other valid, albeit more complex,
definitions are possible. (In the case of \OCaml, the switch
\erlcode{-rectypes} allows us to compile the one above, though.) If we
grant ourselves the use of recursion, which we never banned, we can
actually write a simpler definition of~\erlcode{x/1}, named
\erlcode{y/1}:
\begin{alltt}
y(F) -> fun(X) -> (F(y(F)))(X) end.\hfill% \emph{Recursive}
\end{alltt}
This definition is actually very easy to come by, as it relies on the
computational equivalence, for all~\erlcode{X},
\begin{equation*}
\erlcode{(y(F))(X)} \equiv \erlcode{(F(y(F)))(X)},
\end{equation*}
If we assume the mathematical property \(\forall x.f(x) = g(x)
\Rightarrow f = g\), the previous equivalence would yield
\begin{equation*}
\erlcode{y(F)} \equiv \erlcode{F(y(F))},
\end{equation*}
which, by definition, shows that \erlcode{y(F)} is a
fixed\hyp{}point of~\erlcode{F}. Beware that
\begin{alltt}
y(F) -> F(y(F)).\hfill% \emph{Infinite loop}
\end{alltt}
does not work because the call \erlcode{y(\(F\))} would
\emph{immediately} start evaluating the call \erlcode{y(\(F\))} in the
body, therefore never ending. Some functional programming languages
have a different evaluation strategy than \Erlang and do not always
start by evaluating the argument in a function call, which may make
this definition directly workable. Another example:
\begin{alltt}
fact(N) -> F = fun(F) -> fun(A) -> fun(0) -> A;
                                      (M) -> (F(A*M))(M-1)
                                   end
                         end
               end,
           ((y(F))(1))(N).
\end{alltt}
The technique we developed in the previous lines can be used to reduce
the amount of control stack in some functions. For example, consider
\begin{verbatim}
cat(   [],T) -> T;
cat([X|S],T) -> [X|cat(S,T)].
\end{verbatim}
Note how the parameter~\erlcode{T} is threaded until the first
argument is empty. This means that a reference to the original
stack~\erlcode{T} is duplicated at each rewrite until the last step,
because the definition is not in tail form. In order to avoid this, we
could use a recursive anonymous function which binds~\erlcode{T} not
as a parameter but as part of the (embedding) scope:
\begin{alltt}
cat(S,\textbf{T}) -> G = fun(F) -> fun(   []) -> \textbf{T};\hfill% T \emph{in scope}
                             ([X|U]) -> [X|F(U)]
                          end
                end,
            (y(G))(S).
\end{alltt}
This transformation is called
\emph{lambda\hyp{}dropping}\index{functional
  language!lambda-dropping@$\lambda$-dropping}, and its inverse
\emph{lambda\hyp{}lifting}\index{functional
  language!lambda-lifting@$\lambda$-lifting}. The function
\erlcode{y/1} is called the \emph{Y fixed\hyp{}point combinator}.

We sometimes may want to define two mutually recursive anonymous
functions. Consider the following example, which is in practice
utterly useless and inefficient, but simple enough to illustrate our
point.
\begin{verbatim}
even(0) -> true;
even(N) -> odd(N-1).

odd(0) -> false;
odd(N) -> even(N-1).
\end{verbatim}
Let us say that we do not want \erlcode{even/1} to be callable from
any other function but \erlcode{odd/1}. This means that we want the
following pattern:
\begin{alltt}
odd(I) -> Even = fun(\fbcode{CCC}) -> \fbcode{CCCCC} end,
          Odd  = fun(\fbcode{CCC}) -> \fbcode{CCCCC} end,
          Odd(I).
\end{alltt}
where \erlcode{Even} and \erlcode{Odd} depend on each other. As the
canvas is laid out, \erlcode{Even} cannot call \erlcode{Odd}. The
technique to allow mutual recursion consists in abstracting the first
function over the second, that is, \erlcode{Even} becomes a function
whose parameter is a function destined to be used as \erlcode{Odd}:
\begin{alltt}
odd(I) -> Even = fun(\textbf{Odd}) -> fun(0) -> true;
                                (N) -> \textbf{Odd}(N-1)
                             end
                 end,
          Odd  = fun(\fbcode{CCC}) -> \fbcode{CCCCC} end,
          Odd(I).
\end{alltt}
The next step is more tricky. We can start na\"{\i}vely, though, and
let the problem come to the fore by itself:
\begin{alltt}
odd(I) -> Even = fun(\textbf{Odd}) -> fun(0) -> true;
                                (N) -> \textbf{Odd}(N-1)
                             end
                 end,
          Odd  = fun(0) -> false;
                    (N) -> (Even(\fbox{Odd}))(N-1)
                 end,
          Odd(I).
\end{alltt}
The problem is not unheard of and we already know how to define an
anonymous recursive function by abstracting over the recursive call
and passing the resulting function to the Y~fixed\hyp{}point
combinator:
\begin{alltt}
odd(I) -> Even = fun(Odd) -> fun(0) -> true;
                                (N) -> Odd(N-1)
                             end
                 end,
          Odd  = \textbf{y(fun(F) ->} fun(0) -> false;
                                (N) -> (Even(\textbf{F}))(N-1)
                             end
                   \textbf{end)},
          Odd(I).
\end{alltt}
The technique presented here to achieve local recursion is interesting
beyond compilation, as shown
by \cite{GoldbergWiener_2009}. Fixed\hyp{}point combinators can also
be shown to work in imperative languages, like \Clang:
\begin{alltt}
\textbf{#include}<stdio.h>
\textbf{#include}<stdlib.h>

\textbf{typedef int} (*fp)();

\textbf{int} fact(fp f, \textbf{int} n) \{
  \textbf{return} n? n * ((\textbf{int} (*)(fp,\textbf{int}))f)(f,n-1) : 1; \}

\textbf{int} read(\textbf{int} dec, \textbf{char} arg[]) \{
  \textbf{return} ('0' <= *arg && *arg <= '9')?
         read(10*dec+(*arg - '0'),arg+1) : dec; \}

\textbf{int main}(\textbf{int} argc, \textbf{char}** argv) \{
  \textbf{if} (argc == 2)
     printf("%u\textbackslash{n}",fact(&fact,read(0,argv[1])));
  \textbf{else} printf("Only one integer allowed.\textbackslash{n}");
  \textbf{return} 0;
\}
\end{alltt}
\index{functional language!higher-order $\sim$!fixed-point
  combinator|)}
\index{functional language!Erlang@\Erlang!local
  recursion|)}

\mypar{Continuations}
\index{functional language!higher-order $\sim$!continuations|(}

In section~\ref{sec:into_tail_form}, the transformation into tail form
applies to first\hyp{}order programs, that is, those without
higher\hyp{}order functions, and its result are first\hyp{}order
programs as well. Here, we explain briefly a transformation to tail
form which results in higher\hyp{}order functions in
\emph{continuation\hyp{}passing style}\index{continuation-passing
  style|see{functional language, higher\hyp{}order, continuations}}
(often abbreviated CPS). The main advantage is that programs are
shorter. The first example was \erlcode{flat/1}:
\verbatiminput{flat.def} Applying the algorithm of
section~\ref{sec:into_tail_form}, the tail form we found was
\begin{verbatim}
flat_tf(T)         -> flat(T,[]).
flat(       [],A)  -> appk([],A);
flat(   [[]|T],A)  -> flat(T,A);
flat([[X|S]|T],A)  -> flat([X,S|T],A);
flat(    [Y|T],A)  -> flat(T,[{k1,Y}|A]).

appk(V,        []) -> V;
appk(V,[{k1,Y}|A]) -> appk([Y|V],A).
\end{verbatim}
(This is the version using a stack accumulator instead of embedded
tuples.) The driving idea consists in adding a stack~\erlcode{A} which
accumulates the variables of all the call contexts, each occurrence
being uniquely tagged, and an auxiliary function \erlcode{appk/2} then
reconstructs the contexts.

The idea behind continuation\hyp{}passing style is to not separate the
variables of the contexts and their reconstruction. Instead, what is
saved is a function, named \emph{continuation}, corresponding to one
clause of \erlcode{appk/1} reconstructing a context. This way, there
is no need for \erlcode{appk/1}. First, just like an empty accumulator
was created, an initial continuation is needed. For the moment, we
will ignore it. Just like an extra argument was added for the
accumulator, an extra argument is added for the continuation:
\begin{alltt}
\textbf{flat\_k(T)           -> flat\_k(T,\fbcode{CCCCCCCCC}\,).}
flat\_k(       [],\textbf{K}) -> [];\hfill% K \emph{unused yet}
flat\_k(   [[]|T],\textbf{K}) -> flat\_k(T,\textbf{K});
flat\_k([[X|S]|T],\textbf{K}) -> flat\_k([X,S|T],\textbf{K});
flat\_k(    [X|T],\textbf{K}) -> [X|flat\_k(T,\textbf{K})].
\end{alltt}
Just like before, each right\hyp{}hand side is examined in order. If
it contains no function call, the expression (where \erlcode{appk/2}
was called): is applied to the continuation~\erlcode{K}:
\begin{alltt}
flat\_k(       [],K) -> \textbf{K([])};
\end{alltt}
If it is a function call in tail form, nothing is done, just like
before:
\begin{alltt}
flat\_k(   [[]|T],K) -> flat\_k(T,K);
flat\_k([[X|S]|T],K) -> flat\_k([X,S|T],K);
\end{alltt}
If the right\hyp{}hand side is not in tail form, we identify the first
call to be evaluated. Here, there is only one:
\erlcode{flat\_k(T,K)}. Now is the main difference with the original
transformation. Instead of extracting the variables from the context
and generating a clause of \erlcode{appk/2} reconstructing that
context, we pass to the call a new continuation which applies the
context to the result of the call and then calls~\erlcode{K}, just
like \erlcode{appk/2} was called recursively:
\begin{alltt}
flat\_k(    [X|T],K) -> flat\_k(T,\textbf{fun(V) -> K([X|V]) end}).
\end{alltt}
Finally, we need to determine what is the continuation counterpart of
the empty accumulator. More precisely, we want to find an equivalent
to \erlcode{appk(V,[]) -> V.} That is, we want a continuation such
that, provided with~\erlcode{V}, returns~\erlcode{V}: it is the
identity function. We now have completed the transformation to
continuation\hyp{}passing style:
\begin{alltt}
flat\_k(T)           -> flat\_k(T,\textbf{fun(V) -> V end}).
flat\_k(       [],K) -> K([]);
flat\_k(   [[]|T],K) -> flat\_k(T,K);
flat\_k([[X|S]|T],K) -> flat\_k([X,S|T],K);
flat\_k(    [X|T],K) -> flat\_k(T,\textbf{fun}(V) -> K([X|V]) \textbf{end}).
\end{alltt}
The number of rewrites is the same as with \erlcode{flat\_tf/1}; the
main interest is that the resulting code is shorter, as each clause of
\erlcode{appk/2} is encoded as an anonymous function at each location
not in tail form. (Note that it is tradition to name the continuations
with the letter~\erlcode{K}.)

Let us consider another related example, \erlcode{flat0/1}:
\verbatiminput{flat0.def} The first\hyp{}order tail form we derived
was
\begin{verbatim}
flat0_tf(T)          -> flat0(T,[]).
flat0(         [],A) -> appk([],A);
flat0(     [[]|T],A) -> flat0(T,A);
flat0([Y=[_|_]|T],A) -> flat0(Y,[{k1,T}|A]);
flat0(    [Y|T],A)   -> flat0(T,[{k34,Y}|A]).
cat(   [],T,A)       -> appk(T,A);
cat([X|S],T,A)       -> cat(S,T,[{k34,X}|A]).
appk(V,[{k34,Y}|A])  -> appk([Y|V],A);
appk(V, [{k2,W}|A])  -> cat(W,V,A);
appk(V, [{k1,T}|A])  -> flat0(T,[{k2,V}|A]);
appk(V,         [])  -> V.
\end{verbatim}
Again, for the sake of the argument, we use the non\hyp{}optimised
version without embedded tuples for the stack accumulator. First, we
generate the identity continuation:
\begin{alltt}
flat0\_k(T) -> flat0\_k(T,\textbf{fun(V) -> V end}).
\end{alltt}
The right\hyp{}hand side of the first clause of \erlcode{flat0/1}
contains no calls so we apply to it the current continuation:
\begin{alltt}
flat0\_k(         [],K) -> \textbf{K([])};
\end{alltt}
The right\hyp{}hand side of the second clause of \erlcode{flat0/1} is a
call in tail form, so its transform just passes around the current
continuation:
\begin{alltt}
flat0\_k(     [[]|T],K) -> flat0\_k(T,\textbf{K});
\end{alltt}
The third clause is more complicated because it contains three
calls. Let us decide that the first to be evaluated will be
\erlcode{flat0(Y)}. (\Erlang does not specify the order of evaluation
of arguments.) We start by setting the framework of the new
continuation:
\begin{alltt}
flat0\_k([Y=[\_|\_]|T],K) -> flat0\_k(Y,\textbf{fun(V) -> \fbcode{HHHHH} end});
\end{alltt}
The parameter~\erlcode{V} will hold, when the new continuation will be
called, the value of \erlcode{flat0(Y)}. Next, we must evaluate the
call \erlcode{flat0(T)}, so we set
\begin{alltt}
flat0\_k([Y=[\_|\_]|T],K) ->
  flat0\_k(Y,fun(V) -> \textbf{flat0\_k(T,fun(W) -> \fbcode{HHHHH} end)} end);
\end{alltt}
We have to prepare the future call to \erlcode{cat/2}, which must also
be transformed in continuation\hyp{}passing style. What must be
concatenated are the values of \erlcode{flat0(Y)} and
\erlcode{flat0(T)}. The former will be bound by the
parameter~\erlcode{V} and the latter by~\erlcode{W}, therefore:
\begin{alltt}
flat0\_k([Y=[\_|\_]|T],K) ->
  flat0\_k(Y,\textbf{fun}(V) ->
              flat0\_k(T,\textbf{fun}(W) -> \textbf{cat\_k(V,W,\fbcode{HHH}\,)} \textbf{end}) \textbf{end});
\end{alltt}
Finally, we must put to good use the continuation~\erlcode{K} by
keeping in mind its meaning: `Call~\erlcode{K} with the value of
\erlcode{cat(flat0(Y),flat0(T))}.' At this point, we do not know the
value of this call, so we have to pass~\erlcode{K}
to \erlcode{cat\_k/3}, which will know that value:
\begin{alltt}
flat0\_k([Y=[\_|\_]|T],K) ->
  flat0\_k(Y,\textbf{fun}(V) ->
              flat0\_k(T,\textbf{fun}(W) -> cat\_k(V,W,\textbf{K}) \textbf{end}) \textbf{end});
\end{alltt}
Now, we must transform \erlcode{cat\_k/3} in continuation\hyp{}passing
style. The original \erlcode{cat/2} is
\begin{verbatim}
cat(   [],T) -> T;
cat([X|S],T) -> [X|cat(S,T)].
\end{verbatim}
We have
\begin{alltt}
cat\_k(   [],T,K) -> \textbf{K(T)};
cat\_k([X|S],T,K) -> cat\_k(S,\textbf{fun(V) -> K([X|V]) end}).
\end{alltt}
Note that we did not care for introducing the identity continuation,
as there is only one call to \erlcode{cat\_k/3}. Remains to transform
the last clause of \erlcode{flat0/2}, which contains one call whose
context is \erlcode{[Y|\textvisiblespace]}:
\begin{alltt}
flat0\_k(      [Y|T],K) -> flat0\_k(T,\textbf{fun(V) -> [Y|V] end}).
\end{alltt}
Summing up all previous steps, we arrive at
\begin{alltt}
flat0_k(T)             -> flat0_k(T,fun(V) -> V end).
flat0_k(         [],K) -> K([]);
flat0_k(     [[]|T],K) -> flat0_k(T,K);
flat0_k([Y=[_|_]|T],K) ->
  flat0_k(Y,\textbf{fun}(V) ->
              flat0_k(T,\textbf{fun}(W) -> cat_k(V,W,K) \textbf{end}) \textbf{end});
flat0_k(      [Y|T],K) -> flat0_k(T,\textbf{fun}(V) -> [Y|V] \textbf{end}).

cat_k(   [],T,K) -> K(T);
cat_k([X|S],T,K) -> cat_k(S,\textbf{fun}(V) -> K([X|V]) \textbf{end}).
\end{alltt}
All functions are now in tail form, because a continuation is an
anonymous function, that is, it is a value.

Our next example is \erlcode{fib/1}, the straightforward but extremely
inefficient implementation of the Fibonacci function:
\begin{verbatim}
fib(0) -> 1;
fib(1) -> 1;
fib(N) -> fib(N-1) + fib(N-2).
\end{verbatim}
The corresponding continuation\hyp{}passing style is
\begin{alltt}
fib\_k(N)   -> fib\_k(N,\textbf{fun}(V) -> V \textbf{end}).
fib\_k(0,K) -> K(0);
fib\_k(1,K) -> K(1);
fib\_k(N,K) ->
  fib\_k(N-1,\textbf{fun}(V) -> fib\_k(N-2,\textbf{fun}(W) -> K(V+W) \textbf{end}) \textbf{end}).
\end{alltt}

\index{binary search tree!leaf insertion|(} Continuation\hyp{}passing
style is also interesting because it makes some optimisations easier
to spot \citep{Danvy_2004}. The design of \fun{sfst\(_0\)/2} in
\fig~\vref{fig:sfst0} was motivated by the need to share the input in
case the sought item was missing. This kind of improvement is common
in algorithms combining a search and an optional local update. For
example, let us consider again leaf insertion \emph{without
  duplicates} in a binary search tree\index{binary search tree} in
\fig~\vref{fig:insl0}:
\begin{alltt}
insl0(Y,\{bst,X,T1,T2\}) when X > Y -> \{bst,X,insl0(Y,T1),T2\};
insl0(Y,\{bst,X,T1,T2\}) when Y > X -> \{bst,X,T1,insl0(Y,T2)\};
insl0(Y,          ext)            -> \{bst,Y,ext,ext\};
insl0(Y,            T)            -> T.
\end{alltt}
In case \erlcode{Y}~is present in the tree, the last clause will share
the subtree below the found occurrence of~\erlcode{Y} in the input
tree, but the two first clauses, corresponding to the search phase,
will duplicate all the nodes from the input root to~\erlcode{Y}
(excluded). This can be avoided by threading the original tree through
the recursive calls and transforming the function in tail form, so
if~\erlcode{Y} is found, the entire input is shared and the evaluation
stops immediately (no pending contexts). First, let us transform the
definition into continuation\hyp{}passing style (new continuations set
in boldface type):
\begin{alltt}
insl0(Y,T)               -> insl0(Y,T,\textbf{fun(V) -> V end}).
insl0(Y,\{bst,X,T1,T2\},K) when X > Y ->
\hfill{}insl0(T1,Y,\textbf{fun(V) -> K(\{bst,X,V,T2\}) end});
insl0(Y,\{bst,X,T1,T2\},K) when Y > X ->
\hfill{}insl0(T2,Y,\textbf{fun(V) -> K(\{bst,X,T1,V\}) end});
insl0(Y,          ext,K) -> K(\{bst,Y,ext,ext\});
insl0(Y,            T,K) -> K(T).
\end{alltt}
Second, we thread the original search tree~\erlcode{T} (renamed
\erlcode{U}):
\begin{alltt}
insl0(Y,T)                 -> insl0(TmT,fun(V) -> V end,\textbf{T}).
insl0(Y,\{bst,X,T1,T2\},K,\textbf{U}) when X > Y ->
\hfill{}insl0(T1,Y,fun(V) -> K(\{bst,X,V,T2\}) end,\textbf{U});
insl0(Y,\{bst,X,T1,T2\},K,\textbf{U}) when Y > X ->
\hfill{}insl0(T2,Y,fun(V) -> K(\{bst,X,T1,V\}) end,\textbf{U});
insl0(Y,          ext,K,\textbf{U}) -> K(\{bst,Y,ext,ext\});
insl0(Y,            T,K,\textbf{U}) -> K(T).
\end{alltt}
Finally, we discard the continuation in the last clause of
\erlcode{insl0/4} and the right\hyp{}hand side shares the input:
\begin{alltt}
insl0(Y,T)                 -> insl0(Y,T,fun(V) -> V end,T).
insl0(Y,\{bst,X,T1,T2\},K,U) when X > Y ->
\hfill{}insl0(T1,Y,fun(V) -> K(\{bst,X,V,T2\}) end,U);
insl0(Y,\{bst,X,T1,T2\},K,U) when Y > X ->
\hfill{}insl0(T2,Y,fun(V) -> K(\{bst,X,T1,V\}) end,U);
insl0(Y,          ext,K,U) -> K(\{bst,Y,ext,ext\});
insl0(Y,            T,K,\textbf{U}) -> \textbf{U}.\hfill% \emph{Input shared}
\end{alltt}
In functional languages featuring \emph{exceptions}\index{functional
  language!Erlang@\Erlang!exception}, as \Erlang does, the same effect
can be achieved without continuations:
\begin{alltt}
insl0(Y,T)\hfill{}-> \textbf{try insl\_(Y,T) catch throw:dup -> T end}.
insl\_(Y,\{bst,X,T1,T2\}) when X > Y -> \{bst,X,insl\_(Y,T1),T2\};
insl\_(Y,\{bst,X,T1,T2\}) when Y > X -> \{bst,X,T1,insl\_(Y,T2)\};
insl\_(Y,          ext)            -> \{bst,Y,ext,ext\};
insl\_(Y,            T)            -> \textbf{throw(dup)}.
\end{alltt}
This style is to be preferred over CPS because it preserves most of
the original program (`direct style'). Nevertheless, this shows that
continuations are useful when writing a compiler, so features like
exceptions can be removed, as long as higher\hyp{}order functions are
available \citep{Appel_1992}. These, in turn, can be transformed into
first\hyp{}order functions by
\emph{defunctionalisation} \citep{Reynolds_1972,DanvyNielsen_2001}\index{functional
  language!higher-order $\sim$!defunctionalisation}.
\index{binary search tree!leaf insertion|)}

Continuations can also be a design pattern. Consider the problem of
determining whether a given stack is a
\emph{palindrome}\index{palindrome|(}, that is, given~\(s\), whether
\(s \equiv \fun{rev}(s)\). The obvious
\begin{verbatim}
pal(S) -> S == rev(S).
\end{verbatim}
works in \(n+2\) rewrites because the cost of the operator
(\texttt{==}) is not accounted for. Internally, though, what happens
is that \erlcode{S}~is traversed (completely if it is a
palindrome). If we do not allow ourselves the use of the equality
operator on stacks, we may try
\begin{verbatim}
pal(S)  -> eq(S,rev(S)).
eq(S,S) -> true;
eq(_,_) -> false.
\end{verbatim}
which is cheating in the same way: the non\hyp{}linear pattern
\erlcode{eq(S,S)} \index{functional language!pattern!non-linear
  $\sim$} requires that its arguments are traversed, without impacting
the cost. If we also give up such patterns on stacks, we may come up
with a solution based on continuations \citep{DanvyGoldberg_2001}:
\begin{alltt}
pal(S)               -> pal(S,S,\textbf{fun(\_) -> true end}).
pal(    S,     [],K) -> K(S);\hfill% \emph{Even length}
pal([\_|S],    [\_],K) -> K(S);\hfill% \emph{Odd length}
pal([X|S],[\_,\_|T],K) ->
\hfill{}pal(S,T,\textbf{fun}([Y|U]) -> X == Y \textbf{andalso} K(U) \textbf{end}).
\end{alltt}
We reuse here an idea we saw in \fig~\vref{fig:tms}: duplicating the
reference to~\erlcode{S} and moving into the second copy twice as fast
as in the first (last clause); when reaching the end of the second
copy (first and second clause of \erlcode{pal/3}), the first copy
holds the second half of the original stack, which is applied to the
current continuation. The continuation was constructed by keeping a
reference~\erlcode{X} to the current item in the first copy and
scheduling an equality test with the first item of its parameter (the
operator \erlcode{andalso} is \emph{sequential}, to wit, if its first
argument evaluates to \erlcode{false}, its second argument is not
evaluated, so the continuation is dropped). Indeed, the idea is to
compare the second half of the original stack with the items of the
first half \emph{in reverse order}, and this is the very purpose of
the continuation. Note that the continuation takes a stack as an
argument, but evaluates into a boolean, contrary to previous uses of
continuations, where the initial continuation was the identity
function (compare with \erlcode{par/1}). Notice also how we also find
out the parity of the length of the original stack, without using
integers. It is easy to write an equivalent first\hyp{}order function:
\begin{verbatim}
pal0(S)               -> pal0(S,S,[]).
pal0(    S,     [],A) -> eq(S,A);
pal0([_|S],    [_],A) -> eq(S,A);
pal0([X|S],[_,_|T],A) -> pal0(S,T,[X|A]).

eq(   [],   []) -> true;
eq([X|S],[X|T]) -> eq(S,T);
eq(    _,    _) -> false.
\end{verbatim}
The difference is that \erlcode{pal0/3}, instead of constructing a
continuation holding the items of the first half and preparing a test,
explicitly reverses the first half and compares it to second half by
means of \erlcode{eq/2}. The cost of \erlcode{pal/1} and
\erlcode{pal0/1} is the same.

The minimum cost is \(\B{\fun{pal}}{n} = \B{\fun{pal0}}{n} =
\floor{n/2} + 2\), if \erlcode{S}~is not a palindrome and a difference
lies in the middle.

The maximum cost is \(\W{\fun{pal}}{n} = \W{\fun{pal}_0}{n} =
2\floor{n/2} + 1\), if \erlcode{S}~is a
palindrome.\index{palindrome|)}

Further comparison of their memory consumption would require a way to
quantify the store needed for a functional value, but it is likely
that, in this case, the memory usages of \erlcode{pal/1} and
\erlcode{pal0/1} are similar, so choosing one or the other is purely a
matter of style, for instance, conciseness may be preferred.

\index{suffix!find all suffixes}
\index{suffix|see{word factoring}}

Another entertaining example is provided again by
\cite{Danvy_1988,Danvy_1989}. The purpose is to make all the prefixes
of a word, for example,
\begin{equation*}
\erlcode{allp([a,b,c,d])} \twoheadrightarrow
\erlcode{[[a],[a,b],[a,b,c],[a,b,c,d]]},
\end{equation*}
where \erlcode{allp/1} stands for \emph{all prefixes}. Building the
suffixes with a linear cost would be much easier, in particular, it is
straightforward to maximise memory sharing with an alias:
\begin{verbatim}
alls(     []) -> [];
alls(S=[_|T]) -> [S|alls(T)].
\end{verbatim}
(The name \erlcode{alls} means \emph{all suffixes}.) We have
\(\C{\fun{alls}}{n} = n + 1\) and
\begin{equation*}
\erlcode{alls([a,b,c,d])}
\twoheadrightarrow \erlcode{[[a,b,c,d],[b,c,d],[c,d],[d]]}.
\end{equation*}
\index{prefix!find all prefixes}
\index{prefix|see{word factoring}}

A solution for prefixes, based on continuations, is
\begin{alltt}
allp(S)       -> allp(S,\textbf{fun}(X) -> X \textbf{end}).
allp(   [],\_) -> [];
allp([X|S],K) -> [K([X])|allp(S,\textbf{fun}(T) -> K([X|T]) \textbf{end})].
\end{alltt}
Another higher\hyp{}order solution \erlcode{allp0/1} relies on a map
(see page~\pageref{par:maps}):
\begin{alltt}
allp0(   []) -> [];
allp0([X|S]) -> [[X]|map(\textbf{fun}(T) -> [X|T] \textbf{end},allp0(S))].
\end{alltt}
We have \(\C{\fun{allp}_0}{n} = (n+1) + \sum_{k=1}^{n-1}k =
\frac{1}{2}n^2 + \frac{1}{2}n + 1\).
\index{functional language!higher-order $\sim$!continuations|)}
\index{functional language!Erlang@\Erlang|)}
\index{functional language!higher-order $\sim$|)}

\paragraph{Exercise}

Write a first\hyp{}order version of \erlcode{allp0/1}.
